[
  {
    "objectID": "posts/api.html",
    "href": "posts/api.html",
    "title": "FastAPI",
    "section": "",
    "text": "Did you ever dream to build your own fast API written in Python?\nFastAPI is a modern, high-performance web framework for building APIs with Python. It blends the ease of use of Python with exceptional speed, making it an ideal choice for developing robust and efficient web applications. Whether you‚Äôre crafting a simple REST API or a complex microservices architecture, FastAPI will help you with that.\nThe installation of FastAPI is very simple and it involves the installation of two packages: (1) FastAPI and (2) Uvicorn. The first one is the framework itself and the second one is the server that will run the API.\npip install fastapi\npip install \"uvicorn[standard]\"\nIn this tutorial, we will build a simple API that will allow us to perform basic CRUD (Create, Read, Update, Delete) operations on a database. The access to the database will be done by SQLAlchemy, which is a Python SQL toolkit for SQL databases. By the means of fastAPI, we will also provide a documentation of the API, which will be accessible through a web browser."
  },
  {
    "objectID": "posts/api.html#setting-up-the-project",
    "href": "posts/api.html#setting-up-the-project",
    "title": "FastAPI",
    "section": "Setting Up the Project",
    "text": "Setting Up the Project\nTo begin, we import the necessary libraries including pandas, pydantic, and fastapi. We‚Äôre also using sqlalchemy for database interactions. Our code starts by creating a database connection using the SQLAlchemy library. We‚Äôve chosen an SQLite database for this demonstration. However, you can use any database you like.\nimport pandas as pd\n\nfrom pydantic import BaseModel, Field\nfrom fastapi import FastAPI, HTTPException, \n\nfrom sqlalchemy import create_engine, MetaData, Table, select, insert, update, delete\n\n## Create a metadata object\nmetadata = MetaData()\n\n## Create a connection to the database\nengine = create_engine(              \n    ## Path to database                     \n    'sqlite:///../../data/FastAPI.db',      #¬†<1>\n)\n\n## Reflect census table from the engine: census\nfastapi = Table(\n    \"fastapi\", \n    metadata,\n    autoload_with = engine,\n)\nMoreover, we create a utility function to connect to the database and return a json-style format. This function will be used in the endpoints to fetch data from the database and return the dictionary, which will be converted to json format by FastAPI.\n## Helper function to fetch data\ndef fetch_return_dict(\n        stmt, \n        engine = engine,\n    ) -> dict:\n    \"\"\"\n    Utility function to convert sql query results to dict via pandas dataframe\n    \"\"\"\n    ## Create a connection to the database\n    connection = engine.connection()\n    data = connection.execute(stmt).fetchall()\n    connection.close()\n\n    return (\n        pd.DataFrame(\n            data = data\n        )\n        .to_dict(\n            orient = \"records\", \n        )\n    )"
  },
  {
    "objectID": "posts/api.html#fastapi-object",
    "href": "posts/api.html#fastapi-object",
    "title": "FastAPI",
    "section": "FastAPI Object",
    "text": "FastAPI Object\nWe instantiate a FastAPI app object with a title, description, and version. This app will serve as the backbone of our API. The description section contains details about the API and apear in the documentation.\n## Instantiate a FastAPI object\napp = FastAPI(\n    title           =   \"API Service\",\n    description     =   \"\"\"\n    ...\n    Add more description here\n    ...\n    \"\"\",\n    version         =   \"0.0.1\",\n)"
  },
  {
    "objectID": "posts/api.html#defining-the-data-model",
    "href": "posts/api.html#defining-the-data-model",
    "title": "FastAPI",
    "section": "Defining the Data Model",
    "text": "Defining the Data Model\nFastAPI encourages the use of Pydantic models for data validation and serialization. Pydantic is a Python library that simplifies the process of data validation and settings management in applications. It allows you to define data schemas using Python data classes with type annotations, and automatically validates and parses incoming data according to these schemas.\nWe‚Äôve defined IncomeTaxModel, MunicipalityDataOut, IncomeTaxDataOut, and YearDataOut as Pydantic models (BaseModel and type annotation) to structure the data that will be sent and received by the API endpoints.\nclass TaxModel(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Tax             :   int    =  Field(\"Average tax for a given year\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n\nclass MunicipalityDataOut(BaseModel):\n    Tax             :   int    =  Field(\"Average tax for a given year\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n    \nclass TaxDataOut(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n\nclass YearDataOut(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Tax             :   int    =  Field(\"Average tax for a given year\")"
  },
  {
    "objectID": "posts/api.html#building-api-endpoints",
    "href": "posts/api.html#building-api-endpoints",
    "title": "FastAPI",
    "section": "Building API Endpoints",
    "text": "Building API Endpoints\nWe proceed to define various API endpoints using FastAPI‚Äôs decorators. These endpoints cover different scenarios, from fetching data for a specific year to creating and updating entries in the database. Each endpoint corresponds to a specific HTTP method (GET, POST, PUT, DELETE) and handles specific types of requests.\nFor instance, the index route handles a basic GET request and returns all data from the database, ordered by the year. The get_data_of_year route fetches data for a given year, while get_municipality_data and get_district_data retrieve data for specific municipalities or districts, respectively. Additionally, the create_new_entry, update_tax_entry, update_year_entry, and delete_tax_entry routes allow for CRUD (Create, Read, Update, Delete) operations on the data.\nNext, I will show you the incomplete code for the endpoints (the complete code can be found in the end of the article):\n@app.get(\"/\")\ndef index():\n    return something\n\n##¬†Create a route to return data for a given year\n@app.get(\"/year/{year}\")\ndef get_data_of_year():\n    return something\n\n## Create a route to return data for a given city\n@app.get(\"/municipality/{municipality}\")\ndef get_municipality_data():\n    return something\n\n## Create a route to return data for a given district\n@app.get(\"/district/{district}\")\ndef get_district_data():\n    return something\n\n## Create a route to return data from the canton\n@app.get(\"/canton/\")\ndef get_canton_data():\n    return something\n\n## Create a new entry\n@app.post('/entry/{municipality}/{year}/{tax}')\ndef create_new_entry():\n    return something\n\n## Update an income tax entry\n@app.put(\"/update_tax/{municipality}/{year}/{tax}\")\ndef update_tax_entry():\n    return something\n    \n## update year entry\n@app.put(\"/update_year/{municipality}/{year_old}/{year_new}\")\ndef update_year_entry():\n    return something\n\n@app.delete(\"/delete/{municipality}/{year}/{tax}\")\ndef delete_tax_entry():\n    return something\nThe components of the the fastapi endpoints are as follows:\n\nDecorators are used to define the route of each endpoints and the HTTP-method.\nType annotation of the parameters allows the definition and validation of the data type of the parameters."
  },
  {
    "objectID": "posts/api.html#error-handling",
    "href": "posts/api.html#error-handling",
    "title": "FastAPI",
    "section": "Error Handling",
    "text": "Error Handling\nThe code also incorporates error handling. For instance, if a user attempts to create a new entry that already exists in the database, an HTTPException with an appropriate status code and detail message is raised. Similarly, error handling is employed for updating and deleting entries that don‚Äôt exist in the database. The following codes are use: 400 for Bad Request, 404 for Not Found, and 500 for Internal Server Error. More information can be found here.\nThe implementation of error handling is done by raise HTTPException() as follows:\n##¬†Create a index route\n@app.get(\"/\")\ndef index() -> list[IncomeTaxModel]:\n    \n    ...\n\n    result = fetch_return_dict(stmt)\n\n    if result:\n        return result\n    else:\n        raise HTTPException(\n            status_code = 404, \n            detail = f\"Item not found\",\n        )"
  },
  {
    "objectID": "posts/api.html#conclusion",
    "href": "posts/api.html#conclusion",
    "title": "FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we‚Äôve walked through the creation of a FastAPI web service for accessing income tax data. We‚Äôve covered the setup process, defining data models, creating API endpoints, and handling database interactions. FastAPI‚Äôs intuitive syntax and powerful features make it a fantastic choice for building efficient and robust APIs. This project serves as a foundation, demonstrating how to construct APIs that facilitate data retrieval and manipulation, crucial tasks in today‚Äôs data-centric world.\nOverall, this FastAPI-based web service exemplifies the elegance and utility of modern Python frameworks in building APIs that bridge the gap between data and applications."
  },
  {
    "objectID": "posts/pandas_method_chaning.html",
    "href": "posts/pandas_method_chaning.html",
    "title": "Pandas Method Chaining",
    "section": "",
    "text": "Introduction\nMethod chaining is a way to combine multiple pandas operations together into a single line or statement. It is not necessary to use method chaining, but it can make your code more concise and easier to read. In this post, we will look at how to use method chaining to clean and transform a dataset.\nHonestly, since I started using method chaining, I have found it difficult to go back to the old way of writing pandas code. I hope that by the end of this post, you will feel the same way. :)\nLet‚Äôs get started! First I show how to use method chaining to import a dataset as follows:\n\n## Data imports\n\ndf = (\n    # use sm \n    sm                          \n    # get datasets attribute\n    .datasets                   \n    # use get_rdataset method\n    .get_rdataset(              \n        \"car_prices\", \n        package='modeldata'\n    )\n    # get data attribute\n    .data\n)\n\nChaining in Python uses () in order to chain methods together. This allows the user to write multiple statements on different lines as showed in the code above. On big advantage of chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Not using chaining would require the user to write the code as follows:\ndf = sm.datasets.get_rdataset(\"car_prices\", package='modeldata').data\nUsing chaining makes the code more readable right?\nWhat I like most about chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Especially when working with large datasets, it is very useful to be able to comment out one line at a time.\nBefore continuing with the next section, let‚Äôs have a look at the data:\n\n(\n    df\n    .head()\n)\n\n\n\n\n\nTable¬†1:  Data Preview \n  \n    \n      \n      Price\n      Mileage\n      Cylinder\n      Doors\n      Cruise\n      Sound\n      Leather\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      convertible\n      coupe\n      hatchback\n      sedan\n      wagon\n    \n  \n  \n    \n      0\n      22661.05\n      20105\n      6\n      4\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      21725.01\n      13457\n      6\n      2\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      2\n      29142.71\n      31655\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      30731.94\n      22479\n      4\n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      33358.77\n      17590\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nThis dataset describes the price of sold cars by make, mileage and other features. Let us assume we would like to convert the data into a tidy format. We start with the car features.\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable¬†2:  Tidy format of car features \n  \n    \n      \n      Price\n      Mileage\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      Feature\n      Value\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Leather\n      0\n    \n    \n      5391\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      coupe\n      0\n    \n    \n      567\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cylinder\n      4\n    \n    \n      2175\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cruise\n      0\n    \n    \n      6999\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      sedan\n      1\n    \n  \n\n\n\n\n\nLet‚Äôs now continue and see how we can use the assign method to create new columns in our dataframe. We will use the assign method to create a new column called Make which will a vector representation of the car make.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable¬†3:  Continued from the previous chain I \n  \n    \n      \n      Price\n      Mileage\n      Feature\n      Value\n      Make\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      Leather\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      5391\n      8638.93\n      25216\n      coupe\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      567\n      8638.93\n      25216\n      Cylinder\n      4\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      2175\n      8638.93\n      25216\n      Cruise\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      6999\n      8638.93\n      25216\n      sedan\n      1\n      [0, 0, 1, 0, 0, 0]\n    \n  \n\n\n\n\n\nLet‚Äôs continue and use groupby and aggregate the Feature column by the agg method. This will give us the mean of each feature appears in the dataset.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .groupby(\n        by = 'Feature',\n    )\n    .agg(\n        {\n            'Value': ['mean'],\n        }\n    )\n)\n\n\n\n\n\nTable¬†4:  Continued from the previous chain II \n  \n    \n      \n      Value\n    \n    \n      \n      mean\n    \n    \n      Feature\n      \n    \n  \n  \n    \n      Cruise\n      0.752488\n    \n    \n      Cylinder\n      5.268657\n    \n    \n      Doors\n      3.527363\n    \n    \n      Leather\n      0.723881\n    \n    \n      Sound\n      0.679104\n    \n    \n      convertible\n      0.062189\n    \n    \n      coupe\n      0.174129\n    \n    \n      hatchback\n      0.074627\n    \n    \n      sedan\n      0.609453\n    \n    \n      wagon\n      0.079602\n    \n  \n\n\n\n\n\nSummarizing, we could go on forever and adding more steps to our method chain. But I think you get the point. We can do a lot with method chaining and it is a great way to write clean and readable code.\n\n\nConclusion\nMethod chaining in Pandas is a powerful technique that offers several advantages for data manipulation and analysis workflows. It involves combining multiple operations on a DataFrame or Series into a single, concise chain of method calls. This approach enhances code readability, maintainability, and efficiency.\nFirstly, method chaining reduces the need for intermediate variables, streamlining code and making it more readable. By stringing together operations, such as filtering, transforming, and aggregating, the code becomes a clear and sequential representation of the data transformation process.\nSecondly, it encourages the use of functionally composed operations, leading to more modular and reusable code. This modular nature facilitates changes and updates, as adjustments can be made within the chain without affecting other parts of the code.\nFurthermore, method chaining promotes better memory usage and performance optimization. Pandas optimizes these chains under the hood, reducing the creation of unnecessary intermediate copies of data frames, which leads to improved execution speed and reduced memory overhead.\nLastly, method chaining aligns well with the ‚Äútidy data‚Äù philosophy, as it emphasizes a more structured, organized approach to data manipulation. This promotes consistency and clarity in the analysis process, aiding in collaboration and code maintenance.\nI hope you enjoyed this post and learned something new. If you have any questions contact me. Thanks for reading!"
  },
  {
    "objectID": "posts/glm.html",
    "href": "posts/glm.html",
    "title": "My first post about statmodels - so be nice! :)",
    "section": "",
    "text": "Introduction: Play with GLMs\nDuring my study at HSLU I have learned to use mostly R when it comes to statistic and for some machine learning courses as well. When it comes to work, the most companies I have seen so far, use Python as their main programming language. My previous two employers used Python and R. This article is not going to discuss which language is better, but rather focus on how to use Python and the library statmodels which seems to produce similar outputs as it would in R.\nOne important and cool thing about the statmodels library is that it has a lot of data sets already included. You can find them here. The data set I use is a R data set from Rdatasets. In particular, the dataset from the package modeldata called car_prices is used.\n## Data imports\ndf = (\n    statsmodels.api #¬†imported as sm in following code\n    .datasets\n    .get_rdataset(\"car_prices\", package='modeldata')\n    .data\n)\nAnother important thing about this blog post is the usage of Quarto. Quarto allowed me to write this blog post directly from a Jupyter-Notebook without any fancy and complicated transformations. For more information, please visit the quarto website.\nHowever, let‚Äôs start with the analysis. The first thing we do is to import the necessary libraries and the data set.\n\n\nShow the code\n## General imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\n## Data manipulation imports\nimport pandas as pd\nimport numpy as np\n\n## Display imports\nfrom IPython.display import display, Markdown\n\n## statmodels import\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.genmod.families.family as fam\nfrom patsy import dmatrices\n\n## Plot imports\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (5,5/2.5)\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_theme()\nsns.set_context(\n    \"paper\", \n    rc={\n        \"figsize\"       :   plt.rcParams['figure.figsize'],\n        'font_scale'    :   1.25,\n    }\n)\n\n\n## Data imports\ndf = (\n    sm\n    .datasets\n    .get_rdataset(\n        \"car_prices\", \n        package='modeldata',\n    )\n    .data\n)\n\n\n\n\nExplanatory Data Exploration\nThe data exploration is a crucial step before fitting a model. It allows to understand the data and to identify potential problems. In this notebook, we will explore very briefly the data, as the focus of this article is to understand and apply statmodels.\n\n\nThe data set contains 804 observations (rows) and 18 predictors (columns). The data set contains information about car prices and the variables are described as follows: Price; Mileage; Cylinder; Doors; Cruise; Sound; Leather; Buick; Cadillac; Chevy; Pontiac; Saab; Saturn; convertible; coupe; hatchback; sedan; and wagon.\n\n\nFirst, two approximately continous variables Price and Mileage are investigated by means of graphical analysis. The following bar plots show the distribution of the two variables.\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng1 = sns.displot(\n    data = df,\n    x = 'Price',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of car price')\nplt.xlabel('Price')\nplt.ylabel('Count of occurance')\nplt.show(g1)\n\ng2 = sns.displot(\n    data = df,\n    x = 'Mileage',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Count of occurance')\nplt.show(g2)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable Price\n\n\n\n\n\n\n\n(b) Distribution of the variable Mileage\n\n\n\n\nFigure¬†1: Distribution of price and mileage variables.\n\n\n\nFigure¬†1 (a) shows on the left a right-skewed distribution with a peak around 15k$ and price values ranging from 8k dollars to 70k dollars. On the other hand, figure¬†1 (b) shows on the right a more ballanced distribution with a peak around 20k$ and price values ranging from 266 miles up to 50k miles.\nProceeding to the next two variables, Cylinder and Doors, one can see less possible values, ranging from\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng = sns.displot(\n    data = df,\n    x = 'Cylinder',\n    discrete = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Cylinder')\nplt.xlabel('Cylinder')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n# plt.figure()\ng = sns.displot(\n    data = df,\n    x = 'Doors',\n    discrete = True,\n    shrink = 0.5,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Doors')\nplt.xlabel('Doors')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable cylinder\n\n\n\n\n\n\n\n(b) Distribution of the variable doors\n\n\n\n\nFigure¬†2: Distribution of cylinder and doors variables.\n\n\n\nThe Figure¬†2 (a) surprised me quite a bit. I had anticipated the car to feature more than 8 cylinders, given that this dataset pertains to American cars. The cylinder count typically spans from 4 to 8, with the values accurately reflecting this range. It‚Äôs worth noting that the number of cylinders is expected to be even.\nAgain surprisingly, the Figure¬†2 (b) shows that the number of doors per car. The values are either 2 or 4, with the latter being more common. This is a bit surprising, as I would have expected the number of doors to be higher for American cars (SUV).\nNext, we check the distribution of the make of the cars in the dataset. For this analysis, we first pivot the dataframe using pd.melt() and calculate the sum by means of groupby() and sum() method as follows:\n\n\nShow the code\nbrands = (\n    df\n    [['Buick', 'Cadillac', 'Chevy', 'Pontiac', 'Saab']]\n    .melt()\n    .groupby('variable')\n    .sum()\n    .reset_index()\n)\n\nbrands.head()\n\n\n\n\n\n\nTable¬†1:  Cars by brand after melting, grouping, and summing \n  \n    \n      \n      variable\n      value\n    \n  \n  \n    \n      0\n      Buick\n      80\n    \n    \n      1\n      Cadillac\n      80\n    \n    \n      2\n      Chevy\n      320\n    \n    \n      3\n      Pontiac\n      150\n    \n    \n      4\n      Saab\n      114\n    \n  \n\n\n\n\n\nAfter aggregation, the visualization of the data is as follows:\n\n\nShow the code\nsns.catplot(\n    data    = brands.sort_values('value', ascending=False),\n    x       = 'variable',\n    y       = 'value',\n    kind    = 'bar',\n    height  = 5,\n    aspect  = 2,\n)\nplt.title('Number of cars by brand')\nplt.xlabel('Brand')\nplt.ylabel('Number of cars')\nplt.show()\n\n\n\n\n\nFigure¬†3: Number of cars by make\n\n\n\n\nIn descending order the most present make is: Chevy followed by Pontiac, Saab, Buick and Cadilac.\nAt this point it should be mentioned, that the data set is not balanced and the distribution of the features is not uniform across the different makes and car features. In a normal project this would be a point to consider and to take care of. However, for the purpose of this project, this is not necessary.\n\n\nModeling\nModeling with statsmodels becomes straightforward once the formula API and provided documentation are well understood. I encountered a minor challenge in grasping the formula API, but once I comprehended it, the usage turned out to be quite intuitive.\nLet‚Äôs delve into an understanding of the formula API (statsmodels.formula.api). This feature enables us to employ R-style formulas for specifying models. To illustrate, when fitting a linear model, the following formula suffices:\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\n    formula='y ~ x1 + x2 + x3', \n    data=df\n)\nThe formula API leverages the patsy package (Patsy Package). It‚Äôs worth noting that the formula API‚Äôs potency extends to intricate models. Additionally, it‚Äôs important to mention that the formula API automatically incorporates an intercept into the formula if one isn‚Äôt explicitly specified. For cases where the intercept is undesired, a -1 can be used within the formula.\nWith the glm class, a vast array of models becomes accessible importing as follows:\nimport statsmodels.genmod.families.family as fam\nThe fam import is necessary for specifying the family of the model. The families available are:\n\n\n\n\nTable¬†2: GLM Families\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nFamily(link,¬†variance[,¬†check_link])\nThe parent class for one-parameter exponential families.\n\n\nBinomial([link,¬†check_link])\nBinomial exponential family distribution.\n\n\nGamma([link,¬†check_link])\nGamma exponential family distribution.\n\n\nGaussian([link,¬†check_link])\nGaussian exponential family distribution.\n\n\nInverseGaussian([link,¬†check_link])\nInverseGaussian exponential family.\n\n\nNegativeBinomial([link,¬†alpha,¬†check_link])\nNegative Binomial exponential family (corresponds to NB2).\n\n\nPoisson([link,¬†check_link])\nPoisson exponential family.\n\n\nTweedie([link,¬†var_power,¬†eql,¬†check_link])\nTweedie family.\n\n\n\n\n\n\nFitting the model and analyzing the results are the same as one would using R. First define the model, then fit it, then analyze the results. The details of the fit can be accessed using the summary method.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  Price   No. Observations:                  804\nModel:                            GLM   Df Residuals:                      789\nModel Family:                Gaussian   Df Model:                           14\nLink Function:               Identity   Scale:                      8.4460e+06\nMethod:                          IRLS   Log-Likelihood:                -7544.8\nDate:                Sun, 03 Sep 2023   Deviance:                   6.6639e+09\nTime:                        21:02:49   Pearson chi2:                 6.66e+09\nNo. Iterations:                     3   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nMileage        -0.1842      0.013    -14.664      0.000      -0.209      -0.160\nCylinder     3659.4543    113.345     32.286      0.000    3437.303    3881.606\nDoors        1654.6326    174.525      9.481      0.000    1312.570    1996.695\nCruise        340.8695    295.962      1.152      0.249    -239.205     920.944\nSound         440.9169    234.484      1.880      0.060     -18.664     900.497\nLeather       790.8220    249.745      3.167      0.002     301.331    1280.313\nBuick       -1911.3752    336.292     -5.684      0.000   -2570.495   -1252.256\nCadillac      1.05e+04    409.274     25.663      0.000    9701.132    1.13e+04\nChevy       -3408.2863    213.274    -15.981      0.000   -3826.295   -2990.278\nPontiac     -4258.9628    256.358    -16.613      0.000   -4761.416   -3756.509\nSaab         9419.1227    331.211     28.438      0.000    8769.960    1.01e+04\nSaturn      -2859.0803    358.709     -7.970      0.000   -3562.137   -2156.023\nconvertible  1.258e+04    525.984     23.922      0.000    1.16e+04    1.36e+04\ncoupe        1559.7620    395.946      3.939      0.000     783.723    2335.801\nhatchback   -4977.3196    339.046    -14.680      0.000   -5641.837   -4312.803\nsedan       -3064.7176    215.007    -14.254      0.000   -3486.123   -2643.312\nwagon        1384.6400    364.920      3.794      0.000     669.410    2099.870\n===============================================================================\n\n\nAfter fitting a GLM using the glm class in statsmodels, you can obtain a summary of the model‚Äôs results using the .summary() method on the fitted model object. Here‚Äôs a general overview of the information typically included in the summary output:\n\nModel Information:\n\nThe name of the model.\nThe method used for estimation (e.g., maximum likelihood).\nThe distribution family (e.g., Gaussian, binomial, Poisson).\nThe link function used in the model (e.g., logit, identity, etc.).\nThe number of observations used in the model.\n\nModel Fit Statistics:\n\nLog-likelihood value.\nAIC (Akaike Information Criterion) and/or BIC (Bayesian Information Criterion).\nDeviance and Pearson chi-square statistics.\nDispersion parameter (if applicable).\n\nCoefficients:\n\nEstimated coefficients for each predictor variable.\nStandard errors of the coefficients.\nz-scores and p-values for testing the significance of the coefficients.\n\nConfidence Intervals:\n\nConfidence intervals for each coefficient, often at a default level like 95%.\n\nHypothesis Testing:\n\nHypothesis tests for the coefficients, typically with null hypothesis being that the coefficient is zero.\n\nGoodness of Fit:\n\nLikelihood-ratio test for overall model fit.\nTests for assessing the contribution of individual predictors to the model.\n\nDiagnostic Information:\n\nInformation about model assumptions and diagnostics, depending on the type of GLM and the method used.\n\nResiduals:\n\nInformation about residuals, which can include measures like deviance residuals, Pearson residuals, etc.\n\n\nRefer to the official documentation for the most accurate and up-to-date information about the summary output for your specific use case.\n\n\nConclusion\nStatsmodels is a powerful Python library for statistical modeling and hypothesis testing, making it an excellent transition for R users or Python users who like R to solve certain problems. It offers a familiar syntax and functionality for regression, ANOVA, and more. Its integration with Python‚Äôs data analysis ecosystem, like pandas, allows seamless data manipulation. With support for various statistical methods and a comprehensive summary output, Statsmodels facilitates effortless migration from R, enabling R users to harness its capabilities in a Python environment."
  },
  {
    "objectID": "WorkInProgress/streamlit.html",
    "href": "WorkInProgress/streamlit.html",
    "title": "Streamlit",
    "section": "",
    "text": "Introduction\nStreamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps - so let‚Äôs get started!"
  },
  {
    "objectID": "WorkInProgress/AirflowArticle.html",
    "href": "WorkInProgress/AirflowArticle.html",
    "title": "Airflow",
    "section": "",
    "text": "Introduction\nIn my present work we fetch continously data from the field. This data is used to create models that allow us to predict the behavior of the field, validate the models and take actions. This data is stored in a database and we have a web application that allows us to visualize the data."
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üëã Hi there",
    "text": "üëã Hi there\nI am Bernardo, an aeronautical engineer with a BSc in Aeronautical Engineering and a MSc in Data Science. My passion lies in both coding and engineering. I am from Zurich üá®üá≠ and have over seven years of experience working as an aeronautical engineer and about three years as a data scientist.\nI would like to invite you go through my CV and blog to learn more about me ü§ì. Please don‚Äôt hesitate to contact me if you have any questions or would like to collaborate on a project ü§ù."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üë®üèΩ‚Äçüíª Experience",
    "text": "üë®üèΩ‚Äçüíª Experience\nExplore my journey through various industries and roles in the world of data science and beyond! From predictive maintenance in the railway sector üöÇüöÇüöÇ, to crafting models for turnover prediction in the food industry üçîüçïüçù and upsell models banking üíµüí∞üí≥, my experiences have been diverse and impactful. Delve into my work with Gaussian Mixture Models, Multi-Output Regression, OCR tools, and more. For a detailed account of my professional voyage, head to Section CV."
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üèùÔ∏è üèôÔ∏è üíª üì∫ Hobbies",
    "text": "üèùÔ∏è üèôÔ∏è üíª üì∫ Hobbies\nI love to travel üõ©Ô∏è üöÇ ‚õ∞Ô∏è üèùÔ∏è üèôÔ∏è to code üíª and binge watch üì∫ in my free time. In particular I discovered a passion for web development and this blog ü§ìüòé."
  },
  {
    "objectID": "index.html#tech-stack",
    "href": "index.html#tech-stack",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üíª Tech Stack:",
    "text": "üíª Tech Stack:"
  },
  {
    "objectID": "index.html#github-trophies",
    "href": "index.html#github-trophies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üèÜ GitHub Trophies",
    "text": "üèÜ GitHub Trophies"
  },
  {
    "objectID": "index.html#wisdom-of-the-day",
    "href": "index.html#wisdom-of-the-day",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üßê Wisdom of the Day",
    "text": "üßê Wisdom of the Day"
  },
  {
    "objectID": "index.html#farewell-message",
    "href": "index.html#farewell-message",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üôåüèΩ Farewell message",
    "text": "üôåüèΩ Farewell message\nsee you soon"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\nDatacamp Airflow Course\n\n\n\n\n\n\n\nApache Airflow\n\n\nDAG\n\n\nPython\n\n\nWorkflow\n\n\nETL\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\n55 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "I am a former aeronautical engineer with 3 years of experience as a data scientist and a recent graduate of MSc in Data Science (summer 2023). During my 7 years as an aeronautical engineer, I managed complex interdisciplinary projects in aircraft structure development and certification. As a data scientist, I developed and maintained machine learning models, demonstrating a passion for learning and innovation."
  },
  {
    "objectID": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "href": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "title": "Curriculum Vitae",
    "section": "Stadler Rail AG, Data Scientist (06/2023 ‚Äì present)",
    "text": "Stadler Rail AG, Data Scientist (06/2023 ‚Äì present)\n\nDeveloped machine learning models for predictive maintenance, e.g., door systems, compressor systems, HVAC systems.\nContributed to projects developing data-driven solutions for optimizing rolling stock maintenance processes.\nEstablished CI/CD pipeline for modeling activities and created Python packages for internal use.\n\nTechnologies: Gitlab, Gitlab CI/CD, Sklearn, FastAPI (Python, SQL, API)"
  },
  {
    "objectID": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "href": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "title": "Curriculum Vitae",
    "section": "Prognolite AG, Data Scientist (10/2022 ‚Äì 03/2023)",
    "text": "Prognolite AG, Data Scientist (10/2022 ‚Äì 03/2023)\n\nDeveloped regression models predicting turnover using customer data and external factors like weather and holidays.\nCreated multi-output regression models to predict menu sales based on customer data and external factors.\n\nTechnologies: Github, Tidymodels, Sklearn, Random Forest, XGBoost, Cubist (R, Python)"
  },
  {
    "objectID": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "href": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "title": "Curriculum Vitae",
    "section": "Crealogix AG, Data Scientist (09/2021 ‚Äì 09/2022)",
    "text": "Crealogix AG, Data Scientist (09/2021 ‚Äì 09/2022)\n\nDeveloped prospect and upsell models.\nExtracted information from fact-sheets using customized OCR.\nCreated user journeys for chatbot solutions using IBM Watson and Spacy.\n\nTechnologies: Sklearn, Spacy, Random Forest, XGBoost, RMarkdown (Python, SQL, R)"
  },
  {
    "objectID": "cv.html#bucher-leichtbau-ag-012014-092021",
    "href": "cv.html#bucher-leichtbau-ag-012014-092021",
    "title": "Curriculum Vitae",
    "section": "Bucher Leichtbau AG (01/2014 ‚Äì 09/2021)",
    "text": "Bucher Leichtbau AG (01/2014 ‚Äì 09/2021)\n\nCompliance Verification Engineer (10/2017 ‚Äì 09/2021)\n\nAdvised and supported all departments of Bucher Leichtbau AG in initial airworthiness certification matters.\nIndependently verified documentation for ‚Äúminor‚Äù and ‚Äúmajor changes‚Äù against certification requirements.\n\nTechnologies: FEMAP, Mechanical Engineering\n\n\nCertification Engineer (01/2014 ‚Äì 10/2017)\n\nConducted FEM calculations/verification and prepared internal and external test reports.\nCommunicated with customers, official bodies, and authorities.\nConsulted development department for design optimization.\n\nTechnologies: FEMAP, Mechanical Engineering"
  },
  {
    "objectID": "tutorials/DC_airflow.html",
    "href": "tutorials/DC_airflow.html",
    "title": "Datacamp Airflow Course",
    "section": "",
    "text": "Apache Airflow is a platform to program workflows (general), including the creation, scheduling, and monitoring of said workflows. Airflow can use various tools and languages, but the actual workflow code is written with Python. Airflow implements workflows as DAGs, or Directed Acyclic Graphs. Airflow can be accessed and controlled via code, via the command-line, or via a built-in web interface.\n\n\nBefore we can really discuss Airflow, we need to talk about workflows. A workflow is a set of steps to accomplish a given data engineering task. These can include any given task, such as downloading a file, copying data, filtering information, writing to a database, and so forth. A workflow is of varying levels of complexity. Some workflows may only have 2 or 3 steps, while others consist of hundreds of components.\n\n\n\nWorkflow example containing 5 steps\n\n\nThe complexity of a workflow is completely dependent on the needs of the user. We show an example of a possible workflow to the right. It‚Äôs important to note that we‚Äôre defining a workflow here in a general data engineering sense. This is an informal definition to introduce the concept. As you‚Äôll see later, workflow can have specific meaning within specific tools.\n\n\n\nAirflow is a platform to program workflows (general), including the creation, scheduling, and monitoring of said workflows.\nAirflow can use various tools and languages, but the actual workflow code is written with Python. Airflow implements workflows as DAGs, or Directed Acyclic Graphs. We‚Äôll discuss exactly what this means throughout this course, but for now think of it as a set of tasks and the dependencies between them. Airflow can be accessed and controlled via code, via the command-line, or via a built-in web interface. We‚Äôll look at all these options later on.\n\n\n\nA DAG stands for a Directed Acyclic Graph. In Airflow, this represents the set of tasks that make up your workflow. It consists of the tasks and the dependencies between tasks.\n\n\n\nExample of DAG consisting of five set of tasks\n\n\nDAGs are created with various details about the DAG, including the name, start date, owner, email alerting options, etc.\n\n\n\nWe will go into further detail in the next lesson but a very simple DAG is defined using the following code. A new DAG is created with the dag_id of etl_pipeline and a default_args dictionary containing a start_date for the DAG.\netl_dag = DAG(\n    dag_id = 'etl_pipeline',    \n    default_args = {\"start_date\": \"2020-01-08\"},\n)\n\n\n\n\n\n\nNote that within any Python code, this is referred to via the variable identifier, etl_dag, but within the Airflow shell command, you must use the dag_id.\n\n\n\n\n\n\nTo get started, let‚Äôs look at how to run a component of an Airflow workflow. These components are called tasks and simply represent a portion of the workflow. We‚Äôll go into further detail in later chapters. There are several ways to run a task, but one of the simplest is using the airflow run shell command.\nairflow run <dag_id> <task_id> <start_date>\nAirflow run takes three arguments, a dag_id, a task_id, and a start_date. All of these arguments have specific meaning and will make more sense later in the course. For our example, we‚Äôll use a dag_id of example-etl, a task named download-file, and a start date of 2020-01-10. This task would simply download a specific file, perhaps a daily update from a remote source. Our command as such is airflow run example-etl download-file 2020-01-10. This will then run the specified task within Airflow.\n\n\n\nWe‚Äôve looked at Airflow and some of the basic aspects of why you‚Äôd use it. We‚Äôve also looked at how to run a task within Airflow from the command-line. Let‚Äôs practice what we‚Äôve learned.\n\n\nYou‚Äôve just started looking at using Airflow within your company and would like to try to run a task within the Airflow platform. You remember that you can use the airflow run command to execute a specific task within a workflow.\n\n\n\n\n\n\nNote that an error while using airflow run will return airflow.exceptions.AirflowException: on the last line of output.\n\n\n\nAn Airflow DAG is set up for you with a dag_id of etl_pipeline. The task_id is download_file and the start_date is 2020-01-08. All other components needed are defined for you.\nWhich command would you enter in the console to run the desired task?\n\nairflow run dag task 2020-01-08\n\nairflow run etl_pipeline task 2020-01-08\n\nairflow run etl_pipeline download_file 2020-01-08\n\n\n\n\nWhile researching how to use Airflow, you start to wonder about the airflow command in general. You realize that by simply running airflow you can get further information about various sub-commands that are available.\nWhich of the following is NOT an Airflow sub-command?\n\nlist_dags\nedit_dag\ntest\nscheduler"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-in-airflow",
    "href": "tutorials/DC_airflow.html#dag-in-airflow",
    "title": "Datacamp Airflow Course",
    "section": "DAG in Airflow",
    "text": "DAG in Airflow\nAs we‚Äôre working with Airflow, let‚Äôs look at its implementation of the DAG concept. Within Airflow, DAGs are written in Python, but can use components written in other languages or technologies. This means we‚Äôll define the DAG using Python, but we could include Bash scripts, other executables, Spark jobs, and so on. Airflow DAGs are made up of components to be executed, such as operators, sensors, etc. Airflow typically refers to these as tasks. We‚Äôll cover these in much greater depth later on, but for now think of a task as a thing within the workflow that needs to be done. Airflow DAGs contain dependencies that are defined, either explicitly or implicitly. These dependencies define the execution order so Airflow knows which components should be run at what point within the workflow. For example, you would likely want to copy a file to a server prior to trying to import it to a database."
  },
  {
    "objectID": "tutorials/DC_airflow.html#define-a-dag",
    "href": "tutorials/DC_airflow.html#define-a-dag",
    "title": "Datacamp Airflow Course",
    "section": "Define a DAG",
    "text": "Define a DAG\nLet‚Äôs look at defining a simple DAG within Airflow. When defining the DAG in Python, you must first import the DAG object from airflow dot models. Once imported, we create a default arguments dictionary consisting of attributes that will be applied to the components of our DAG. These attributes are optional, but provide a lot of power to define the runtime behavior of Airflow.\nfrom airflow.models import DAG\nfrom datetime import datetime\n\ndefault_arguments = {\n    'owner'         : 'jdoe',\n    'email'         : 'jdoe@email.com', \n    'start_date'    : datetime(2020, 1, 20)\n}\n\netl_dag = DAG(\n    'etl_workflow', \n    default_args = default_arguments\n)\nHere we define the owner name as jdoe, an email address for any alerting, and specify the start date of the DAG. The start date represents the earliest datetime that a DAG could be run. Finally, we define our DAG object with the first argument using a name for the DAG, etl underscore workflow, and assign the default arguments dictionary to the default underscore args argument. There are many other optional configurations we will use later on.\n\n\n\n\n\n\nNote that the entire DAG is assigned to a variable called etl underscore dag. This will be used later when defining the components of the DAG, but the variable name etl underscore dag does not actually appear in the Airflow interfaces. Note, DAG is case sensitive in Python code."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-on-the-command-line",
    "href": "tutorials/DC_airflow.html#dags-on-the-command-line",
    "title": "Datacamp Airflow Course",
    "section": "DAGs on the command line",
    "text": "DAGs on the command line\nWhen working with DAGs (and Airflow in general), you‚Äôll often want to use the airflow command line tool. The airflow command line program contains many subcommands that handle various aspects of running Airflow. You‚Äôve used a couple of these already in previous exercises. Use the\nairflow -h\ncommand for help and descriptions of the subcommands. Many of these subcommands are related to DAGs. You can use the\nairflow list_dags\noption to see all recognized DAGs in an installation. When in doubt, try a few different commands to find the information you‚Äôre looking for."
  },
  {
    "objectID": "tutorials/DC_airflow.html#command-line-vs-python",
    "href": "tutorials/DC_airflow.html#command-line-vs-python",
    "title": "Datacamp Airflow Course",
    "section": "Command line vs Python",
    "text": "Command line vs Python\nYou may be wondering when to use the Airflow command line tool vs writing Python.\n\nCommand line vs Python\n\n\nCommand line\nPython\n\n\n\n\nStart Airflow processes\nCreate a DAGs\n\n\nManually run DAGs / tasks\nEdit individual prop of DAG\n\n\nReview logging information\n\n\n\n\nIn general, the airflow command line program is used to start Airflow processes (ie, webserver or scheduler), manually run DAGs or tasks, and review logging information. Python code itself is usually used in the creation and editing of a DAG, not to mention the actual data processing code itself."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-1",
    "href": "tutorials/DC_airflow.html#exercises-1",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Defining a simple DAG\nYou‚Äôve spent some time reviewing the Airflow components and are interested in testing out your own workflows. To start you decide to define the default arguments and create a DAG object for your workflow.\nThe DateTime object has been imported for you.\n\nImport the Airflow DAG object. Note that it is case-sensitive.\nDefine the default_args dictionary with a key owner and a value of ‚Äòdsmith‚Äô. Add a start_date of January 14, 2020 to default_args using the value 1 for the month of January. Add a retries count of 2 to default_args.\nInstantiate the DAG object to a variable called etl_dag with a DAG named example_etl. Add the default_args dictionary to the appropriate argument.\n\n\n# Import the DAG object\nfrom datetime import datetime\nfrom airflow.models import DAG\n\n# Define the default_args dictionary\ndefault_args = {\n  'owner'       : 'dsmith',\n  'start_date'  : datetime(2020, 1, 14),\n  'retries'     : 2\n}\n\n# Instantiate the DAG object\netl_dag = DAG(\n    'example_etl', \n    default_args=default_args\n)\n\n/Users/bernardo/miniforge3/envs/blog/lib/python3.11/site-packages/airflow/configuration.py:751 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.\n\n\n\n\n\nExercise 2: Working with DAGs and the Airflow shell\nWhile working with Airflow, sometimes it can be tricky to remember what DAGs are defined and what they do. You want to gain some further knowledge of the Airflow shell command so you‚Äôd like to see what options are available.\nMultiple DAGs are already defined for you. How many DAGs are present in the Airflow system from the command-line?\n\n!airflow dags list\n\n/Users/bernardo/miniforge3/envs/blog/lib/python3.11/site-packages/airflow/configuration.py:751 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.\n\nPlease confirm database upgrade (or wait 4 seconds to skip it). Are you sure? [y/N]\nNo data found"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-dags",
    "href": "tutorials/DC_airflow.html#dags-view-dags",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view DAGs",
    "text": "DAGs view DAGs\nIt provides a quick status of the number of DAGs / workflows available.\n\n\nIt shows us the schedule for the DAG (in date or cron format).\n\nWe can see the owner of the DAG.\n\nwhich of the most recent tasks have run,\n\nwhen the last run started,\n\nand the last three DAG runs.\n\nThe links area on the right gives us quick access to many of the DAG specific views.\n\nDon‚Äôt worry about those for now - instead we‚Äôll click on the ‚Äúexample_dag‚Äù link which takes us to our DAG detail page."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-detail-view",
    "href": "tutorials/DC_airflow.html#dag-detail-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG detail view",
    "text": "DAG detail view\nThe DAG detail view gives us specific access to information about the DAG itself, including several views of information (Graph, Tree, and Code) illustrating the tasks and dependencies in the code. We also get access to the Task duration, task tries, timings, a Gantt chart view, and specific details about the DAG. We have the ability to trigger the DAG (to start), refresh our view, and delete the DAG if we desire. The detail view defaults to the Tree view, showing the specific named tasks, which operators are in use, and any dependencies between tasks. The circles in front of the words represent the state of the task / DAG. In the case of our specific DAG, we see that we have one task called generate_random_number."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-graph-view",
    "href": "tutorials/DC_airflow.html#dag-graph-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG graph view",
    "text": "DAG graph view\nThe DAG graph view arranges the tasks and dependencies in a chart format - this provides another view into the flow of the DAG. You can see the operators in use and the state of the tasks at any point in time. The tree and graph view provide different information depending on what you‚Äôd like to know. Try moving between them when examining a DAG to obtain further details. For this view we again see that we have a task called generate_random_number. We can also see that it is of the type BashOperator in the middle left of the image."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-code-view",
    "href": "tutorials/DC_airflow.html#dag-code-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG code view",
    "text": "DAG code view\nThe DAG code view does exactly as it sounds - it provides a copy of the Python code that makes up the DAG. The code view provides easy access to exactly what defines the DAG without clicking in various portions of the UI. As you use Airflow, you‚Äôll determine which tools work best for you. It is worth noting that the code view is read-only. Any DAG code changes must be done via the actual DAG script. In this view, we can finally see the code making up the generate_random_number task and that it runs the bash command echo $RANDOM."
  },
  {
    "objectID": "tutorials/DC_airflow.html#logs",
    "href": "tutorials/DC_airflow.html#logs",
    "title": "Datacamp Airflow Course",
    "section": "Logs",
    "text": "Logs\nThe Logs page, under the Browse menu option, provides troubleshooting and audit ability while using Airflow. This includes items such as starting the Airflow webserver, viewing the graph or tree nodes, creating users, starting DAGs, etc. When using Airflow, look at the logs often to become more familiar with the types of information included, and also what happens behind the scenes of an Airflow install.\n\n\n\n\n\n\nNote that you‚Äôll often refer to the Event type present on the Logs view when searching (such as graph, tree, cli scheduler)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#web-ui-vs-command-line",
    "href": "tutorials/DC_airflow.html#web-ui-vs-command-line",
    "title": "Datacamp Airflow Course",
    "section": "Web UI vs command line",
    "text": "Web UI vs command line\nIn most circumstances, you can choose between using the Airflow web UI or the command line tool based on your preference. The web UI is often easier to use overall. The command line tool may be simpler to access depending on settings (via SSH, etc.)"
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-2",
    "href": "tutorials/DC_airflow.html#exercises-2",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Examining DAGs with the Airflow UI\nYou‚Äôve become familiar with the basics of an Airflow DAG and the basics of interacting with Airflow on the command-line. Your boss would like you to show others on your team how to examine any available DAGs. In this instance, she would like to know which operator is NOT in use with the DAG called update_state, as your team is trying to verify the components used in production workflows.\nRemember that the Airflow UI allows various methods to view the state of DAGs. The Tree View lists the tasks and any ordering between them in a tree structure, with the ability to compress / expand the nodes. The Graph View shows any tasks and their dependencies in a graph structure, along with the ability to access further details about task runs. The Code view provides full access to the Python code that makes up the DAG.\nRemember to select the operator NOT used in this DAG.\n\n\nBashOperator\nPythonOperator\nJdbcOperator\nSimpleHttpOperator"
  },
  {
    "objectID": "tutorials/DC_airflow.html#bashoperator",
    "href": "tutorials/DC_airflow.html#bashoperator",
    "title": "Datacamp Airflow Course",
    "section": "BashOperator",
    "text": "BashOperator\nThe BashOperator executes a given Bash command or script.\nBashOperator(    \n    task_id='bash_example',    \n    bash_command='echo \"Example!\"',    \n    dag=ml_dag\n)\n\nBashOperator(    \n    task_id='bash_script_example',    \n    bash_command='runcleanup.sh',    \n    dag=ml_dag\n)\nThis command can be pretty much anything Bash is capable of that would make sense in a given workflow. The BashOperator requires three arguments: the task id which is the name that shows up in the UI, the bash command (the raw command or script), and the dag it belongs to. The BashOperator runs the command in a temporary directory that gets automatically cleaned up afterwards. It is possible to specify environment variables for the bash command to try to replicate running the task as you would on a local system. If you‚Äôre unfamiliar with environment variables, these are run-time settings interpreted by the shell. It provides flexibility while running scripts in a generalized way. The first example runs a bash command to echo Example exclamation mark to standard out. The second example uses a predefined bash script for its command, runcleanup.sh."
  },
  {
    "objectID": "tutorials/DC_airflow.html#bashoperator-examples",
    "href": "tutorials/DC_airflow.html#bashoperator-examples",
    "title": "Datacamp Airflow Course",
    "section": "BashOperator examples",
    "text": "BashOperator examples\nBefore using the BashOperator, it must be imported:\nfrom airflow.operators.bash_operator import BashOperator\n\n...\n\nexample_task = BashOperator(\n    task_id = 'example',\n    bash_command = 'echo 1',\n    dag = dag,\n)\nThe first example creates a BashOperator that takes a task_id, runs the bash command ‚Äúecho 1‚Äù, and assigns the operator to the dag. ::: {.callout-caution collapse=‚Äútrue‚Äù apperance=‚Äòsimple‚Äô} Note that we‚Äôve previously defined the dag in an earlier exercise. :::\nThe second example is a BashOperator to run a quick data cleaning operation using cat and awk.\nbash_task = BashOperator(\n    task_id = 'clean_addresses',\n    bash_command = 'cat addresses.txt | awk \"NF==10\" > cleaned.txt',\n    dag = dag,\n)\nDon‚Äôt worry if you don‚Äôt understand exactly what this is doing. This is a common scenario when running workflows - you may not know exactly what a command does, but you can still run it in a reliable way."
  },
  {
    "objectID": "tutorials/DC_airflow.html#operator-gotchas",
    "href": "tutorials/DC_airflow.html#operator-gotchas",
    "title": "Datacamp Airflow Course",
    "section": "Operator gotchas",
    "text": "Operator gotchas\nThere are some general gotchas when using Operators. The biggest is that individual operators are not guaranteed to run in the same location or environment. This means that just because one operator ran in a given directory with a certain setup, it does not necessarily mean that the next operator will have access to that same information. If this is required, you must explicitly set it up. You may need to set up environment variables, especially for the BashOperator.\n\nFor example, it‚Äôs common in bash to use the tilde character to represent a home directory. This is not defined by default in Airflow.\n\n\nAnother example of an environment variable could be AWS credentials, database connectivity details, or other information specific to running a script.\n\nFinally, it can also be tricky to run tasks with any form of elevated privilege. This means that any access to resources must be setup for the specific user running the tasks. If you‚Äôre uncertain what elevated privileges are, think of running a command as root or the administrator on a system."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-3",
    "href": "tutorials/DC_airflow.html#exercises-3",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExericse 1: Defining a BashOperator task\nThe BashOperator allows you to specify any given Shell command or script and add it to an Airflow workflow. This can be a great start to implementing Airflow in your environment.\nAs such, you‚Äôve been running some scripts manually to clean data (using a script called cleanup.sh) prior to delivery to your colleagues in the Data Analytics group. As you get more of these tasks assigned, you‚Äôve realized it‚Äôs becoming difficult to keep up with running everything manually, much less dealing with errors or retries. You‚Äôd like to implement a simple script as an Airflow operator.\nThe Airflow DAG analytics_dag is already defined for you and has the appropriate configurations in place.\n\nImport the BashOperator object.\nDefine a BashOperator called cleanup with the task_id of cleanup_task.\nUse the command cleanup.sh.\nAdd the operator to the DAG.\n\n# Import the BashOperator\nfrom airflow.operators.bash_operator import BashOperator\n\n# Define the BashOperator \ncleanup = BashOperator(\n    task_id='cleanup_task',\n    # Define the bash_command\n    bash_command='cleanup.sh',\n    # Add the task to the dag\n    dag=analytics_dag,\n)\n\n\nExercise 2: Multiple BashOperators\nAirflow DAGs can contain many operators, each performing their defined tasks.\nYou‚Äôve successfully implemented one of your scripts as an Airflow task and have decided to continue migrating your individual scripts to a full Airflow DAG. You now want to add more components to the workflow. In addition to the cleanup.sh used in the previous exercise you have two more scripts, consolidate_data.sh and push_data.sh. These further process your data and copy to its final location.\nThe DAG analytics_dag is available as before, and your cleanup task is still defined. The BashOperator is already imported.\n\nDefine a BashOperator called consolidate, to run consolidate_data.sh with a task_id of consolidate_task.\nAdd a final BashOperator called push_data, running push_data.sh and a task_id of pushdata_task.\n\n# Define a second operator to run the `consolidate_data.sh` script\nconsolidate = BashOperator(\n    task_id = 'consolidate_task',\n    bash_command = 'consolidate_data.sh',\n    dag = analytics_dag,\n)\n\n# Define a final operator to execute the `push_data.sh` script\npush_data = BashOperator(\n    task_id = 'pushdata_task',\n    bash_command = 'push_data.sh',\n    dag = analytics_dag,\n)"
  },
  {
    "objectID": "tutorials/DC_airflow.html#task-dependencies",
    "href": "tutorials/DC_airflow.html#task-dependencies",
    "title": "Datacamp Airflow Course",
    "section": "Task dependencies",
    "text": "Task dependencies\nTask dependencies in Airflow define an order of task completion. While not required, task dependencies are usually present. If task dependencies are not defined, task execution is handled by Airflow itself with no guarantees of order. Task dependencies are referred to as upstream or downstream tasks. An upstream task means that it must complete prior to any downstream tasks. Since Airflow 1.8, task dependencies are defined using the bitshift operators. The upstream operator is two greater-than symbols. The downstream operator is two less-than symbols."
  },
  {
    "objectID": "tutorials/DC_airflow.html#upstream-vs-downstream",
    "href": "tutorials/DC_airflow.html#upstream-vs-downstream",
    "title": "Datacamp Airflow Course",
    "section": "Upstream vs Downstream",
    "text": "Upstream vs Downstream\nIt‚Äôs easy to get confused on when to use an upstream or downstream operator. The simplest analogy is that upstream means before and downstream means after. This means that any upstream tasks would need to complete prior to any downstream ones."
  },
  {
    "objectID": "tutorials/DC_airflow.html#simple-task-dependency",
    "href": "tutorials/DC_airflow.html#simple-task-dependency",
    "title": "Datacamp Airflow Course",
    "section": "Simple task dependency",
    "text": "Simple task dependency\nLet‚Äôs look at a simple example involving two bash operators. We define our first task, and assign it to the variable task1. We then create our second task and assign it to the variable task2. Once each operator is defined and assigned to a variable, we can define the task order using the bitshift operators. In this case, we want to run task1 before task2.\n# Define the tasks\ntask1 = BashOperator(\n    task_id='first_task',                     \n    bash_command='echo 1',                     \n    dag=example_dag)\n\ntask2 = BashOperator(\n    task_id='second_task',                     \n    bash_command='echo 2',                     \n    dag=example_dag)\n    \n# Set first_task to run before second_task\ntask1 >> task2   # or task2 << task1\nThe most readable method for this is using the upstream operator, two greater-than symbols, as task1 upstream operator task2. ::: {.callout-caution collapse=‚Äútrue‚Äù apperance=‚Äòsimple‚Äô} Note that you could also define this in reverse using the downstream operator to accomplish the same thing. In this case, it‚Äôd be task2 two less-than symbols task1. :::"
  },
  {
    "objectID": "tutorials/DC_airflow.html#task-dependencies-in-the-airflow-ui",
    "href": "tutorials/DC_airflow.html#task-dependencies-in-the-airflow-ui",
    "title": "Datacamp Airflow Course",
    "section": "Task dependencies in the Airflow UI",
    "text": "Task dependencies in the Airflow UI\nLet‚Äôs take a look at what the Airflow UI shows for tasks and their dependencies. In this case, we‚Äôre looking at the graph view within the Airflow web interface. ::: {.callout-caution collapse=‚Äútrue‚Äù apperance=‚Äòsimple‚Äô} Note that in the task area, our two tasks, first_task and second_task, are both present, but there is no order to the task execution. This is the DAG prior to setting the task dependency using the bitshift operator. :::\n\nNow let‚Äôs look again at the view with a defined order via the bitshift operators. The view is similar but we can see the order of tasks indicated by the directed arrow between first underscore task and second underscore task."
  },
  {
    "objectID": "tutorials/DC_airflow.html#multiple-dependencies",
    "href": "tutorials/DC_airflow.html#multiple-dependencies",
    "title": "Datacamp Airflow Course",
    "section": "Multiple dependencies",
    "text": "Multiple dependencies\nDependencies can be as complex as required to define the workflow to your needs. We can chain a dependency, in this case setting task1 upstream of task2 upstream of task3 upstream of task4. The Airflow graph view shows a dependency view indicating this order.\ntask1 >> task2 >> task3 >> task4\n\nYou can also mix upstream and downstream bitshift operators in the same workflow. If we define task1 upstream of task2 then downstream of task3, we get a configuration different than what we might expect.\ntask1 >> task2 << task3\n\n##¬†or\ntask1 >> task2\ntask3 >> task2\n\nThis creates a DAG where first underscore task and third underscore task must finish prior to second underscore task. This means we could define the same dependency graph on two lines, in a possibly clearer form. task1 upstream of task2. task3 upstream of task2.\n\n\n\n\n\n\nNote that because we don‚Äôt require it, either task1 or task3 could run first depending on Airflow‚Äôs scheduling."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-4",
    "href": "tutorials/DC_airflow.html#exercises-4",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Define order of BashOperators\nNow that you‚Äôve learned about the bitshift operators, it‚Äôs time to modify your workflow to include a pull step and to include the task ordering. You have three currently defined components, cleanup, consolidate, and push_data.\nThe DAG analytics_dag is available as before and the BashOperator is already imported.\n\nDefine a BashOperator called pull_sales with a bash command of wget https://salestracking/latestinfo?json.\nSet the pull_sales operator to run before the cleanup task.\nConfigure consolidate to run next, using the downstream operator.\nSet push_data to run last using either bitshift operator.\n\n# Define a new pull_sales task\npull_sales = BashOperator(\n    task_id = 'pullsales_task',\n     bash_command = 'wget https://salestracking/latestinfo?json',\n    dag = analytics_dag,\n)\n\n# Set pull_sales to run prior to cleanup\npull_sales >> cleanup\n\n# Configure consolidate to run after cleanup\ncleanup >> consolidate\n\n# Set push_data to run last\nconsolidate >> push_data\n\n\nExercise 2: Determining the order of tasks\nWhile looking through a colleague‚Äôs workflow definition, you‚Äôre trying to decipher exactly in which order the defined tasks run. The code in question shows the following:\npull_data << initialize_process\npull_data >> clean >> run_ml_pipeline\ngenerate_reports << run_ml_pipeline\nOrder the tasks in the sequence defined by the bitshift code, with the first task to run on top and the last task to run on the bottom.\n\ninitialize_process\npull_data\nclean\nrun_ml_pipeline\ngenerate_reports\n\n\n\nExercise 3: Troubleshooting DAG dependencies\nYou‚Äôve created a DAG with intended dependencies based on your workflow but for some reason Airflow won‚Äôt load / execute the DAG. Try using the terminal to:\n\nList the DAGs.\nDecipher the error message.\nUse cat workspace/dags/codependent.py to view the Python code.\nDetermine which of the following lines should be removed from the Python code. You may want to consider the last line of the file.\n\n\n\ntask1 >> task2\ntask2 >> task3\ntask3 >> task1"
  },
  {
    "objectID": "tutorials/DC_airflow.html#pythonoperator",
    "href": "tutorials/DC_airflow.html#pythonoperator",
    "title": "Datacamp Airflow Course",
    "section": "PythonOperator",
    "text": "PythonOperator\nThe PythonOperator is similar to the BashOperator, except that it runs a Python function or callable method. Much like the BashOperator, it requires a taskid, a dag entry, and most importantly a python underscore callable argument set to the name of the function in question. You can also pass arguments or keyword style arguments into the Python callable as needed. Our first example shows a simple printme function that writes a message to the task logs.\nfrom airflow.operators.python_operator import PythonOperator\n\ndef printme():\n    print(\"This goes in the logs!\")\n\npython_task = PythonOperator(\n    task_id = 'simple_print',\n    python_callable = printme,\n    dag = example_dag,\n)\nWe must first import the PythonOperator from the airflow dot operators dot python underscore operator library. Afterwards, we create our function printme, which will write a quick log message when run. Once defined, we create the PythonOperator instance called python underscore task and add the necessary arguments."
  },
  {
    "objectID": "tutorials/DC_airflow.html#arguments",
    "href": "tutorials/DC_airflow.html#arguments",
    "title": "Datacamp Airflow Course",
    "section": "Arguments",
    "text": "Arguments\nThe PythonOperator supports adding arguments to a given task. This allows you to pass arguments that can then be passed to the Python function assigned to python callable. The PythonOperator supports both positional and keyword style arguments as options to the task. > For this course, we‚Äôll focus on using keyword arguments only for the sake of clarity.\nTo implement keyword arguments with the PythonOperator, we define an argument on the task called op_kwargs. This is a dictionary consisting of the named arguments for the intended Python function."
  },
  {
    "objectID": "tutorials/DC_airflow.html#op_kwargs-example",
    "href": "tutorials/DC_airflow.html#op_kwargs-example",
    "title": "Datacamp Airflow Course",
    "section": "op_kwargs example",
    "text": "op_kwargs example\nLet‚Äôs create a new function called sleep, which takes a length of time argument. It uses this argument to call the time dot sleep method. Once defined, we create a new task called sleep underscore task, with the taskid, dag, and python callable arguments added as before. This time we‚Äôll add our op underscore kwargs dictionary with the length of time variable and the value of 5.\n## Function to sleep (needs argument to be passed)\ndef sleep(length_of_time):\n    time.sleep(length_of_time)\n\nsleep_task = PythonOperator(\n    task_id = 'sleep',\n    python_callable = sleep,\n    ## Pass argument to callable\n    op_kwargs = {'length_of_time': 5},\n    dag = example_dag,\n)\n\n\n\n\n\n\nNote that the dictionary key must match the name of the function argument. If the dictionary contains an unexpected key, it will be passed to the Python function and typically cause an unexpected keyword argument error."
  },
  {
    "objectID": "tutorials/DC_airflow.html#emailoperator",
    "href": "tutorials/DC_airflow.html#emailoperator",
    "title": "Datacamp Airflow Course",
    "section": "EmailOperator",
    "text": "EmailOperator\nThere are many other operators available within the Airflow ecosystem. The primary operators are in the airflow.operators or airflow.contrib.operators libraries.\nAnother useful operator is the EmailOperator, which as expected sends an email from within an Airflow task. It can contain the typical components of an email, including HTML content and attachments. ::: {.callout-caution collapse=‚Äútrue‚Äù apperance=‚Äòsimple‚Äô} Note that the Airflow system must be configured with the email server details to successfully send a message. :::"
  },
  {
    "objectID": "tutorials/DC_airflow.html#emailoperator-example",
    "href": "tutorials/DC_airflow.html#emailoperator-example",
    "title": "Datacamp Airflow Course",
    "section": "EmailOperator example",
    "text": "EmailOperator example\nA quick example for sending an email would be sending a generated sales report upon completion of a workflow. We first must import the EmailOperator object from airflow dot operators dot email underscore operator. We can then create our EmailOperator instance with the task id, the to, subject, and content fields and a list of any files to attach.\nfrom airflow.operators.email_operator import EmailOperator\n\nemail_task = EmailOperator(\n    task_id = 'email_sales_report',\n    to = 'sales_manager@example.com',\n    subject = 'Automated Sales Report',\n    html_content = 'Attached is the latest sales report',\n    files = 'latest_sales.xlsx',\n    dag = example_dag\n)\n::: {.callout-caution collapse=‚Äútrue‚Äù apperance=‚Äòsimple‚Äô} Note that in this case we assume the file latest underscore sales dot xlsx was previously generated - later in the course we‚Äôll see how to verify that first. Finally we add it to our dag as usual."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-5",
    "href": "tutorials/DC_airflow.html#exercises-5",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Using the PythonOperator\nYou‚Äôve implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You‚Äôll implement a task to download and save a file to the system within Airflow.\nThe requests library is imported for you, and the DAG process_sales_dag is already defined.\n\nDefine a function called pull_file with two parameters, URL and savepath.\nUse the print() function and Python f-strings to write a message to the logs.\nImport the necessary object to use the Python Operator.\nCreate a new task assigned to the variable pull_file_task, with the id pull_file.\nAdd the pull_file(URL, savepath) function defined previously to the operator.\nDefine the arguments needed for the task.\n\n# Define the method\ndef pull_file(URL, savepath):\n    r = requests.get(URL)\n    with open(savepath, 'wb') as f:\n        f.write(r.content)    \n    # Use the print method for logging\n    print(f\"File pulled from {URL} and saved to {savepath}\")\n\n## Import the PythonOperator\nfrom airflow.operators.python_operator import PythonOperator\n\n# Create the task\npull_file_task = PythonOperator(\n    task_id = 'pull_file',\n    # Add the callable\n    python_callable = pull_file,\n    # Define the arguments\n    op_kwargs = {\n        'URL' : 'http://dataserver/sales.json', \n        'savepath' : 'latestsales.json',\n    },\n    dag = process_sales_dag\n)\n\n\nExercise 2: More PythonOperators\nTo continue implementing your workflow, you need to add another step to parse and save the changes of the downloaded file. The DAG process_sales_dag is defined and has the pull_file task already added. In this case, the Python function is already defined for you, parse_file(inputfile, outputfile).\nNote that often when implementing Airflow tasks, you won‚Äôt necessarily understand the individual steps given to you. As long as you understand how to wrap the steps within Airflow‚Äôs structure, you‚Äôll be able to implement a desired workflow.\n\nDefine the Python task to the variable parse_file_task with the id parse_file.\nAdd the parse_file(inputfile, outputfile) to the Operator.\nDefine the arguments to pass to the callable.\nAdd the task to the DAG.\n\n# Add another Python task\nparse_file_task = PythonOperator(\n    task_id='parse_file',\n    # Set the function to call\n    python_callable = parse_file,\n    # Add the arguments\n    op_kwargs = {\n        'inputfile':'latestsales.json', \n        'outputfile':'parsedfile.json',\n    },\n    # Add the DAG\n    dag = process_sales_dag,\n)\n    \n\n\nExercise 3: EmailOperator and dependencies\nNow that you‚Äôve successfully defined the PythonOperators for your workflow, your manager would like to receive a copy of the parsed JSON file via email when the workflow completes. The previous tasks are still defined and the DAG process_sales_dag is configured.\n\nImport the class to send emails.\nDefine the Operator and add the appropriate arguments (to, subject, files).\nSet the task order so the tasks run sequentially (Pull the file, parse the file, then email your manager).\n\n# Import the Operator\nfrom airflow.operators.email_operator import EmailOperator\n\n# Define the task\nemail_manager_task = EmailOperator(\n    task_id = 'email_manager',\n    to = 'manager@datacamp.com',\n    subject = 'Latest sales JSON',\n    html_content = 'Attached is the latest sales JSON file as requested.',\n    files = 'parsedfile.json',\n    dag = process_sales_dag,\n)\n\n# Set the order of tasks\npull_file_task >> parse_file_task >> email_manager_task"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-runs-view",
    "href": "tutorials/DC_airflow.html#dag-runs-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG Runs view",
    "text": "DAG Runs view\nWithin the Airflow UI, you can view all DAG runs under the Browse: DAG Runs menu option. This provides the assorted details about any DAGs that have run within the current Airflow instance.\n\nAs mentioned, you can view the state of a DAG run within this page, illustrating whether the DAG run was successful or not."
  },
  {
    "objectID": "tutorials/DC_airflow.html#schedule-details",
    "href": "tutorials/DC_airflow.html#schedule-details",
    "title": "Datacamp Airflow Course",
    "section": "Schedule details",
    "text": "Schedule details\nWhen scheduling a DAG, there are many attributes to consider depending on your scheduling needs. The start_date value specifies the first time the DAG could be scheduled. This is typically defined with a Python datetime object. The end_date represents the last possible time to schedule the DAG. max_tries represents how many times to retry before fully failing the DAG run. The schedule_interval represents how often to schedule the DAG for execution. There are many nuances to this which we‚Äôll cover in a moment."
  },
  {
    "objectID": "tutorials/DC_airflow.html#schedule-interval",
    "href": "tutorials/DC_airflow.html#schedule-interval",
    "title": "Datacamp Airflow Course",
    "section": "Schedule interval",
    "text": "Schedule interval\nThe schedule interval represents how often to schedule the DAG runs. The scheduling occurs between the start date and the potential end date. Note that this is not when the DAGs will absolutely run, but rather a minimum and maximum value of when they could be scheduled. The schedule interval can be defined by a couple methods - with a cron style syntax or via built-in presets."
  },
  {
    "objectID": "tutorials/DC_airflow.html#cron-syntax",
    "href": "tutorials/DC_airflow.html#cron-syntax",
    "title": "Datacamp Airflow Course",
    "section": "cron syntax",
    "text": "cron syntax\nThe cron syntax3 is the same as the format for scheduling jobs using the Unix cron tool.\n\nIt consists of five fields separated by a space, starting with the minute value (0 through 59), the hour (0 through 23), the day of the month (1 through 31), the month (1 through 12), and the day of week (0 through 6). An asterisk in any of the fields represents running for every interval (for example, an asterisk in the minute field means run every minute) A list of values can be given on a field via comma separated values."
  },
  {
    "objectID": "tutorials/DC_airflow.html#cron-examples",
    "href": "tutorials/DC_airflow.html#cron-examples",
    "title": "Datacamp Airflow Course",
    "section": "cron examples",
    "text": "cron examples\n0 12 * * *              # Run daily at Noon (12:00)\n* * 25 2 *              # Run once per minute, but only on February 25th\n0,15,30,45 * * * *      # Run every 15 minutes\nThe cron entry 0 12 asterisk asterisk asterisk means run daily at Noon (12:00) asterisk asterisk 25 2 asterisk represents running once per minute, but only on February 25th. 0 comma 15 comma 30 comma 45 asterisk asterisk asterisk asterisk means to run every 15 minutes."
  },
  {
    "objectID": "tutorials/DC_airflow.html#airflow-scheduler-presets",
    "href": "tutorials/DC_airflow.html#airflow-scheduler-presets",
    "title": "Datacamp Airflow Course",
    "section": "Airflow scheduler presets",
    "text": "Airflow scheduler presets\nAirflow has several presets, or shortcut syntax options representing often used time intervals.\n\n\n\nPreset\ncron equivalent\n\n\n\n\n@hour\n0 * * * *\n\n\n@daily\n0 0 * * *\n\n\n@weekly\n0 0 * * 0\n\n\n@monthly\n0 0 1 * *\n\n\n@yearly\n0 0 1 1 *\n\n\n\nThe @hourly preset means run once an hour at the beginning of the hour. It‚Äôs equivalent to 0 asterisk asterisk asterisk asterisk in cron. The @daily, @weekly, @monthly, and @yearly presets behave similarly.\n\n\n\n\n\n\nNote: ‚Äú0 12 * * ‚Äù != ‚Äú 12 * * *‚Äù\n‚Äú0 12 * * *‚Äù This cron expression specifies a specific minute and hour. It means the task should run at 12:00 PM (noon) every day. The 0 in the first position represents the minute, and 12 in the second position represents the hour.\n‚Äú* 12 * * *‚Äù This cron expression uses an asterisk in the minute position, which means the task will run every minute of the 12th hour (noon) of every day. In other words, it will run once every minute between 12:00 PM and 12:59 PM."
  },
  {
    "objectID": "tutorials/DC_airflow.html#special-presets",
    "href": "tutorials/DC_airflow.html#special-presets",
    "title": "Datacamp Airflow Course",
    "section": "Special presets",
    "text": "Special presets\nAirflow also has two special presets for schedule intervals.\n\n@none means don‚Äôt ever schedule the DAG and is used for manually triggered workflows.\n@once means to only schedule a DAG once.\n\n\n\n\n\n\n\nScheduling DAGs has an important nuance to consider. When scheduling DAG runs, Airflow will use the start date as the earliest possible value, but not actually schedule anything until at least one schedule interval has passed beyond the start date. Given a start_date of February 25, 2020 and a @daily schedule interval, Airflow would then use the date of February 26, 2020 for the first run of the DAG. This can be tricky to consider when adding new DAG schedules, especially if they have longer schedule intervals."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-6",
    "href": "tutorials/DC_airflow.html#exercises-6",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Schedule a DAG via Python\nYou‚Äôve learned quite a bit about creating DAGs, but now you would like to schedule a specific DAG on a specific day of the week at a certain time. You‚Äôd like the code include this information in case a colleague needs to reinstall the DAG to a different server.\nThe Airflow DAG object and the appropriate datetime methods have been imported for you.\n\nSet the start date of the DAG to November 1, 2019.\nConfigure the retry_delay to 20 minutes. You will learn more about the timedelta object in Chapter 3. For now, you just need to know it expects an integer value.\nUse the cron syntax to configure a schedule of every Wednesday at 12:30pm.\n\n# Update the scheduling arguments as defined\ndefault_args = {\n  'owner': 'Engineering',\n  'start_date': datetime(2019, 11, 1),\n  'email': ['airflowresults@datacamp.com'],\n  'email_on_failure': False,\n  'email_on_retry': False,\n  'retries': 3,\n  'retry_delay': timedelta(minutes=20)\n}\n\ndag = DAG(\n    'update_dataflows', \n    default_args=default_args, \n    schedule_interval = '30 12 * * 3',\n)\n\n\nExercise 2: Deciphering Airflow schedules\nGiven the various options for Airflow‚Äôs schedule_interval, you‚Äôd like to verify that you understand exactly how intervals relate to each other, whether it‚Äôs a cron format, timedelta object, or a preset.\n\nOrder the schedule intervals from least to greatest amount of time.\n\n\n\n\nExercise 3: Troubleshooting DAG runs\nYou‚Äôve scheduled a DAG called process_sales which is set to run on the first day of the month and email your manager a copy of the report generated in the workflow. The start_date for the DAG is set to February 15, 2020. Unfortunately it‚Äôs now March 2nd and your manager did not receive the report and would like to know what happened.\nUse the information you‚Äôve learned about Airflow scheduling to determine what the issue is.\n\n\nThe schedule_interval has not yet passed since the start_date.\nThe email_manager_task is not downstream of the other tasks.\nThe DAG run has an error.\nThe op_kwargs are incorrect for the EmailOperator."
  },
  {
    "objectID": "tutorials/DC_airflow.html#sensor-details",
    "href": "tutorials/DC_airflow.html#sensor-details",
    "title": "Datacamp Airflow Course",
    "section": "Sensor details",
    "text": "Sensor details\nAll sensors are derived from the airflow.sensors.base_ underscore _sensor_ underscore _operator class. There are some default arguments available to all sensors, including mode, poke_interval, and timeout. The mode tells the sensor how to check for the condition and has two options, poke or reschedule.\n\npoke: The default is poke, and means to continue checking until complete without giving up a worker slot.\nreschedule: Reschedule means to give up the worker slot and wait for another slot to become available.\n\nWe‚Äôll discuss worker slots in the next lesson, but for now consider a worker slot to be the capability to run a task.\n\npoke_interval: The poke_interval is used in the poke mode, and tells Airflow how often to check for the condition. This is should be at least 1 minute to keep from overloading the Airflow scheduler.\ntimeout: The timeout field is how long to wait (in seconds) before marking the sensor task as failed. To avoid issues, make sure your timeout is significantly shorter than your schedule interval.\n\n\n\n\n\n\n\nNote that as sensors are operators, they also include normal operator attributes such as task_id and dag."
  },
  {
    "objectID": "tutorials/DC_airflow.html#file-sensor",
    "href": "tutorials/DC_airflow.html#file-sensor",
    "title": "Datacamp Airflow Course",
    "section": "File sensor",
    "text": "File sensor\nA useful sensor is the FileSensor, found in the airflow.contrib.sensors library. The FileSensor checks for the existence of a file at a certain location in the file system. It can also check for any files within a given directory. A quick example is importing the FileSensor object, then defining a task called file underscore sensor underscore task. We set the task_id and dag entries as usual.\nfrom airflow.contrib.sensors.file_sensor import FileSensor\n\nfile_sensor_task = FileSensor(\n    task_id='file_sense',\n    filepath='salesdata.csv',\n    poke_interval=300,\n    dag=sales_report_dag,\n)\n    \ninit_sales_cleanup >> file_sensor_task >> generate_report\nThe filepath argument is set to salesdata.csv, looking for a file with this filename to exist before continuing. We set the poke_interval to 300 seconds, or to repeat the check every 5 minutes until true.\nFinally, we use the bitshift operators to define the sensor‚Äôs dependencies within our DAG. In this case, we must run init_sales_cleanup, then wait for the file_sensor_task to finish, then we run generate_report."
  },
  {
    "objectID": "tutorials/DC_airflow.html#other-sensors",
    "href": "tutorials/DC_airflow.html#other-sensors",
    "title": "Datacamp Airflow Course",
    "section": "Other sensors",
    "text": "Other sensors\nThere are many types of sensors available within Airflow.\n\nExternalTaskSensor: The ExternalTaskSensor waits for a task in a separate DAG to complete. This allows a loose connection to other workflow tasks without making any one workflow too complex.\nHttpSensor: The HttpSensor will request a web URL and allow you define the content to check for.\nSqlSensor: The SqlSensor runs a SQL query to check for content.\n\nMany other sensors are available in the airflow.sensors and airflow.contrib.sensors libraries."
  },
  {
    "objectID": "tutorials/DC_airflow.html#why-sensors",
    "href": "tutorials/DC_airflow.html#why-sensors",
    "title": "Datacamp Airflow Course",
    "section": "Why sensors?",
    "text": "Why sensors?\nYou may be wondering when to use a sensor vs an operator.\nFor the most part, you‚Äôll want to use a normal operator unless you have any of the following requirements:\n\nYou‚Äôre uncertain when a condition will be true.\nIf you know something will complete that day but it might vary by an hour or so, you can use a sensor to check until it is.\nIf you want to continue to check for a condition but not necessarily fail the entire DAG immediately. This provides some flexibility in defining your DAG.\nFinally, if you want to repeatedly run a check without adding cycles to your DAG, sensors are a good choice."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-7",
    "href": "tutorials/DC_airflow.html#exercises-7",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Sensors vs operators\nAs you‚Äôve just learned about sensors, you want to verify you understand what they have in common with normal operators and where they differ.\nMove each entry into the Sensors, Operators, or Both bucket.\n\n\n\nExercise 2: Sensory deprivation\nYou‚Äôve recently taken over for another Airflow developer and are trying to learn about the various workflows defined within the system. You come across a DAG that you can‚Äôt seem to make run properly using any of the normal tools.\nTry exploring the DAG for any information about what it might be looking for before continuing.\n\nThe DAG is waiting for the file salesdata_ready.csv to be present.\nThe DAG expects a response from the SimpleHttpOperator before starting.\npart1 needs a dependency added."
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-an-executor",
    "href": "tutorials/DC_airflow.html#what-is-an-executor",
    "title": "Datacamp Airflow Course",
    "section": "What is an executor?",
    "text": "What is an executor?\nIn Airflow, an executor is the component that actually runs the tasks defined within your workflows. Each executor has different capabilities and behaviors for running the set of tasks. Some may run a single task at a time on a local system, while others might split individual tasks among all the systems in a cluster. As mentioned in the previous lesson, this is often referred to as the number of worker slots available. We‚Äôll discuss some of these in more detail soon, but a few examples of executors are: - SequentialExecutor - LocalExecutor - CeleryExecutor\nThis is not an exhaustive list, and you can also create your own executor if required (though we won‚Äôt cover that in this course)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#sequentialexecutor",
    "href": "tutorials/DC_airflow.html#sequentialexecutor",
    "title": "Datacamp Airflow Course",
    "section": "SequentialExecutor",
    "text": "SequentialExecutor\nThe SequentialExecutor is the default execution engine for Airflow. It runs only a single task at a time. This means having multiple workflows scheduled around the same timeframe may cause things to take longer than expected. The SequentialExecutor is useful for debugging as it‚Äôs fairly simple to follow the flow of tasks and it can also be used with some integrated development environments (though we won‚Äôt cover that here).\nThe most important aspect of the SequentialExecutor is that while it‚Äôs very functional for learning and testing, it‚Äôs not really recommended for production due to the limitations of task resources."
  },
  {
    "objectID": "tutorials/DC_airflow.html#localexecutor",
    "href": "tutorials/DC_airflow.html#localexecutor",
    "title": "Datacamp Airflow Course",
    "section": "LocalExecutor",
    "text": "LocalExecutor\nThe LocalExecutor is another option for Airflow that runs entirely on a single system. It basically treats each task as a process on the local system, and is able to start as many concurrent tasks as desired / requested / and permitted by the system resources (ie, CPU cores, memory, etc). This concurrency is the parallelism of the system, and it is defined by the user in one of two ways - either unlimited, or limited to a certain number of simultaneous tasks.\nDefined intelligently, the LocalExecutor is a good choice for a single production Airflow system and can utilize all the resources of a given host system."
  },
  {
    "objectID": "tutorials/DC_airflow.html#celeryexecutor",
    "href": "tutorials/DC_airflow.html#celeryexecutor",
    "title": "Datacamp Airflow Course",
    "section": "CeleryExecutor",
    "text": "CeleryExecutor\nThe last executor we‚Äôll look at is the Celery executor. If you‚Äôre not familiar with Celery, it‚Äôs a general queuing system written in Python that allows multiple systems to communicate as a basic cluster. Using a CeleryExecutor, multiple Airflow systems can be configured as workers for a given set of workflows / tasks. You can add extra systems at any time to better balance workflows. The power of the CeleryExecutor is significantly more difficult to setup and configure. It requires a working Celery configuration prior to configuring Airflow, not to mention some method to share DAGs between the systems (ie, a git server, Network File System, etc). While it is more difficult to configure, the CeleryExecutor is a powerful choice for anyone working with a large number of DAGs and / or expects their processing needs to grow."
  },
  {
    "objectID": "tutorials/DC_airflow.html#determine-your-executor",
    "href": "tutorials/DC_airflow.html#determine-your-executor",
    "title": "Datacamp Airflow Course",
    "section": "Determine your executor",
    "text": "Determine your executor\nSometimes when developing Airflow workflows, you may want to know the executor being used. If you have access to the command line, you can determine this by: Looking at the appropriate airflow.cfg file. Search for the executor equal line, and it will specify the executor in use.\n\n\n\n\n\n\nNote that we haven‚Äôt discussed the airflow.cfg file in depth as we assume a configured Airflow instance in this course. The airflow.cfg file is where most of the configuration and settings for the Airflow instance are defined, including the type of executor."
  },
  {
    "objectID": "tutorials/DC_airflow.html#determine-your-executor-2",
    "href": "tutorials/DC_airflow.html#determine-your-executor-2",
    "title": "Datacamp Airflow Course",
    "section": "Determine your executor #2",
    "text": "Determine your executor #2\nYou can also determine the executor by running airflow list_dags from the command line. Within the first few lines, you should see an entry for which executor is in use (In this case, it‚Äôs the SequentialExecutor).\n\n! cat ~/airflow/airflow.cfg | grep \"executor = \"\n\nexecutor = SequentialExecutor"
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-8",
    "href": "tutorials/DC_airflow.html#exercises-8",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Determining the executor\nWhile developing your DAGs in Airflow, you realize you‚Äôre not certain the configuration of the system. Using the commands you‚Äôve learned, determine which of the following statements is true.\n\nThis system can run 12 tasks at the same time.\nThis system can run one task at a time.\nThis system can run as many tasks as needed at a time.\n\n\n! cat ~/airflow/airflow.cfg | grep \"executor = \"\n\nexecutor = SequentialExecutor\n\n\nThe airflow.cfg file is configured to use the SequentialExecutor -> one per time\n\n\nExercise 2: Executor implications\nYou‚Äôre learning quite a bit about running Airflow DAGs and are gaining some confidence at developing new workflows. That said, your manager has mentioned that on some days, the workflows are taking a lot longer to finish and asks you to investigate. She also mentions that the salesdata_ready.csv file is taking longer to generate these days and the time of day it is completed is variable.\nThis exercise requires information from the previous two lessons - remember the implications of the available arguments and modify the workflow accordingly.\n\n\n\n\n\n\nNote that for this exercise, you‚Äôre expected to modify one line of code, not add any extra code.\n\n\n\n\nDetermine the level of parallelism available on this system. You can do this by listing dags (airflow list_dags).\nLook at the source for the DAG file and fix which entry is causing the problem.\n\nairflow dags list\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='poke',\n    dag=report_dag\n)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck >> generate_report_task\nuse FileSensor with mode=‚Äòreschedule‚Äô instead of ‚Äòpoke‚Äô\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='reschedule',\n    dag=report_dag\n)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck >> generate_report_task"
  },
  {
    "objectID": "tutorials/DC_airflow.html#typical-issues",
    "href": "tutorials/DC_airflow.html#typical-issues",
    "title": "Datacamp Airflow Course",
    "section": "Typical issues‚Ä¶",
    "text": "Typical issues‚Ä¶\nThere are several common issues you may run across while working with Airflow - it helps to have an idea of what these might be and how best handle them. - The first common issue is a DAG or DAGs that won‚Äôt run on schedule. - The next is a DAG that simply won‚Äôt load into the system. - The last common scenario involves syntax errors.\nLet‚Äôs look at these more closely."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-wont-run-on-schedule",
    "href": "tutorials/DC_airflow.html#dag-wont-run-on-schedule",
    "title": "Datacamp Airflow Course",
    "section": "DAG won‚Äôt run on schedule",
    "text": "DAG won‚Äôt run on schedule\nThe most common reason why a DAG won‚Äôt run on schedule is the scheduler is not running. Airflow contains several components that accomplish various aspects of the system. The Airflow scheduler handles DAG run and task scheduling. If it is not running, no new tasks can run. You‚Äôll often see this error within the web UI if the scheduler component is not running.\n\nYou can easily fix this issue by running airflow scheduler from the command-line."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-wont-run-on-schedule-1",
    "href": "tutorials/DC_airflow.html#dag-wont-run-on-schedule-1",
    "title": "Datacamp Airflow Course",
    "section": "DAG won‚Äôt run on schedule",
    "text": "DAG won‚Äôt run on schedule\n\nAs we‚Äôve covered before, another common issue with scheduling is the scenario where at least one schedule interval period has not passed since either the start date or the last DAG run. There isn‚Äôt a specific fix for this, but you might want to modify the start date or schedule interval to meet your requirements.\nThe last scheduling issue you‚Äôll often see is related to what we covered in the previous lesson - the executor does not have enough free slots to run tasks. There are basically three ways to alleviate this problem - by changing the executor type to something capable of more tasks (LocalExecutor or CeleryExecutor), by adding systems or system resources (RAM, CPUs), or finally by changing the scheduling of your DAGs."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-wont-load",
    "href": "tutorials/DC_airflow.html#dag-wont-load",
    "title": "Datacamp Airflow Course",
    "section": "DAG won‚Äôt load",
    "text": "DAG won‚Äôt load\nYou‚Äôll often see an issue where a new DAG will not appear in your DAG view of the web UI or in the airflow list_dags output. The first thing to check is that the python file is in the expected DAGs folder or directory. You can determine the current DAGs folder setting by examining the airflow.cfg file. The line dags underscore folder will indicate where Airflow expects to find your Python DAG files.\n\n\n\n\n\n\n\nNote that the folder path must be an absolute path."
  },
  {
    "objectID": "tutorials/DC_airflow.html#syntax-errors",
    "href": "tutorials/DC_airflow.html#syntax-errors",
    "title": "Datacamp Airflow Course",
    "section": "Syntax errors",
    "text": "Syntax errors\nProbably the most common reason a DAG workflow won‚Äôt appear in your DAG list is one or more syntax errors in your python code. These are sometimes difficult to find, especially in an editor not setup for Python / Airflow (such as a base Vim install). I tend to prefer using Vim with some Python tools loaded, or VSCode but it‚Äôs really up to your preference. There are two quick methods to check for these issues - airflow list_dags, and running your DAG script with python."
  },
  {
    "objectID": "tutorials/DC_airflow.html#airflow-list_dags",
    "href": "tutorials/DC_airflow.html#airflow-list_dags",
    "title": "Datacamp Airflow Course",
    "section": "airflow list_dags",
    "text": "airflow list_dags\nThe first is to run airflow space list underscore dags.\n\nAs we‚Äôve seen before, Airflow will output some debugging information and the list of DAGs it‚Äôs processed. If there are any errors, those will appear in the output, helping you to troubleshoot further."
  },
  {
    "objectID": "tutorials/DC_airflow.html#running-the-python-interpreter",
    "href": "tutorials/DC_airflow.html#running-the-python-interpreter",
    "title": "Datacamp Airflow Course",
    "section": "Running the Python interpreter",
    "text": "Running the Python interpreter\nAnother method to verify Python syntax is to run the actual python3 interpreter against the file. You won‚Äôt see any output normally as there‚Äôs nothing for the interpreter to do, but it can check for any syntax errors in your code. If there are errors, you‚Äôll get an appropriate error message. If there are no errors, you‚Äôll be returned to the command prompt."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-9",
    "href": "tutorials/DC_airflow.html#exercises-9",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: DAGs in the bag\nYou‚Äôve taken over managing an Airflow cluster that you did not setup and are trying to learn a bit more about the system. Which of the following is true?\n\n\nThe DAG is scheduled for hourly processing.\nThe Airflow user does not have proper permissions.\nThe dags_folder is set to /home/repl/workspace/dags.\n\n\n\nExercise 2: Missing DAG\nYour manager calls you before you‚Äôre about to leave for the evening and wants to know why a new DAG workflow she‚Äôs created isn‚Äôt showing up in the system. She needs this DAG called execute_report to appear in the system so she can properly schedule it for some tests before she leaves on a trip.\nAirflow is configured using the ~/airflow/airflow.cfg file.\n\nExamine the DAG for any errors and fix those.\nDetermine if the DAG has loaded after fixing the errors.\nIf not, determine why the DAG has not loaded and fix the final issue.\n\nfrom airflow.models import DAG\n# from airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='poke',\n    dag=report_dag)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck >> generate_report_task\nremove the comment from the line 1\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='poke',\n    dag=report_dag)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck >> generate_report_task"
  },
  {
    "objectID": "tutorials/DC_airflow.html#sla-misses",
    "href": "tutorials/DC_airflow.html#sla-misses",
    "title": "Datacamp Airflow Course",
    "section": "SLA Misses",
    "text": "SLA Misses\nTo view any given SLA miss, you can access it in the web UI, via the Browse: SLA Misses link. It provides you general information about what task missed the SLA and when it failed. It also indicates if an email has been sent when the SLA failed."
  },
  {
    "objectID": "tutorials/DC_airflow.html#defining-slas",
    "href": "tutorials/DC_airflow.html#defining-slas",
    "title": "Datacamp Airflow Course",
    "section": "Defining SLAs",
    "text": "Defining SLAs\nThere are several ways to define an SLA but we‚Äôll only look at two for this course. The first is via an sla argument on the task itself. This takes a timedelta object with the amount of time to pass.\ntask1 = BashOperator(\n    task_id = 'sla_task',\n    bash_command = 'runcode.sh',\n    sla = timedelta(seconds = 30),\n    dag = dag,\n)\nThe second way is using the default_args dictionary and defining an sla key.\ndefault_args = {\n    'sla': timedelta(minutes=20),\n    'start_date': datetime(2020,2,20)\n},\n\ndag = DAG(\n    'sla_dag', \n    default_args = default_args\n)\nThe dictionary is then passed into the default_args argument of the DAG and applies to any tasks internally."
  },
  {
    "objectID": "tutorials/DC_airflow.html#timedelta-object",
    "href": "tutorials/DC_airflow.html#timedelta-object",
    "title": "Datacamp Airflow Course",
    "section": "timedelta object",
    "text": "timedelta object\nWe haven‚Äôt covered the timedelta object yet so let‚Äôs look at some of the details. It‚Äôs found in the datetime library, along with the datetime object. Most easily accessed with an import statement of from datetime import timedelta. Takes arguments of days, seconds, minutes, hours, and weeks.\ntimedelta(seconds=30)\ntimedelta(weeks=2)\ntimedelta(days=4, hours=10, minutes=20, seconds=30)\nIt also has milliseconds and microseconds available, but those wouldn‚Äôt apply to Airflow. To create the object, you simply call timedelta with the argument or arguments you wish to reference. To create a 30 second time delta, call it with seconds equals 30. Or weeks equals 2. Or you can combine it into a longer mix of any of the arguments you wish (in this case, 4 days, 10 hours, 20 minutes, and 30 seconds)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#general-reporting",
    "href": "tutorials/DC_airflow.html#general-reporting",
    "title": "Datacamp Airflow Course",
    "section": "General reporting",
    "text": "General reporting\nFor reporting purposes you can use email alerting built into Airflow. There are a couple ways to do this. Airflow has built-in options for sending messages:\n\nsuccess\nfailure\nerror\nretry\n\nThese are handled via keys in the default_args dictionary that gets passed on DAG creation. The required component is the list of emails assigned to the email key. Then there are boolean options for email underscore on underscore failure, email underscore on underscore retry, and email underscore on underscore success.\ndefault_args = {\n    'email': ['address@domain.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'email_on_success': True,\n    ...\n}\nIn addition, we‚Äôve already looked at the EmailOperator earlier but this is useful for sending emails outside of one of the defined Airflow options. Note that sending an email does require configuration within Airflow that is outside the scope of this course. The Airflow documentation provides information on how to set up the global email configuration."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-10",
    "href": "tutorials/DC_airflow.html#exercises-10",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Defining an SLA\nYou‚Äôve successfully implemented several Airflow workflows into production, but you don‚Äôt currently have any method of determining if a workflow takes too long to run. After consulting with your manager and your team, you decide to implement an SLA at the DAG level on a test workflow.\nAll appropriate Airflow libraries have been imported for you.\n\nImport the timedelta object.\nDefine an SLA of 30 minutes.\nAdd the SLA to the DAG.\n\n# Import the timedelta object\nfrom datetime import timedelta\n\n# Create the dictionary entry\ndefault_args = {\n  'start_date': datetime(2020, 2, 20),\n  'sla': timedelta(minutes=30),\n}\n\n# Add to the DAG\ntest_dag = DAG(\n  'test_workflow',\n  default_args=default_args,\n  schedule_interval='@None',\n)\n\n\nExercise 2: Defining a task SLA\nAfter completing the SLA on the entire workflow, you realize you really only need the SLA timing on a specific task instead of the full workflow.\nThe appropriate Airflow libraries are imported for you.\n\nImport the timedelta object.\nAdd a 3 hour SLA to the task object\n\n# Import the timedelta object\nfrom datetime import timedelta\n\ntest_dag = DAG(\n    'test_workflow', \n    start_date=datetime(2020,2,20), \n    schedule_interval='@None'\n)\n\n# Create the task with the SLA\ntask1 = BashOperator(\n    task_id = 'first_task',\n    sla = timedelta(hours = 3),\n    bash_command = 'initialize_data.sh',\n    dag = test_dag\n)\n\n\nExercise 3: Generate and email a report\nAirflow provides the ability to automate almost any style of workflow. You would like to receive a report from Airflow when tasks complete without requiring constant monitoring of the UI or log files. You decide to use the email functionality within Airflow to provide this message.\nAll the typical Airflow components have been imported for you, and a DAG is already defined as dag.\n\nDefine the proper operator for the email_report task.\nFill the missing details for the Operator. Use the file named monthly_report.pdf.\nSet the email_report task to occur after the generate_report task.\n\n# Define the email task\nemail_report = EmailOperator(\n        task_id = 'email_report',\n        to = 'airflow@datacamp.com',\n        subject = 'Airflow Monthly Report',\n        html_content = \"\"\"\n        Attached is your monthly workflow report - please refer to it for more detail\n        \"\"\",\n        files = ['monthly_report.pdf'],\n        dag = report_dag,\n)\n\n# Set the email task to run after the report is generated\nemail_report << generate_report\n\n\nExercise 4: Adding status emails\nYou‚Äôve worked through most of the Airflow configuration for setting up your workflows, but you realize you‚Äôre not getting any notifications when DAG runs complete or fail. You‚Äôd like to setup email alerting for the success and failure cases, but you want to send it to two addresses.\n\nEdit the execute_report_dag.py workflow.\nAdd the emails airflowalerts@datacamp.com and airflowadmin@datacamp.com to the appropriate key in default_args.\nSet the failure email option to True.\nConfigure the success email to send you messages as well.\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\ndefault_args={\n    'email': [\n        'airflowalerts@datacamp.com',\n        'airflowadmin@datacamp.com',\n        ...\n    ],\n    'email_on_failure': True,\n    'email_on_success': True,\n}\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\",\n    default_args = default_args\n)\n\nprecheck = FileSensor(\n    task_id = 'check_for_datafile',\n    filepath = 'salesdata_ready.csv',\n    start_date = datetime(2020,2,20),\n    mode = 'reschedule',\n    dag = report_dag\n)\n\ngenerate_report_task = BashOperator(\n    task_id = 'generate_report',\n    bash_command = 'generate_report.sh',\n    start_date = datetime(2020,2,20),\n    dag = report_dag,\n)\n\nprecheck >> generate_report_task"
  },
  {
    "objectID": "data/SwissParliamentAPI.html",
    "href": "data/SwissParliamentAPI.html",
    "title": "Bernardo Cruz",
    "section": "",
    "text": "Show the code\nimport swissparlpy\nimport requests\nimport pandas as pd\nimport os\nimport urllib3\nfrom datetime import datetime\n\n\n\n\nShow the code\nurllib3.disable_warnings()\n__location__ = os.path.realpath(os.getcwd())\n\n\n\n\nSometimes it‚Äôs necessary to tweak the requests Session (e.g.¬†to provide authentication or disable SSL verification). For this purpose a custom session can be passed to SwissParlClient.\n\n\nShow the code\nsession = requests.Session()\nsession.verify = False # disable SSL verification\nclient = swissparlpy.SwissParlClient(session=session)\n\n\nFor most common cases, this is not necessary and you don‚Äôt even have to create your own SwissParlClient.\nSimply use the shorthand methods to get the data:\n\n\nShow the code\nimport swissparlpy as spp\n\ntables = spp.get_tables()\nglimpse_df = (\n    pd.DataFrame(\n        spp.get_glimpse(tables[0])\n    )\n)\nglimpse_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      PartyNumber\n      PartyName\n      PersonNumber\n      PersonIdCode\n      FirstName\n      LastName\n      GenderAsString\n      PartyFunction\n      Modified\n      PartyAbbreviation\n    \n  \n  \n    \n      0\n      1\n      DE\n      12\n      Sozialdemokratische Partei der Schweiz\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      SP\n    \n    \n      1\n      1\n      EN\n      12\n      Parti socialiste suisse\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      PSS\n    \n    \n      2\n      1\n      FR\n      12\n      Parti socialiste suisse\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      PSS\n    \n    \n      3\n      1\n      IT\n      12\n      Partito socialista svizzero\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      PSS\n    \n    \n      4\n      1\n      RM\n      12\n      Sozialdemokratische Partei der Schweiz\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      SP\n    \n  \n\n\n\n\n\n\n\n\n\nShow the code\nclient.get_tables() # get list of all tables\n\n\n['MemberParty',\n 'Party',\n 'Person',\n 'PersonAddress',\n 'PersonCommunication',\n 'PersonInterest',\n 'Session',\n 'Committee',\n 'MemberCommittee',\n 'Canton',\n 'Council',\n 'Objective',\n 'Resolution',\n 'Publication',\n 'External',\n 'Meeting',\n 'Subject',\n 'Citizenship',\n 'Preconsultation',\n 'Bill',\n 'BillLink',\n 'BillStatus',\n 'Business',\n 'BusinessResponsibility',\n 'BusinessRole',\n 'LegislativePeriod',\n 'MemberCouncil',\n 'MemberParlGroup',\n 'ParlGroup',\n 'PersonOccupation',\n 'RelatedBusiness',\n 'BusinessStatus',\n 'BusinessType',\n 'MemberCouncilHistory',\n 'MemberCommitteeHistory',\n 'Vote',\n 'Voting',\n 'SubjectBusiness',\n 'Transcript',\n 'ParlGroupHistory',\n 'Tags',\n 'SeatOrganisationNr',\n 'PersonEmployee',\n 'Rapporteur',\n 'Mutation']\n\n\n\n\nShow the code\nclient.get_variables('Party') # get list of variables of a table\n\n\n['ID',\n 'Language',\n 'PartyNumber',\n 'PartyName',\n 'StartDate',\n 'EndDate',\n 'Modified',\n 'PartyAbbreviation']\n\n\n\n\n\n\n\nShow the code\nparties = client.get_data('Party', Language='DE')\nparties_df = pd.DataFrame(parties)\nparties_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      PartyNumber\n      PartyName\n      StartDate\n      EndDate\n      Modified\n      PartyAbbreviation\n    \n  \n  \n    \n      0\n      12\n      DE\n      12\n      Sozialdemokratische Partei der Schweiz\n      1888-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      SP\n    \n    \n      1\n      13\n      DE\n      13\n      Schweizerische Volkspartei\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      SVP\n    \n    \n      2\n      14\n      DE\n      14\n      Christlichdemokratische Volkspartei der Schweiz\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      CVP\n    \n    \n      3\n      15\n      DE\n      15\n      FDP.Die Liberalen\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      FDP-Liberale\n    \n    \n      4\n      16\n      DE\n      16\n      Liberal-Demokratische Partei\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      LDP\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      78\n      1582\n      DE\n      1582\n      Gr√ºne (Basels starke Alternative)\n      1995-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2015-12-03 08:48:38.250000+00:00\n      BastA\n    \n    \n      79\n      1583\n      DE\n      1583\n      Christlichdemokratische Volkspartei Oberwallis\n      2019-03-04 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2019-03-07 17:24:15.013000+00:00\n      CVPO\n    \n    \n      80\n      1584\n      DE\n      1584\n      Alternative-die Gr√ºnen Kanton Zug\n      2019-11-08 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2019-11-08 17:28:43.947000+00:00\n      Al\n    \n    \n      81\n      1585\n      DE\n      1585\n      Ensemble √† Gauche\n      2019-11-08 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2019-11-08 17:41:39.513000+00:00\n      E√†G\n    \n    \n      82\n      1586\n      DE\n      1586\n      Die Mitte\n      2021-02-02 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2021-08-12 07:59:22.627000+00:00\n      M-E\n    \n  \n\n83 rows √ó 8 columns\n\n\n\n\n\n\n\n\nShow the code\npersons = client.get_data(\"Person\", Language=\"DE\", LastName__startswith='Bal')\npersons.count\n\n\n12\n\n\n\n\nShow the code\nperson_df = pd.DataFrame(persons)\nperson_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      PersonNumber\n      PersonIdCode\n      Title\n      TitleText\n      LastName\n      GenderAsString\n      DateOfBirth\n      DateOfDeath\n      ...\n      MaritalStatusText\n      PlaceOfBirthCity\n      PlaceOfBirthCanton\n      Modified\n      FirstName\n      OfficialName\n      MilitaryRank\n      MilitaryRankText\n      NativeLanguage\n      NumberOfChildren\n    \n  \n  \n    \n      0\n      1442\n      DE\n      1442\n      0\n      0\n      \n      Baldinger\n      m\n      1838-06-26 00:00:00+00:00\n      1907-01-05 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:45:47.190000+00:00\n      Emil A.\n      Emil A. Baldinger\n      0\n      \n      \n      0\n    \n    \n      1\n      1443\n      DE\n      1443\n      0\n      0\n      \n      Baldinger\n      m\n      1800-11-29 00:00:00+00:00\n      1881-01-26 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 12:07:54.837000+00:00\n      Karl L.\n      Karl L. Baldinger\n      0\n      \n      \n      0\n    \n    \n      2\n      1444\n      DE\n      1444\n      0\n      0\n      \n      Baldinger\n      m\n      1810-11-30 00:00:00+00:00\n      1881-07-13 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:52:09.533000+00:00\n      Wilhelm K.\n      Wilhelm K. Baldinger\n      0\n      \n      \n      0\n    \n    \n      3\n      1445\n      DE\n      1445\n      0\n      0\n      \n      Balestra\n      m\n      1873-07-17 00:00:00+00:00\n      1970-03-30 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 12:10:58.890000+00:00\n      Luigi A.\n      Luigi A. Balestra\n      0\n      \n      \n      0\n    \n    \n      4\n      1446\n      DE\n      1446\n      0\n      0\n      \n      Balli\n      m\n      1852-09-20 00:00:00+00:00\n      1924-12-21 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:42:29.907000+00:00\n      Francesco\n      Francesco Balli\n      0\n      \n      \n      0\n    \n    \n      5\n      1447\n      DE\n      1447\n      0\n      0\n      \n      Balli\n      m\n      1796-10-09 00:00:00+00:00\n      1863-09-08 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:14:45.110000+00:00\n      Valentino Alessandro\n      Valentino Alessandro Balli\n      0\n      \n      \n      0\n    \n    \n      6\n      1448\n      DE\n      1448\n      0\n      0\n      \n      Ballmoos\n      m\n      1911-07-07 00:00:00+00:00\n      1993-07-20 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:27:08.147000+00:00\n      Walter\n      Walter Ballmoos\n      0\n      \n      \n      0\n    \n    \n      7\n      1449\n      DE\n      1449\n      0\n      0\n      \n      Bally\n      m\n      1821-10-24 00:00:00+00:00\n      1899-08-05 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:43:44.773000+00:00\n      Carl Franz\n      Carl Franz Bally\n      0\n      \n      \n      0\n    \n    \n      8\n      1450\n      DE\n      1450\n      0\n      0\n      \n      Bally\n      m\n      1847-08-11 00:00:00+00:00\n      1926-07-24 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:40:52.433000+00:00\n      Eduard\n      Eduard Bally\n      0\n      \n      \n      0\n    \n    \n      9\n      1451\n      DE\n      1451\n      0\n      0\n      \n      Bally\n      m\n      1876-12-13 00:00:00+00:00\n      1965-08-02 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:51:15.100000+00:00\n      Iwan\n      Iwan Bally\n      0\n      \n      \n      0\n    \n    \n      10\n      1452\n      DE\n      1452\n      0\n      0\n      \n      Balmer\n      m\n      1859-04-25 00:00:00+00:00\n      1936-03-13 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 12:00:11.637000+00:00\n      Josef Anton\n      Josef Anton Balmer\n      0\n      \n      \n      0\n    \n    \n      11\n      1453\n      DE\n      1453\n      0\n      0\n      \n      Balmer\n      m\n      1872-10-01 00:00:00+00:00\n      1951-07-20 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:26:07.167000+00:00\n      Peter\n      Peter Balmer\n      0\n      \n      \n      0\n    \n  \n\n12 rows √ó 21 columns\n\n\n\n\n\nShow the code\nco2_business = client.get_data(\"Business\", Title__contains=\"CO2\", Language = \"DE\")\nco2_business.count\n\n\n294\n\n\n\n\nShow the code\nco2_df = pd.DataFrame(co2_business)\nco2_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      BusinessShortNumber\n      BusinessType\n      BusinessTypeName\n      BusinessTypeAbbreviation\n      Title\n      Description\n      InitialSituation\n      Proceedings\n      ...\n      SubmissionCouncilAbbreviation\n      SubmissionSession\n      SubmissionLegislativePeriod\n      FirstCouncil1\n      FirstCouncil1Name\n      FirstCouncil1Abbreviation\n      FirstCouncil2\n      FirstCouncil2Name\n      FirstCouncil2Abbreviation\n      TagNames\n    \n  \n  \n    \n      0\n      19923245\n      DE\n      92.3245\n      5\n      Motion\n      Mo.\n      Senkung der CO2-Emissionen\n      \n      \n      \n      ...\n      NR\n      4404\n      44\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      \n    \n    \n      1\n      19952011\n      DE\n      95.2011\n      10\n      Petition\n      Pet.\n      CO2 Lenkungsabgaben\n      \n      \n      \n      ...\n      \n      4418\n      44\n      2\n      St√§nderat\n      SR\n      0\n      \n      \n      \n    \n    \n      2\n      19953546\n      DE\n      95.3546\n      5\n      Motion\n      Mo.\n      Reduktion des CO2-Ausstosses und Kernenergie\n      \n      \n      \n      ...\n      NR\n      4420\n      44\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      \n    \n    \n      3\n      19970030\n      DE\n      97.030\n      1\n      Gesch√§ft des Bundesrates\n      BRG\n      Reduktion der CO2-Emissionen. Bundesgesetz\n      Botschaft vom 17. M√§rz 1997 zum Bundesgesetz √º...\n      <p>Der Klimaschutz geh√∂rt zu den wichtigsten g...\n      <p> Der <b>St√§nderat </b>verabschiedete das CO...\n      ...\n      \n      4506\n      45\n      2\n      St√§nderat\n      SR\n      0\n      \n      \n      \n    \n    \n      4\n      20005227\n      DE\n      00.5227\n      14\n      Fragestunde. Frage\n      Fra.\n      Wer rechnet die CO2-Emissionen des Luftverkehr...\n      \n      \n      \n      ...\n      NR\n      4605\n      46\n      0\n      \n      \n      0\n      \n      \n      Verkehr|Umwelt\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      289\n      20233569\n      DE\n      23.3569\n      8\n      Interpellation\n      Ip.\n      Wirksamkeit der CO2-Sanktionen f√ºr neue Person...\n      \n      \n      \n      ...\n      NR\n      5120\n      51\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      Staatspolitik|Verkehr|Umwelt\n    \n    \n      290\n      20233940\n      DE\n      23.3940\n      6\n      Postulat\n      Po.\n      Alternativen zur CO2-Bet√§ubung. Auftrag des BL...\n      \n      \n      \n      ...\n      NR\n      5121\n      51\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      Umwelt|Landwirtschaft\n    \n    \n      291\n      20237089\n      DE\n      23.7089\n      14\n      Fragestunde. Frage\n      Fra.\n      CO2-Preis zur Lenkung?\n      \n      \n      \n      ...\n      NR\n      5118\n      51\n      0\n      \n      \n      0\n      \n      \n      Umwelt|Energie|Steuer\n    \n    \n      292\n      20237091\n      DE\n      23.7091\n      14\n      Fragestunde. Frage\n      Fra.\n      Stellungnahme des Bundesrats zur Medienmitteil...\n      \n      \n      \n      ...\n      NR\n      5118\n      51\n      0\n      \n      \n      0\n      \n      \n      Finanzwesen|Umwelt|Energie\n    \n    \n      293\n      20237413\n      DE\n      23.7413\n      14\n      Fragestunde. Frage\n      Fra.\n      Auswirkung ge√§nderter rechtlicher Vorgaben zu ...\n      \n      \n      \n      ...\n      NR\n      5121\n      51\n      0\n      \n      \n      0\n      \n      \n      Verkehr|Umwelt|Energie\n    \n  \n\n294 rows √ó 43 columns\n\n\n\n\n\n\n\n\nShow the code\nbusiness_oct19 = client.get_data(\n    \"Business\",\n    Language=\"DE\",\n    SubmissionDate__gte=datetime.fromisoformat('2019-10-01'),\n    SubmissionDate__lt=datetime.fromisoformat('2019-10-31')\n)\nbusiness_oct19.count\n\n\nPyODataModelError: Edm.DateTime accepts only UTC\n\n\n\n\nShow the code\nbusi_oct19 = pd.DataFrame(business_oct19)\nbusi_oct19 = busi_oct19.sort_values(by=['SubmissionDate']).reset_index(drop=True)\nbusi_oct19[['SubmissionDate', 'Title']]\n\n\nNameError: name 'business_oct19' is not defined\n\n\n\n\n\nThis script shows how to download votes from the Voting table by iterating over each session in a legislative period. The chunks are then saved in a directory as pickled DataFrames.\nLater on, those chunks can easily be combined together as a single DataFrame containing all the votes of a legislative period.\n\n\nShow the code\npath = os.path.join(__location__, \"voting50\")\n\ndef save_votes_of_session(id):\n    if not os.path.exists(path):\n        os.mkdir(path)\n    pickle_path = os.path.join(path, f'{id}.pks')\n    \n    if os.path.exists(pickle_path):\n        print(f\"File {pickle_path} already exists, skipping\")\n        return\n    \n    print(f\"Loading votes of session {id}...\")\n    data = client.get_data(\"Voting\", Language=\"DE\", IdSession=id)\n    print(f\"{data.count} rows loaded.\")\n    df = pd.DataFrame(data)\n    \n    df.to_pickle(pickle_path)\n    print(f\"Saved pickle at {pickle_path}\")\n    print(\"\")\n\n\n# get all session of the 50 legislative period\nsessions50 = client.get_data(\"Session\", Language=\"DE\", LegislativePeriodNumber=50)\nsessions50.count\n\nfor session in sessions50:\n    print(f\"Loading session {session['ID']}\")\n    save_votes_of_session(session['ID'])\n\n# Combine to one dataframe\npath = os.path.join(__location__, \"voting50\")\ndf_voting50 = pd.concat([pd.read_pickle(os.path.join(path, x)) for x in os.listdir(path)])\ndf_voting50\n\n\nLoading session 5001\nLoading votes of session 5001...\n\n\n\n\nShow the code\ndf_5005 = pd.read_pickle(os.path.join(__location__, \"voting50\", '5005.pks'))\ndf_5005\n\n\n\n\nShow the code\n# Combine to one dataframe\npath = os.path.join(__location__, \"voting50\")\ndf_voting50 = pd.concat([pd.read_pickle(os.path.join(path, x)) for x in os.listdir(path)])\ndf_voting50"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\nMy first post about statmodels - so be nice! :)\n\n\n\n\n\n\n\nstatmodels\n\n\nSeaborn\n\n\nPandas\n\n\nPython vs.¬†R\n\n\nFormula API\n\n\nRDatasets in Python\n\n\nPatsy in Python\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nPandas Method Chaining\n\n\n\n\n\n\n\nPandas\n\n\nReadability\n\n\nMethod Chaining\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nFastAPI\n\n\n\n\n\n\n\nAPI Development\n\n\nFastAPI\n\n\nPython\n\n\nSQL\n\n\nCRUD Operations\n\n\nSQLAlchemy\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "posts/api.html",
    "href": "posts/api.html",
    "title": "FastAPI",
    "section": "",
    "text": "Did you ever dream to build your own fast API written in Python?\nFastAPI is a modern, high-performance web framework for building APIs with Python. It blends the ease of use of Python with exceptional speed, making it an ideal choice for developing robust and efficient web applications. Whether you‚Äôre crafting a simple REST API or a complex microservices architecture, FastAPI will help you with that.\nThe installation of FastAPI is very simple and it involves the installation of two packages: (1) FastAPI and (2) Uvicorn. The first one is the framework itself and the second one is the server that will run the API.\npip install fastapi\npip install \"uvicorn[standard]\"\nIn this tutorial, we will build a simple API that will allow us to perform basic CRUD (Create, Read, Update, Delete) operations on a database. The access to the database will be done by SQLAlchemy, which is a Python SQL toolkit for SQL databases. By the means of fastAPI, we will also provide a documentation of the API, which will be accessible through a web browser."
  },
  {
    "objectID": "posts/api.html#setting-up-the-project",
    "href": "posts/api.html#setting-up-the-project",
    "title": "FastAPI",
    "section": "Setting Up the Project",
    "text": "Setting Up the Project\nTo begin, we import the necessary libraries including pandas, pydantic, and fastapi. We‚Äôre also using sqlalchemy for database interactions. Our code starts by creating a database connection using the SQLAlchemy library. We‚Äôve chosen an SQLite database for this demonstration. However, you can use any database you like.\nimport pandas as pd\n\nfrom pydantic import BaseModel, Field\nfrom fastapi import FastAPI, HTTPException, \n\nfrom sqlalchemy import create_engine, MetaData, Table, select, insert, update, delete\n\n## Create a metadata object\nmetadata = MetaData()\n\n## Create a connection to the database\nengine = create_engine(              \n    ## Path to database                     \n    'sqlite:///../../data/FastAPI.db',      #¬†<1>\n)\n\n## Reflect census table from the engine: census\nfastapi = Table(\n    \"fastapi\", \n    metadata,\n    autoload_with = engine,\n)\nMoreover, we create a utility function to connect to the database and return a json-style format. This function will be used in the endpoints to fetch data from the database and return the dictionary, which will be converted to json format by FastAPI.\n## Helper function to fetch data\ndef fetch_return_dict(\n        stmt, \n        engine = engine,\n    ) -> dict:\n    \"\"\"\n    Utility function to convert sql query results to dict via pandas dataframe\n    \"\"\"\n    ## Create a connection to the database\n    connection = engine.connection()\n    data = connection.execute(stmt).fetchall()\n    connection.close()\n\n    return (\n        pd.DataFrame(\n            data = data\n        )\n        .to_dict(\n            orient = \"records\", \n        )\n    )"
  },
  {
    "objectID": "posts/api.html#fastapi-object",
    "href": "posts/api.html#fastapi-object",
    "title": "FastAPI",
    "section": "FastAPI Object",
    "text": "FastAPI Object\nWe instantiate a FastAPI app object with a title, description, and version. This app will serve as the backbone of our API. The description section contains details about the API and apear in the documentation.\n## Instantiate a FastAPI object\napp = FastAPI(\n    title           =   \"API Service\",\n    description     =   \"\"\"\n    ...\n    Add more description here\n    ...\n    \"\"\",\n    version         =   \"0.0.1\",\n)"
  },
  {
    "objectID": "posts/api.html#defining-the-data-model",
    "href": "posts/api.html#defining-the-data-model",
    "title": "FastAPI",
    "section": "Defining the Data Model",
    "text": "Defining the Data Model\nFastAPI encourages the use of Pydantic models for data validation and serialization. Pydantic is a Python library that simplifies the process of data validation and settings management in applications. It allows you to define data schemas using Python data classes with type annotations, and automatically validates and parses incoming data according to these schemas.\nWe‚Äôve defined IncomeTaxModel, MunicipalityDataOut, IncomeTaxDataOut, and YearDataOut as Pydantic models (BaseModel and type annotation) to structure the data that will be sent and received by the API endpoints.\nclass TaxModel(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Tax             :   int    =  Field(\"Average tax for a given year\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n\nclass MunicipalityDataOut(BaseModel):\n    Tax             :   int    =  Field(\"Average tax for a given year\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n    \nclass TaxDataOut(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n\nclass YearDataOut(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Tax             :   int    =  Field(\"Average tax for a given year\")"
  },
  {
    "objectID": "posts/api.html#building-api-endpoints",
    "href": "posts/api.html#building-api-endpoints",
    "title": "FastAPI",
    "section": "Building API Endpoints",
    "text": "Building API Endpoints\nWe proceed to define various API endpoints using FastAPI‚Äôs decorators. These endpoints cover different scenarios, from fetching data for a specific year to creating and updating entries in the database. Each endpoint corresponds to a specific HTTP method (GET, POST, PUT, DELETE) and handles specific types of requests.\nFor instance, the index route handles a basic GET request and returns all data from the database, ordered by the year. The get_data_of_year route fetches data for a given year, while get_municipality_data and get_district_data retrieve data for specific municipalities or districts, respectively. Additionally, the create_new_entry, update_tax_entry, update_year_entry, and delete_tax_entry routes allow for CRUD (Create, Read, Update, Delete) operations on the data.\nNext, I will show you the incomplete code for the endpoints (the complete code can be found in the end of the article):\n@app.get(\"/\")\ndef index():\n    return something\n\n##¬†Create a route to return data for a given year\n@app.get(\"/year/{year}\")\ndef get_data_of_year():\n    return something\n\n## Create a route to return data for a given city\n@app.get(\"/municipality/{municipality}\")\ndef get_municipality_data():\n    return something\n\n## Create a route to return data for a given district\n@app.get(\"/district/{district}\")\ndef get_district_data():\n    return something\n\n## Create a route to return data from the canton\n@app.get(\"/canton/\")\ndef get_canton_data():\n    return something\n\n## Create a new entry\n@app.post('/entry/{municipality}/{year}/{tax}')\ndef create_new_entry():\n    return something\n\n## Update an income tax entry\n@app.put(\"/update_tax/{municipality}/{year}/{tax}\")\ndef update_tax_entry():\n    return something\n    \n## update year entry\n@app.put(\"/update_year/{municipality}/{year_old}/{year_new}\")\ndef update_year_entry():\n    return something\n\n@app.delete(\"/delete/{municipality}/{year}/{tax}\")\ndef delete_tax_entry():\n    return something\nThe components of the the fastapi endpoints are as follows:\n\nDecorators are used to define the route of each endpoints and the HTTP-method.\nType annotation of the parameters allows the definition and validation of the data type of the parameters."
  },
  {
    "objectID": "posts/api.html#error-handling",
    "href": "posts/api.html#error-handling",
    "title": "FastAPI",
    "section": "Error Handling",
    "text": "Error Handling\nThe code also incorporates error handling. For instance, if a user attempts to create a new entry that already exists in the database, an HTTPException with an appropriate status code and detail message is raised. Similarly, error handling is employed for updating and deleting entries that don‚Äôt exist in the database. The following codes are use: 400 for Bad Request, 404 for Not Found, and 500 for Internal Server Error. More information can be found here.\nThe implementation of error handling is done by raise HTTPException() as follows:\n##¬†Create a index route\n@app.get(\"/\")\ndef index() -> list[IncomeTaxModel]:\n    \n    ...\n\n    result = fetch_return_dict(stmt)\n\n    if result:\n        return result\n    else:\n        raise HTTPException(\n            status_code = 404, \n            detail = f\"Item not found\",\n        )"
  },
  {
    "objectID": "posts/api.html#conclusion",
    "href": "posts/api.html#conclusion",
    "title": "FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we‚Äôve walked through the creation of a FastAPI web service for accessing income tax data. We‚Äôve covered the setup process, defining data models, creating API endpoints, and handling database interactions. FastAPI‚Äôs intuitive syntax and powerful features make it a fantastic choice for building efficient and robust APIs. This project serves as a foundation, demonstrating how to construct APIs that facilitate data retrieval and manipulation, crucial tasks in today‚Äôs data-centric world.\nOverall, this FastAPI-based web service exemplifies the elegance and utility of modern Python frameworks in building APIs that bridge the gap between data and applications."
  },
  {
    "objectID": "posts/pandas_method_chaning.html",
    "href": "posts/pandas_method_chaning.html",
    "title": "Pandas Method Chaining",
    "section": "",
    "text": "Introduction\nMethod chaining is a way to combine multiple pandas operations together into a single line or statement. It is not necessary to use method chaining, but it can make your code more concise and easier to read. In this post, we will look at how to use method chaining to clean and transform a dataset.\nHonestly, since I started using method chaining, I have found it difficult to go back to the old way of writing pandas code. I hope that by the end of this post, you will feel the same way. :)\nLet‚Äôs get started! First I show how to use method chaining to import a dataset as follows:\n\n## Data imports\n\ndf = (\n    # use sm \n    sm                          \n    # get datasets attribute\n    .datasets                   \n    # use get_rdataset method\n    .get_rdataset(              \n        \"car_prices\", \n        package='modeldata'\n    )\n    # get data attribute\n    .data\n)\n\nChaining in Python uses () in order to chain methods together. This allows the user to write multiple statements on different lines as showed in the code above. On big advantage of chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Not using chaining would require the user to write the code as follows:\ndf = sm.datasets.get_rdataset(\"car_prices\", package='modeldata').data\nUsing chaining makes the code more readable right?\nWhat I like most about chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Especially when working with large datasets, it is very useful to be able to comment out one line at a time.\nBefore continuing with the next section, let‚Äôs have a look at the data:\n\n(\n    df\n    .head()\n)\n\n\n\n\n\nTable¬†1:  Data Preview \n  \n    \n      \n      Price\n      Mileage\n      Cylinder\n      Doors\n      Cruise\n      Sound\n      Leather\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      convertible\n      coupe\n      hatchback\n      sedan\n      wagon\n    \n  \n  \n    \n      0\n      22661.05\n      20105\n      6\n      4\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      21725.01\n      13457\n      6\n      2\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      2\n      29142.71\n      31655\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      30731.94\n      22479\n      4\n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      33358.77\n      17590\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nThis dataset describes the price of sold cars by make, mileage and other features. Let us assume we would like to convert the data into a tidy format. We start with the car features.\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable¬†2:  Tidy format of car features \n  \n    \n      \n      Price\n      Mileage\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      Feature\n      Value\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Leather\n      0\n    \n    \n      5391\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      coupe\n      0\n    \n    \n      567\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cylinder\n      4\n    \n    \n      2175\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cruise\n      0\n    \n    \n      6999\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      sedan\n      1\n    \n  \n\n\n\n\n\nLet‚Äôs now continue and see how we can use the assign method to create new columns in our dataframe. We will use the assign method to create a new column called Make which will a vector representation of the car make.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable¬†3:  Continued from the previous chain I \n  \n    \n      \n      Price\n      Mileage\n      Feature\n      Value\n      Make\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      Leather\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      5391\n      8638.93\n      25216\n      coupe\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      567\n      8638.93\n      25216\n      Cylinder\n      4\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      2175\n      8638.93\n      25216\n      Cruise\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      6999\n      8638.93\n      25216\n      sedan\n      1\n      [0, 0, 1, 0, 0, 0]\n    \n  \n\n\n\n\n\nLet‚Äôs continue and use groupby and aggregate the Feature column by the agg method. This will give us the mean of each feature appears in the dataset.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .groupby(\n        by = 'Feature',\n    )\n    .agg(\n        {\n            'Value': ['mean'],\n        }\n    )\n)\n\n\n\n\n\nTable¬†4:  Continued from the previous chain II \n  \n    \n      \n      Value\n    \n    \n      \n      mean\n    \n    \n      Feature\n      \n    \n  \n  \n    \n      Cruise\n      0.752488\n    \n    \n      Cylinder\n      5.268657\n    \n    \n      Doors\n      3.527363\n    \n    \n      Leather\n      0.723881\n    \n    \n      Sound\n      0.679104\n    \n    \n      convertible\n      0.062189\n    \n    \n      coupe\n      0.174129\n    \n    \n      hatchback\n      0.074627\n    \n    \n      sedan\n      0.609453\n    \n    \n      wagon\n      0.079602\n    \n  \n\n\n\n\n\nSummarizing, we could go on forever and adding more steps to our method chain. But I think you get the point. We can do a lot with method chaining and it is a great way to write clean and readable code.\n\n\nConclusion\nMethod chaining in Pandas is a powerful technique that offers several advantages for data manipulation and analysis workflows. It involves combining multiple operations on a DataFrame or Series into a single, concise chain of method calls. This approach enhances code readability, maintainability, and efficiency.\nFirstly, method chaining reduces the need for intermediate variables, streamlining code and making it more readable. By stringing together operations, such as filtering, transforming, and aggregating, the code becomes a clear and sequential representation of the data transformation process.\nSecondly, it encourages the use of functionally composed operations, leading to more modular and reusable code. This modular nature facilitates changes and updates, as adjustments can be made within the chain without affecting other parts of the code.\nFurthermore, method chaining promotes better memory usage and performance optimization. Pandas optimizes these chains under the hood, reducing the creation of unnecessary intermediate copies of data frames, which leads to improved execution speed and reduced memory overhead.\nLastly, method chaining aligns well with the ‚Äútidy data‚Äù philosophy, as it emphasizes a more structured, organized approach to data manipulation. This promotes consistency and clarity in the analysis process, aiding in collaboration and code maintenance.\nI hope you enjoyed this post and learned something new. If you have any questions contact me. Thanks for reading!"
  },
  {
    "objectID": "posts/glm.html",
    "href": "posts/glm.html",
    "title": "My first post about statmodels - so be nice! :)",
    "section": "",
    "text": "Introduction: Play with GLMs\nDuring my study at HSLU I have learned to use mostly R when it comes to statistic and for some machine learning courses as well. When it comes to work, the most companies I have seen so far, use Python as their main programming language. My previous two employers used Python and R. This article is not going to discuss which language is better, but rather focus on how to use Python and the library statmodels which seems to produce similar outputs as it would in R.\nOne important and cool thing about the statmodels library is that it has a lot of data sets already included. You can find them here. The data set I use is a R data set from Rdatasets. In particular, the dataset from the package modeldata called car_prices is used.\n## Data imports\ndf = (\n    statsmodels.api #¬†imported as sm in following code\n    .datasets\n    .get_rdataset(\"car_prices\", package='modeldata')\n    .data\n)\nAnother important thing about this blog post is the usage of Quarto. Quarto allowed me to write this blog post directly from a Jupyter-Notebook without any fancy and complicated transformations. For more information, please visit the quarto website.\nHowever, let‚Äôs start with the analysis. The first thing we do is to import the necessary libraries and the data set.\n\n\nShow the code\n## General imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\n## Data manipulation imports\nimport pandas as pd\nimport numpy as np\n\n## Display imports\nfrom IPython.display import display, Markdown\n\n## statmodels import\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.genmod.families.family as fam\nfrom patsy import dmatrices\n\n## Plot imports\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (5,5/2.5)\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_theme()\nsns.set_context(\n    \"paper\", \n    rc={\n        \"figsize\"       :   plt.rcParams['figure.figsize'],\n        'font_scale'    :   1.25,\n    }\n)\n\n\n## Data imports\ndf = (\n    sm\n    .datasets\n    .get_rdataset(\n        \"car_prices\", \n        package='modeldata',\n    )\n    .data\n)\n\n\n\n\nExplanatory Data Exploration\nThe data exploration is a crucial step before fitting a model. It allows to understand the data and to identify potential problems. In this notebook, we will explore very briefly the data, as the focus of this article is to understand and apply statmodels.\n\n\nThe data set contains 804 observations (rows) and 18 predictors (columns). The data set contains information about car prices and the variables are described as follows: Price; Mileage; Cylinder; Doors; Cruise; Sound; Leather; Buick; Cadillac; Chevy; Pontiac; Saab; Saturn; convertible; coupe; hatchback; sedan; and wagon.\n\n\nFirst, two approximately continous variables Price and Mileage are investigated by means of graphical analysis. The following bar plots show the distribution of the two variables.\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng1 = sns.displot(\n    data = df,\n    x = 'Price',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of car price')\nplt.xlabel('Price')\nplt.ylabel('Count of occurance')\nplt.show(g1)\n\ng2 = sns.displot(\n    data = df,\n    x = 'Mileage',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Count of occurance')\nplt.show(g2)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable Price\n\n\n\n\n\n\n\n(b) Distribution of the variable Mileage\n\n\n\n\nFigure¬†1: Distribution of price and mileage variables.\n\n\n\nFigure¬†1 (a) shows on the left a right-skewed distribution with a peak around 15k$ and price values ranging from 8k dollars to 70k dollars. On the other hand, figure¬†1 (b) shows on the right a more ballanced distribution with a peak around 20k$ and price values ranging from 266 miles up to 50k miles.\nProceeding to the next two variables, Cylinder and Doors, one can see less possible values, ranging from\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng = sns.displot(\n    data = df,\n    x = 'Cylinder',\n    discrete = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Cylinder')\nplt.xlabel('Cylinder')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n# plt.figure()\ng = sns.displot(\n    data = df,\n    x = 'Doors',\n    discrete = True,\n    shrink = 0.5,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Doors')\nplt.xlabel('Doors')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable cylinder\n\n\n\n\n\n\n\n(b) Distribution of the variable doors\n\n\n\n\nFigure¬†2: Distribution of cylinder and doors variables.\n\n\n\nThe Figure¬†2 (a) surprised me quite a bit. I had anticipated the car to feature more than 8 cylinders, given that this dataset pertains to American cars. The cylinder count typically spans from 4 to 8, with the values accurately reflecting this range. It‚Äôs worth noting that the number of cylinders is expected to be even.\nAgain surprisingly, the Figure¬†2 (b) shows that the number of doors per car. The values are either 2 or 4, with the latter being more common. This is a bit surprising, as I would have expected the number of doors to be higher for American cars (SUV).\nNext, we check the distribution of the make of the cars in the dataset. For this analysis, we first pivot the dataframe using pd.melt() and calculate the sum by means of groupby() and sum() method as follows:\n\n\nShow the code\nbrands = (\n    df\n    [['Buick', 'Cadillac', 'Chevy', 'Pontiac', 'Saab']]\n    .melt()\n    .groupby('variable')\n    .sum()\n    .reset_index()\n)\n\nbrands.head()\n\n\n\n\n\n\nTable¬†1:  Cars by brand after melting, grouping, and summing \n  \n    \n      \n      variable\n      value\n    \n  \n  \n    \n      0\n      Buick\n      80\n    \n    \n      1\n      Cadillac\n      80\n    \n    \n      2\n      Chevy\n      320\n    \n    \n      3\n      Pontiac\n      150\n    \n    \n      4\n      Saab\n      114\n    \n  \n\n\n\n\n\nAfter aggregation, the visualization of the data is as follows:\n\n\nShow the code\nsns.catplot(\n    data    = brands.sort_values('value', ascending=False),\n    x       = 'variable',\n    y       = 'value',\n    kind    = 'bar',\n    height  = 5,\n    aspect  = 2,\n)\nplt.title('Number of cars by brand')\nplt.xlabel('Brand')\nplt.ylabel('Number of cars')\nplt.show()\n\n\n\n\n\nFigure¬†3: Number of cars by make\n\n\n\n\nIn descending order the most present make is: Chevy followed by Pontiac, Saab, Buick and Cadilac.\nAt this point it should be mentioned, that the data set is not balanced and the distribution of the features is not uniform across the different makes and car features. In a normal project this would be a point to consider and to take care of. However, for the purpose of this project, this is not necessary.\n\n\nModeling\nModeling with statsmodels becomes straightforward once the formula API and provided documentation are well understood. I encountered a minor challenge in grasping the formula API, but once I comprehended it, the usage turned out to be quite intuitive.\nLet‚Äôs delve into an understanding of the formula API (statsmodels.formula.api). This feature enables us to employ R-style formulas for specifying models. To illustrate, when fitting a linear model, the following formula suffices:\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\n    formula='y ~ x1 + x2 + x3', \n    data=df\n)\nThe formula API leverages the patsy package (Patsy Package). It‚Äôs worth noting that the formula API‚Äôs potency extends to intricate models. Additionally, it‚Äôs important to mention that the formula API automatically incorporates an intercept into the formula if one isn‚Äôt explicitly specified. For cases where the intercept is undesired, a -1 can be used within the formula.\nWith the glm class, a vast array of models becomes accessible importing as follows:\nimport statsmodels.genmod.families.family as fam\nThe fam import is necessary for specifying the family of the model. The families available are:\n\n\n\n\nTable¬†2: GLM Families\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nFamily(link,¬†variance[,¬†check_link])\nThe parent class for one-parameter exponential families.\n\n\nBinomial([link,¬†check_link])\nBinomial exponential family distribution.\n\n\nGamma([link,¬†check_link])\nGamma exponential family distribution.\n\n\nGaussian([link,¬†check_link])\nGaussian exponential family distribution.\n\n\nInverseGaussian([link,¬†check_link])\nInverseGaussian exponential family.\n\n\nNegativeBinomial([link,¬†alpha,¬†check_link])\nNegative Binomial exponential family (corresponds to NB2).\n\n\nPoisson([link,¬†check_link])\nPoisson exponential family.\n\n\nTweedie([link,¬†var_power,¬†eql,¬†check_link])\nTweedie family.\n\n\n\n\n\n\nFitting the model and analyzing the results are the same as one would using R. First define the model, then fit it, then analyze the results. The details of the fit can be accessed using the summary method.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  Price   No. Observations:                  804\nModel:                            GLM   Df Residuals:                      789\nModel Family:                Gaussian   Df Model:                           14\nLink Function:               Identity   Scale:                      8.4460e+06\nMethod:                          IRLS   Log-Likelihood:                -7544.8\nDate:                Sun, 03 Sep 2023   Deviance:                   6.6639e+09\nTime:                        21:02:49   Pearson chi2:                 6.66e+09\nNo. Iterations:                     3   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nMileage        -0.1842      0.013    -14.664      0.000      -0.209      -0.160\nCylinder     3659.4543    113.345     32.286      0.000    3437.303    3881.606\nDoors        1654.6326    174.525      9.481      0.000    1312.570    1996.695\nCruise        340.8695    295.962      1.152      0.249    -239.205     920.944\nSound         440.9169    234.484      1.880      0.060     -18.664     900.497\nLeather       790.8220    249.745      3.167      0.002     301.331    1280.313\nBuick       -1911.3752    336.292     -5.684      0.000   -2570.495   -1252.256\nCadillac      1.05e+04    409.274     25.663      0.000    9701.132    1.13e+04\nChevy       -3408.2863    213.274    -15.981      0.000   -3826.295   -2990.278\nPontiac     -4258.9628    256.358    -16.613      0.000   -4761.416   -3756.509\nSaab         9419.1227    331.211     28.438      0.000    8769.960    1.01e+04\nSaturn      -2859.0803    358.709     -7.970      0.000   -3562.137   -2156.023\nconvertible  1.258e+04    525.984     23.922      0.000    1.16e+04    1.36e+04\ncoupe        1559.7620    395.946      3.939      0.000     783.723    2335.801\nhatchback   -4977.3196    339.046    -14.680      0.000   -5641.837   -4312.803\nsedan       -3064.7176    215.007    -14.254      0.000   -3486.123   -2643.312\nwagon        1384.6400    364.920      3.794      0.000     669.410    2099.870\n===============================================================================\n\n\nAfter fitting a GLM using the glm class in statsmodels, you can obtain a summary of the model‚Äôs results using the .summary() method on the fitted model object. Here‚Äôs a general overview of the information typically included in the summary output:\n\nModel Information:\n\nThe name of the model.\nThe method used for estimation (e.g., maximum likelihood).\nThe distribution family (e.g., Gaussian, binomial, Poisson).\nThe link function used in the model (e.g., logit, identity, etc.).\nThe number of observations used in the model.\n\nModel Fit Statistics:\n\nLog-likelihood value.\nAIC (Akaike Information Criterion) and/or BIC (Bayesian Information Criterion).\nDeviance and Pearson chi-square statistics.\nDispersion parameter (if applicable).\n\nCoefficients:\n\nEstimated coefficients for each predictor variable.\nStandard errors of the coefficients.\nz-scores and p-values for testing the significance of the coefficients.\n\nConfidence Intervals:\n\nConfidence intervals for each coefficient, often at a default level like 95%.\n\nHypothesis Testing:\n\nHypothesis tests for the coefficients, typically with null hypothesis being that the coefficient is zero.\n\nGoodness of Fit:\n\nLikelihood-ratio test for overall model fit.\nTests for assessing the contribution of individual predictors to the model.\n\nDiagnostic Information:\n\nInformation about model assumptions and diagnostics, depending on the type of GLM and the method used.\n\nResiduals:\n\nInformation about residuals, which can include measures like deviance residuals, Pearson residuals, etc.\n\n\nRefer to the official documentation for the most accurate and up-to-date information about the summary output for your specific use case.\n\n\nConclusion\nStatsmodels is a powerful Python library for statistical modeling and hypothesis testing, making it an excellent transition for R users or Python users who like R to solve certain problems. It offers a familiar syntax and functionality for regression, ANOVA, and more. Its integration with Python‚Äôs data analysis ecosystem, like pandas, allows seamless data manipulation. With support for various statistical methods and a comprehensive summary output, Statsmodels facilitates effortless migration from R, enabling R users to harness its capabilities in a Python environment."
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üëã Hi there",
    "text": "üëã Hi there\nI am Bernardo, an aeronautical engineer with a BSc in Aeronautical Engineering and a MSc in Data Science. My passion lies in both coding and engineering. I am from Zurich üá®üá≠ and have over seven years of experience working as an aeronautical engineer and about three years as a data scientist.\nI would like to invite you go through my CV and blog to learn more about me ü§ì. Please don‚Äôt hesitate to contact me if you have any questions or would like to collaborate on a project ü§ù."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üë®üèΩ‚Äçüíª Experience",
    "text": "üë®üèΩ‚Äçüíª Experience\nExplore my journey through various industries and roles in the world of data science and beyond! From predictive maintenance in the railway sector üöÇüöÇüöÇ, to crafting models for turnover prediction in the food industry üçîüçïüçù and upsell models banking üíµüí∞üí≥, my experiences have been diverse and impactful. Delve into my work with Gaussian Mixture Models, Multi-Output Regression, OCR tools, and more. For a detailed account of my professional voyage, head to Section CV."
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üèùÔ∏è üèôÔ∏è üíª üì∫ Hobbies",
    "text": "üèùÔ∏è üèôÔ∏è üíª üì∫ Hobbies\nI love to travel üõ©Ô∏è üöÇ ‚õ∞Ô∏è üèùÔ∏è üèôÔ∏è to code üíª and binge watch üì∫ in my free time. In particular I discovered a passion for web development and this blog ü§ìüòé."
  },
  {
    "objectID": "index.html#tech-stack",
    "href": "index.html#tech-stack",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üíª Tech Stack:",
    "text": "üíª Tech Stack:"
  },
  {
    "objectID": "index.html#github-trophies",
    "href": "index.html#github-trophies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üèÜ GitHub Trophies",
    "text": "üèÜ GitHub Trophies"
  },
  {
    "objectID": "index.html#wisdom-of-the-day",
    "href": "index.html#wisdom-of-the-day",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üßê Wisdom of the Day",
    "text": "üßê Wisdom of the Day"
  },
  {
    "objectID": "index.html#farewell-message",
    "href": "index.html#farewell-message",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üôåüèΩ Farewell message",
    "text": "üôåüèΩ Farewell message\nsee you soon"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "I am a former aeronautical engineer with 3 years of experience as a data scientist and a recent graduate of MSc in Data Science (summer 2023). During my 7 years as an aeronautical engineer, I managed complex interdisciplinary projects in aircraft structure development and certification. As a data scientist, I developed and maintained machine learning models, demonstrating a passion for learning and innovation."
  },
  {
    "objectID": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "href": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "title": "Curriculum Vitae",
    "section": "Stadler Rail AG, Data Scientist (06/2023 ‚Äì present)",
    "text": "Stadler Rail AG, Data Scientist (06/2023 ‚Äì present)\n\nDeveloped machine learning models for predictive maintenance, e.g., door systems, compressor systems, HVAC systems.\nContributed to projects developing data-driven solutions for optimizing rolling stock maintenance processes.\nEstablished CI/CD pipeline for modeling activities and created Python packages for internal use.\n\nTechnologies: Gitlab, Gitlab CI/CD, Sklearn, FastAPI (Python, SQL, API)"
  },
  {
    "objectID": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "href": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "title": "Curriculum Vitae",
    "section": "Prognolite AG, Data Scientist (10/2022 ‚Äì 03/2023)",
    "text": "Prognolite AG, Data Scientist (10/2022 ‚Äì 03/2023)\n\nDeveloped regression models predicting turnover using customer data and external factors like weather and holidays.\nCreated multi-output regression models to predict menu sales based on customer data and external factors.\n\nTechnologies: Github, Tidymodels, Sklearn, Random Forest, XGBoost, Cubist (R, Python)"
  },
  {
    "objectID": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "href": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "title": "Curriculum Vitae",
    "section": "Crealogix AG, Data Scientist (09/2021 ‚Äì 09/2022)",
    "text": "Crealogix AG, Data Scientist (09/2021 ‚Äì 09/2022)\n\nDeveloped prospect and upsell models.\nExtracted information from fact-sheets using customized OCR.\nCreated user journeys for chatbot solutions using IBM Watson and Spacy.\n\nTechnologies: Sklearn, Spacy, Random Forest, XGBoost, RMarkdown (Python, SQL, R)"
  },
  {
    "objectID": "cv.html#bucher-leichtbau-ag-012014-092021",
    "href": "cv.html#bucher-leichtbau-ag-012014-092021",
    "title": "Curriculum Vitae",
    "section": "Bucher Leichtbau AG (01/2014 ‚Äì 09/2021)",
    "text": "Bucher Leichtbau AG (01/2014 ‚Äì 09/2021)\n\nCompliance Verification Engineer (10/2017 ‚Äì 09/2021)\n\nAdvised and supported all departments of Bucher Leichtbau AG in initial airworthiness certification matters.\nIndependently verified documentation for ‚Äúminor‚Äù and ‚Äúmajor changes‚Äù against certification requirements.\n\nTechnologies: FEMAP, Mechanical Engineering\n\n\nCertification Engineer (01/2014 ‚Äì 10/2017)\n\nConducted FEM calculations/verification and prepared internal and external test reports.\nCommunicated with customers, official bodies, and authorities.\nConsulted development department for design optimization.\n\nTechnologies: FEMAP, Mechanical Engineering"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\nMy first post about statmodels - so be nice! :)\n\n\n\n\n\n\n\nstatmodels\n\n\nSeaborn\n\n\nPandas\n\n\nPython vs.¬†R\n\n\nFormula API\n\n\nRDatasets in Python\n\n\nPatsy in Python\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\nPandas Method Chaining\n\n\n\n\n\n\n\nPandas\n\n\nReadability\n\n\nMethod Chaining\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\n3 min\n\n\n\n\n\n\n\n\nFastAPI\n\n\n\n\n\n\n\nAPI Development\n\n\nFastAPI\n\n\nPython\n\n\nSQL\n\n\nCRUD Operations\n\n\nSQLAlchemy\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "WorkInProgress/streamlit.html",
    "href": "WorkInProgress/streamlit.html",
    "title": "Streamlit",
    "section": "",
    "text": "Introduction\nStreamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps - so let‚Äôs get started!"
  },
  {
    "objectID": "WorkInProgress/AirflowArticle.html",
    "href": "WorkInProgress/AirflowArticle.html",
    "title": "Airflow",
    "section": "",
    "text": "Introduction\nIn my present work we fetch continously data from the field. This data is used to create models that allow us to predict the behavior of the field, validate the models and take actions. This data is stored in a database and we have a web application that allows us to visualize the data."
  },
  {
    "objectID": "tutorials/DC_airflow.html",
    "href": "tutorials/DC_airflow.html",
    "title": "Datacamp Airflow Course",
    "section": "",
    "text": "Apache Airflow is a platform to program workflows (general), including the creation, scheduling, and monitoring of said workflows. Airflow can use various tools and languages, but the actual workflow code is written with Python. Airflow implements workflows as DAGs, or Directed Acyclic Graphs. Airflow can be accessed and controlled via code, via the command-line, or via a built-in web interface.\n\n\nBefore we can really discuss Airflow, we need to talk about workflows. A workflow is a set of steps to accomplish a given data engineering task. These can include any given task, such as downloading a file, copying data, filtering information, writing to a database, and so forth. A workflow is of varying levels of complexity. Some workflows may only have 2 or 3 steps, while others consist of hundreds of components.\n\n\n\nWorkflow example containing 5 steps\n\n\nThe complexity of a workflow is completely dependent on the needs of the user. We show an example of a possible workflow to the right. It‚Äôs important to note that we‚Äôre defining a workflow here in a general data engineering sense. This is an informal definition to introduce the concept. As you‚Äôll see later, workflow can have specific meaning within specific tools.\n\n\n\nAirflow is a platform to program workflows (general), including the creation, scheduling, and monitoring of said workflows.\nAirflow can use various tools and languages, but the actual workflow code is written with Python. Airflow implements workflows as DAGs, or Directed Acyclic Graphs. We‚Äôll discuss exactly what this means throughout this course, but for now think of it as a set of tasks and the dependencies between them. Airflow can be accessed and controlled via code, via the command-line, or via a built-in web interface. We‚Äôll look at all these options later on.\n\n\n\nA DAG stands for a Directed Acyclic Graph. In Airflow, this represents the set of tasks that make up your workflow. It consists of the tasks and the dependencies between tasks.\n\n\n\nExample of DAG consisting of five set of tasks\n\n\nDAGs are created with various details about the DAG, including the name, start date, owner, email alerting options, etc.\n\n\n\nWe will go into further detail in the next lesson but a very simple DAG is defined using the following code. A new DAG is created with the dag_id of etl_pipeline and a default_args dictionary containing a start_date for the DAG.\netl_dag = DAG(\n    dag_id = 'etl_pipeline',    \n    default_args = {\"start_date\": \"2020-01-08\"},\n)\nNote that within any Python code, this is referred to via the variable identifier, etl_dag, but within the Airflow shell command, you must use the dag_id.\n\n\n\nTo get started, let‚Äôs look at how to run a component of an Airflow workflow. These components are called tasks and simply represent a portion of the workflow. We‚Äôll go into further detail in later chapters. There are several ways to run a task, but one of the simplest is using the airflow run shell command.\nairflow run <dag_id> <task_id> <start_date>\nAirflow run takes three arguments, a dag_id, a task_id, and a start_date. All of these arguments have specific meaning and will make more sense later in the course. For our example, we‚Äôll use a dag_id of example-etl, a task named download-file, and a start date of 2020-01-10. This task would simply download a specific file, perhaps a daily update from a remote source. Our command as such is airflow run example-etl download-file 2020-01-10. This will then run the specified task within Airflow.\n\n\n\nWe‚Äôve looked at Airflow and some of the basic aspects of why you‚Äôd use it. We‚Äôve also looked at how to run a task within Airflow from the command-line. Let‚Äôs practice what we‚Äôve learned.\n\n\nYou‚Äôve just started looking at using Airflow within your company and would like to try to run a task within the Airflow platform. You remember that you can use the airflow run command to execute a specific task within a workflow. Note that an error while using airflow run will return airflow.exceptions.AirflowException: on the last line of output.\nAn Airflow DAG is set up for you with a dag_id of etl_pipeline. The task_id is download_file and the start_date is 2020-01-08. All other components needed are defined for you.\nWhich command would you enter in the console to run the desired task?\n\nairflow run dag task 2020-01-08\n\nairflow run etl_pipeline task 2020-01-08\n\nairflow run etl_pipeline download_file 2020-01-08\n\n\n\n\nWhile researching how to use Airflow, you start to wonder about the airflow command in general. You realize that by simply running airflow you can get further information about various sub-commands that are available.\nWhich of the following is NOT an Airflow sub-command?\n\nlist_dags\nedit_dag\ntest\nscheduler"
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-data-engineering",
    "href": "tutorials/DC_airflow.html#what-is-data-engineering",
    "title": "Datacamp Airflow course",
    "section": "What is data engineering?",
    "text": "What is data engineering?\nBefore getting into workflows and Airflow, let‚Äôs discuss a bit about data engineering. While there are many specific definitions based on context, the general meaning behind data engineering is taking any action involving data and making it a reliable, repeatable, and maintainable process."
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-a-workflow",
    "href": "tutorials/DC_airflow.html#what-is-a-workflow",
    "title": "Datacamp Airflow Course",
    "section": "What is a workflow?",
    "text": "What is a workflow?\nBefore we can really discuss Airflow, we need to talk about workflows. A workflow is a set of steps to accomplish a given data engineering task. These can include any given task, such as downloading a file, copying data, filtering information, writing to a database, and so forth. A workflow is of varying levels of complexity. Some workflows may only have 2 or 3 steps, while others consist of hundreds of components.\n\n\n\nWorkflow example containing 5 steps\n\n\nThe complexity of a workflow is completely dependent on the needs of the user. We show an example of a possible workflow to the right. It‚Äôs important to note that we‚Äôre defining a workflow here in a general data engineering sense. This is an informal definition to introduce the concept. As you‚Äôll see later, workflow can have specific meaning within specific tools."
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-airflow",
    "href": "tutorials/DC_airflow.html#what-is-airflow",
    "title": "Datacamp Airflow Course",
    "section": "What is Airflow?",
    "text": "What is Airflow?\nAirflow is a platform to program workflows (general), including the creation, scheduling, and monitoring of said workflows.\nAirflow can use various tools and languages, but the actual workflow code is written with Python. Airflow implements workflows as DAGs, or Directed Acyclic Graphs. We‚Äôll discuss exactly what this means throughout this course, but for now think of it as a set of tasks and the dependencies between them. Airflow can be accessed and controlled via code, via the command-line, or via a built-in web interface. We‚Äôll look at all these options later on."
  },
  {
    "objectID": "tutorials/DC_airflow.html#other-workflow-tools",
    "href": "tutorials/DC_airflow.html#other-workflow-tools",
    "title": "Datacamp Airflow Course",
    "section": "Other workflow tools",
    "text": "Other workflow tools\nAirflow is not the only tool available for running data engineering workflows. Some other options are Spotify‚Äôs Luigi, Microsoft‚Äôs SSIS, or even just Bash scripting. We‚Äôll use some Bash scripting within our Airflow usage, but otherwise we‚Äôll focus on Airflow."
  },
  {
    "objectID": "tutorials/DC_airflow.html#quick-introduction-to-dags",
    "href": "tutorials/DC_airflow.html#quick-introduction-to-dags",
    "title": "Datacamp Airflow Course",
    "section": "Quick introduction to DAGs",
    "text": "Quick introduction to DAGs\nA DAG stands for a Directed Acyclic Graph. In Airflow, this represents the set of tasks that make up your workflow. It consists of the tasks and the dependencies between tasks.\n\n\n\nExample of DAG consisting of five set of tasks\n\n\nDAGs are created with various details about the DAG, including the name, start date, owner, email alerting options, etc."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-code-example",
    "href": "tutorials/DC_airflow.html#dag-code-example",
    "title": "Datacamp Airflow Course",
    "section": "DAG code example",
    "text": "DAG code example\nWe will go into further detail in the next lesson but a very simple DAG is defined using the following code. A new DAG is created with the dag_id of etl_pipeline and a default_args dictionary containing a start_date for the DAG.\netl_dag = DAG(\n    dag_id = 'etl_pipeline',    \n    default_args = {\"start_date\": \"2020-01-08\"},\n)\nNote that within any Python code, this is referred to via the variable identifier, etl_dag, but within the Airflow shell command, you must use the dag_id."
  },
  {
    "objectID": "tutorials/DC_airflow.html#running-a-workflow-in-airflow",
    "href": "tutorials/DC_airflow.html#running-a-workflow-in-airflow",
    "title": "Datacamp Airflow Course",
    "section": "Running a workflow in Airflow",
    "text": "Running a workflow in Airflow\nTo get started, let‚Äôs look at how to run a component of an Airflow workflow. These components are called tasks and simply represent a portion of the workflow. We‚Äôll go into further detail in later chapters. There are several ways to run a task, but one of the simplest is using the airflow run shell command.\nairflow run <dag_id> <task_id> <start_date>\nAirflow run takes three arguments, a dag_id, a task_id, and a start_date. All of these arguments have specific meaning and will make more sense later in the course. For our example, we‚Äôll use a dag_id of example-etl, a task named download-file, and a start date of 2020-01-10. This task would simply download a specific file, perhaps a daily update from a remote source. Our command as such is airflow run example-etl download-file 2020-01-10. This will then run the specified task within Airflow."
  },
  {
    "objectID": "tutorials/DC_airflow.html#lets-practice",
    "href": "tutorials/DC_airflow.html#lets-practice",
    "title": "Datacamp Airflow Course",
    "section": "Let‚Äôs practice!",
    "text": "Let‚Äôs practice!\nNow that we‚Äôve covered some of the most important pages of the Airflow UI, let‚Äôs practice examining some workflows using it."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\nDatacamp Airflow Course\n\n\n\n\n\n\n\nApache Airflow\n\n\nDAG\n\n\nPython\n\n\nWorkflow\n\n\nETL\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-a-dag",
    "href": "tutorials/DC_airflow.html#what-is-a-dag",
    "title": "Datacamp Airflow Course",
    "section": "What is a DAG?",
    "text": "What is a DAG?\nOur first question is what is a DAG? Beyond any specific mathematical meaning, a DAG, or Directed Acyclic Graph, has the following attributes: It is Directed, meaning there is an inherent flow representing the dependencies or order between execution of components. These dependencies (even implicit ones) provide context to the tools on how to order the running of components. A DAG is also Acyclic - it does not loop or repeat. This does not imply that the entire DAG cannot be rerun, only that the individual components are executed once per run. In this case, a Graph represents the components and the relationships (or dependencies) between them. The term DAG is found often in data engineering, not just in Airflow but also Apache Spark, Luigi, and others.\n1 https://en.m.wikipedia.org/wiki/Directed_acyclic_graph ## DAG in Airflow\nAs we‚Äôre working with Airflow, let‚Äôs look at its implementation of the DAG concept. Within Airflow, DAGs are written in Python, but can use components written in other languages or technologies. This means we‚Äôll define the DAG using Python, but we could include Bash scripts, other executables, Spark jobs, and so on. Airflow DAGs are made up of components to be executed, such as operators, sensors, etc. Airflow typically refers to these as tasks. We‚Äôll cover these in much greater depth later on, but for now think of a task as a thing within the workflow that needs to be done. Airflow DAGs contain dependencies that are defined, either explicitly or implicitly. These dependencies define the execution order so Airflow knows which components should be run at what point within the workflow. For example, you would likely want to copy a file to a server prior to trying to import it to a database."
  },
  {
    "objectID": "tutorials/DC_airflow.html#define-a-dag",
    "href": "tutorials/DC_airflow.html#define-a-dag",
    "title": "Datacamp Airflow Course",
    "section": "Define a DAG",
    "text": "Define a DAG\nLet‚Äôs look at defining a simple DAG within Airflow. When defining the DAG in Python, you must first import the DAG object from airflow dot models. Once imported, we create a default arguments dictionary consisting of attributes that will be applied to the components of our DAG. These attributes are optional, but provide a lot of power to define the runtime behavior of Airflow.\nfrom airflow.models import DAG\nfrom datetime import datetime\n\ndefault_arguments = {\n    'owner'         : 'jdoe',\n    'email'         : 'jdoe@email.com', \n    'start_date'    : datetime(2020, 1, 20)\n}\n\netl_dag = DAG(\n    'etl_workflow', \n    default_args = default_arguments\n)\nHere we define the owner name as jdoe, an email address for any alerting, and specify the start date of the DAG. The start date represents the earliest datetime that a DAG could be run. Finally, we define our DAG object with the first argument using a name for the DAG, etl underscore workflow, and assign the default arguments dictionary to the default underscore args argument. There are many other optional configurations we will use later on. Note that the entire DAG is assigned to a variable called etl underscore dag. This will be used later when defining the components of the DAG, but the variable name etl underscore dag does not actually appear in the Airflow interfaces. Note, DAG is case sensitive in Python code."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-on-the-command-line",
    "href": "tutorials/DC_airflow.html#dags-on-the-command-line",
    "title": "Datacamp Airflow Course",
    "section": "DAGs on the command line",
    "text": "DAGs on the command line\nWhen working with DAGs (and Airflow in general), you‚Äôll often want to use the airflow command line tool. The airflow command line program contains many subcommands that handle various aspects of running Airflow. You‚Äôve used a couple of these already in previous exercises. Use the\nairflow -h\ncommand for help and descriptions of the subcommands. Many of these subcommands are related to DAGs. You can use the\nairflow list_dags\noption to see all recognized DAGs in an installation. When in doubt, try a few different commands to find the information you‚Äôre looking for."
  },
  {
    "objectID": "tutorials/DC_airflow.html#command-line-vs-python",
    "href": "tutorials/DC_airflow.html#command-line-vs-python",
    "title": "Datacamp Airflow Course",
    "section": "Command line vs Python",
    "text": "Command line vs Python\nYou may be wondering when to use the Airflow command line tool vs writing Python.\n\nCommand line vs Python\n\n\nCommand line\nPython\n\n\n\n\nStart Airflow processes\nCreate a DAGs\n\n\nManually run DAGs / tasks\nEdit individual prop of DAG\n\n\nReview logging information\n\n\n\n\nIn general, the airflow command line program is used to start Airflow processes (ie, webserver or scheduler), manually run DAGs or tasks, and review logging information. Python code itself is usually used in the creation and editing of a DAG, not to mention the actual data processing code itself."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-in-airflow",
    "href": "tutorials/DC_airflow.html#dag-in-airflow",
    "title": "Datacamp Airflow Course",
    "section": "DAG in Airflow",
    "text": "DAG in Airflow\nAs we‚Äôre working with Airflow, let‚Äôs look at its implementation of the DAG concept. Within Airflow, DAGs are written in Python, but can use components written in other languages or technologies. This means we‚Äôll define the DAG using Python, but we could include Bash scripts, other executables, Spark jobs, and so on. Airflow DAGs are made up of components to be executed, such as operators, sensors, etc. Airflow typically refers to these as tasks. We‚Äôll cover these in much greater depth later on, but for now think of a task as a thing within the workflow that needs to be done. Airflow DAGs contain dependencies that are defined, either explicitly or implicitly. These dependencies define the execution order so Airflow knows which components should be run at what point within the workflow. For example, you would likely want to copy a file to a server prior to trying to import it to a database."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-1",
    "href": "tutorials/DC_airflow.html#exercises-1",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Defining a simple DAG\nYou‚Äôve spent some time reviewing the Airflow components and are interested in testing out your own workflows. To start you decide to define the default arguments and create a DAG object for your workflow.\nThe DateTime object has been imported for you.\n\nImport the Airflow DAG object. Note that it is case-sensitive.\nDefine the default_args dictionary with a key owner and a value of ‚Äòdsmith‚Äô. Add a start_date of January 14, 2020 to default_args using the value 1 for the month of January. Add a retries count of 2 to default_args.\nInstantiate the DAG object to a variable called etl_dag with a DAG named example_etl. Add the default_args dictionary to the appropriate argument.\n\n\n# Import the DAG object\nfrom datetime import datetime\nfrom airflow.models import DAG\n\n# Define the default_args dictionary\ndefault_args = {\n  'owner'       : 'dsmith',\n  'start_date'  : datetime(2020, 1, 14),\n  'retries'     : 2\n}\n\n# Instantiate the DAG object\netl_dag = DAG(\n    'example_etl', \n    default_args=default_args\n)\n\n\n\nExercise 2: Working with DAGs and the Airflow shell\nWhile working with Airflow, sometimes it can be tricky to remember what DAGs are defined and what they do. You want to gain some further knowledge of the Airflow shell command so you‚Äôd like to see what options are available.\nMultiple DAGs are already defined for you. How many DAGs are present in the Airflow system from the command-line?\n\n!airflow dags list\n\n/Users/bernardo/miniforge3/envs/blog/lib/python3.11/site-packages/airflow/configuration.py:751 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.\n\nPlease confirm database upgrade (or wait 4 seconds to skip it). Are you sure? [y/N]\nNo data found"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-dags",
    "href": "tutorials/DC_airflow.html#dags-view-dags",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view DAGs",
    "text": "DAGs view DAGs\nIt provides a quick status of the number of DAGs / workflows available.\n\n\nIt shows us the schedule for the DAG (in date or cron format).\n\nWe can see the owner of the DAG.\n\nwhich of the most recent tasks have run,\n\nwhen the last run started,\n\nand the last three DAG runs.\n\nThe links area on the right gives us quick access to many of the DAG specific views.\n\nDon‚Äôt worry about those for now - instead we‚Äôll click on the ‚Äúexample_dag‚Äù link which takes us to our DAG detail page."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-schedule",
    "href": "tutorials/DC_airflow.html#dags-view-schedule",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view schedule",
    "text": "DAGs view schedule\nIt shows us the schedule for the DAG (in date or cron format)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-owner",
    "href": "tutorials/DC_airflow.html#dags-view-owner",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view owner",
    "text": "DAGs view owner\nWe can see the owner of the DAG."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-recent-tasks",
    "href": "tutorials/DC_airflow.html#dags-view-recent-tasks",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view recent tasks",
    "text": "DAGs view recent tasks\nwhich of the most recent tasks have run,"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-last-run",
    "href": "tutorials/DC_airflow.html#dags-view-last-run",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view last run",
    "text": "DAGs view last run\nwhen the last run started,"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-last-three",
    "href": "tutorials/DC_airflow.html#dags-view-last-three",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view last three",
    "text": "DAGs view last three\nand the last three DAG runs."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-links",
    "href": "tutorials/DC_airflow.html#dags-view-links",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view links",
    "text": "DAGs view links\nThe links area on the right gives us quick access to many of the DAG specific views."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-example_dag",
    "href": "tutorials/DC_airflow.html#dags-view-example_dag",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view example_dag",
    "text": "DAGs view example_dag\nDon‚Äôt worry about those for now - instead we‚Äôll click on the ‚Äúexample_dag‚Äù link which takes us to our DAG detail page."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-detail-view",
    "href": "tutorials/DC_airflow.html#dag-detail-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG detail view",
    "text": "DAG detail view\nThe DAG detail view gives us specific access to information about the DAG itself, including several views of information (Graph, Tree, and Code) illustrating the tasks and dependencies in the code. We also get access to the Task duration, task tries, timings, a Gantt chart view, and specific details about the DAG. We have the ability to trigger the DAG (to start), refresh our view, and delete the DAG if we desire. The detail view defaults to the Tree view, showing the specific named tasks, which operators are in use, and any dependencies between tasks. The circles in front of the words represent the state of the task / DAG. In the case of our specific DAG, we see that we have one task called generate_random_number."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-graph-view",
    "href": "tutorials/DC_airflow.html#dag-graph-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG graph view",
    "text": "DAG graph view\nThe DAG graph view arranges the tasks and dependencies in a chart format - this provides another view into the flow of the DAG. You can see the operators in use and the state of the tasks at any point in time. The tree and graph view provide different information depending on what you‚Äôd like to know. Try moving between them when examining a DAG to obtain further details. For this view we again see that we have a task called generate_random_number. We can also see that it is of the type BashOperator in the middle left of the image."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-code-view",
    "href": "tutorials/DC_airflow.html#dag-code-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG code view",
    "text": "DAG code view\nThe DAG code view does exactly as it sounds - it provides a copy of the Python code that makes up the DAG. The code view provides easy access to exactly what defines the DAG without clicking in various portions of the UI. As you use Airflow, you‚Äôll determine which tools work best for you. It is worth noting that the code view is read-only. Any DAG code changes must be done via the actual DAG script. In this view, we can finally see the code making up the generate_random_number task and that it runs the bash command echo $RANDOM."
  },
  {
    "objectID": "tutorials/DC_airflow.html#logs",
    "href": "tutorials/DC_airflow.html#logs",
    "title": "Datacamp Airflow Course",
    "section": "Logs",
    "text": "Logs\nThe Logs page, under the Browse menu option, provides troubleshooting and audit ability while using Airflow. This includes items such as starting the Airflow webserver, viewing the graph or tree nodes, creating users, starting DAGs, etc. When using Airflow, look at the logs often to become more familiar with the types of information included, and also what happens behind the scenes of an Airflow install. Note that you‚Äôll often refer to the Event type present on the Logs view when searching (such as graph, tree, cli scheduler)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#web-ui-vs-command-line",
    "href": "tutorials/DC_airflow.html#web-ui-vs-command-line",
    "title": "Datacamp Airflow Course",
    "section": "Web UI vs command line",
    "text": "Web UI vs command line\nIn most circumstances, you can choose between using the Airflow web UI or the command line tool based on your preference. The web UI is often easier to use overall. The command line tool may be simpler to access depending on settings (via SSH, etc.)"
  }
]
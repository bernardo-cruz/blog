[
  {
    "objectID": "posts/api.html",
    "href": "posts/api.html",
    "title": "FastAPI",
    "section": "",
    "text": "Did you ever dream to build your own fast API written in Python?\nFastAPI is a modern, high-performance web framework for building APIs with Python. It blends the ease of use of Python with exceptional speed, making it an ideal choice for developing robust and efficient web applications. Whether you’re crafting a simple REST API or a complex microservices architecture, FastAPI will help you with that.\nThe installation of FastAPI is very simple and it involves the installation of two packages: (1) FastAPI and (2) Uvicorn. The first one is the framework itself and the second one is the server that will run the API.\npip install fastapi\npip install \"uvicorn[standard]\"\nIn this tutorial, we will build a simple API that will allow us to perform basic CRUD (Create, Read, Update, Delete) operations on a database. The access to the database will be done by SQLAlchemy, which is a Python SQL toolkit for SQL databases. By the means of fastAPI, we will also provide a documentation of the API, which will be accessible through a web browser."
  },
  {
    "objectID": "posts/api.html#setting-up-the-project",
    "href": "posts/api.html#setting-up-the-project",
    "title": "FastAPI",
    "section": "Setting Up the Project",
    "text": "Setting Up the Project\nTo begin, we import the necessary libraries including pandas, pydantic, and fastapi. We’re also using sqlalchemy for database interactions. Our code starts by creating a database connection using the SQLAlchemy library. We’ve chosen an SQLite database for this demonstration. However, you can use any database you like.\nimport pandas as pd\n\nfrom pydantic import BaseModel, Field\nfrom fastapi import FastAPI, HTTPException, \n\nfrom sqlalchemy import create_engine, MetaData, Table, select, insert, update, delete\n\n## Create a metadata object\nmetadata = MetaData()\n\n## Create a connection to the database\nengine = create_engine(              \n    ## Path to database                     \n    'sqlite:///../../data/FastAPI.db',      # &lt;1&gt;\n)\n\n## Reflect census table from the engine: census\nfastapi = Table(\n    \"fastapi\", \n    metadata,\n    autoload_with = engine,\n)\nMoreover, we create a utility function to connect to the database and return a json-style format. This function will be used in the endpoints to fetch data from the database and return the dictionary, which will be converted to json format by FastAPI.\n## Helper function to fetch data\ndef fetch_return_dict(\n        stmt, \n        engine = engine,\n    ) -&gt; dict:\n    \"\"\"\n    Utility function to convert sql query results to dict via pandas dataframe\n    \"\"\"\n    ## Create a connection to the database\n    connection = engine.connection()\n    data = connection.execute(stmt).fetchall()\n    connection.close()\n\n    return (\n        pd.DataFrame(\n            data = data\n        )\n        .to_dict(\n            orient = \"records\", \n        )\n    )"
  },
  {
    "objectID": "posts/api.html#fastapi-object",
    "href": "posts/api.html#fastapi-object",
    "title": "FastAPI",
    "section": "FastAPI Object",
    "text": "FastAPI Object\nWe instantiate a FastAPI app object with a title, description, and version. This app will serve as the backbone of our API. The description section contains details about the API and apear in the documentation.\n## Instantiate a FastAPI object\napp = FastAPI(\n    title           =   \"API Service\",\n    description     =   \"\"\"\n    ...\n    Add more description here\n    ...\n    \"\"\",\n    version         =   \"0.0.1\",\n)"
  },
  {
    "objectID": "posts/api.html#defining-the-data-model",
    "href": "posts/api.html#defining-the-data-model",
    "title": "FastAPI",
    "section": "Defining the Data Model",
    "text": "Defining the Data Model\nFastAPI encourages the use of Pydantic models for data validation and serialization. Pydantic is a Python library that simplifies the process of data validation and settings management in applications. It allows you to define data schemas using Python data classes with type annotations, and automatically validates and parses incoming data according to these schemas.\nWe’ve defined IncomeTaxModel, MunicipalityDataOut, IncomeTaxDataOut, and YearDataOut as Pydantic models (BaseModel and type annotation) to structure the data that will be sent and received by the API endpoints.\nclass TaxModel(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Tax             :   int    =  Field(\"Average tax for a given year\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n\nclass MunicipalityDataOut(BaseModel):\n    Tax             :   int    =  Field(\"Average tax for a given year\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n    \nclass TaxDataOut(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Year            :   int    =  Field(\"Year of meassurement\")\n\nclass YearDataOut(BaseModel):\n    Municipality    :   str    =  Field(\"Name of item\")\n    Tax             :   int    =  Field(\"Average tax for a given year\")"
  },
  {
    "objectID": "posts/api.html#building-api-endpoints",
    "href": "posts/api.html#building-api-endpoints",
    "title": "FastAPI",
    "section": "Building API Endpoints",
    "text": "Building API Endpoints\nWe proceed to define various API endpoints using FastAPI’s decorators. These endpoints cover different scenarios, from fetching data for a specific year to creating and updating entries in the database. Each endpoint corresponds to a specific HTTP method (GET, POST, PUT, DELETE) and handles specific types of requests.\nFor instance, the index route handles a basic GET request and returns all data from the database, ordered by the year. The get_data_of_year route fetches data for a given year, while get_municipality_data and get_district_data retrieve data for specific municipalities or districts, respectively. Additionally, the create_new_entry, update_tax_entry, update_year_entry, and delete_tax_entry routes allow for CRUD (Create, Read, Update, Delete) operations on the data.\nNext, I will show you the incomplete code for the endpoints (the complete code can be found in the end of the article):\n@app.get(\"/\")\ndef index():\n    return something\n\n## Create a route to return data for a given year\n@app.get(\"/year/{year}\")\ndef get_data_of_year():\n    return something\n\n## Create a route to return data for a given city\n@app.get(\"/municipality/{municipality}\")\ndef get_municipality_data():\n    return something\n\n## Create a route to return data for a given district\n@app.get(\"/district/{district}\")\ndef get_district_data():\n    return something\n\n## Create a route to return data from the canton\n@app.get(\"/canton/\")\ndef get_canton_data():\n    return something\n\n## Create a new entry\n@app.post('/entry/{municipality}/{year}/{tax}')\ndef create_new_entry():\n    return something\n\n## Update an income tax entry\n@app.put(\"/update_tax/{municipality}/{year}/{tax}\")\ndef update_tax_entry():\n    return something\n    \n## update year entry\n@app.put(\"/update_year/{municipality}/{year_old}/{year_new}\")\ndef update_year_entry():\n    return something\n\n@app.delete(\"/delete/{municipality}/{year}/{tax}\")\ndef delete_tax_entry():\n    return something\nThe components of the the fastapi endpoints are as follows:\n\nDecorators are used to define the route of each endpoints and the HTTP-method.\nType annotation of the parameters allows the definition and validation of the data type of the parameters."
  },
  {
    "objectID": "posts/api.html#error-handling",
    "href": "posts/api.html#error-handling",
    "title": "FastAPI",
    "section": "Error Handling",
    "text": "Error Handling\nThe code also incorporates error handling. For instance, if a user attempts to create a new entry that already exists in the database, an HTTPException with an appropriate status code and detail message is raised. Similarly, error handling is employed for updating and deleting entries that don’t exist in the database. The following codes are use: 400 for Bad Request, 404 for Not Found, and 500 for Internal Server Error. More information can be found here.\nThe implementation of error handling is done by raise HTTPException() as follows:\n## Create a index route\n@app.get(\"/\")\ndef index() -&gt; list[IncomeTaxModel]:\n    \n    ...\n\n    result = fetch_return_dict(stmt)\n\n    if result:\n        return result\n    else:\n        raise HTTPException(\n            status_code = 404, \n            detail = f\"Item not found\",\n        )"
  },
  {
    "objectID": "posts/api.html#conclusion",
    "href": "posts/api.html#conclusion",
    "title": "FastAPI",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we’ve walked through the creation of a FastAPI web service for accessing income tax data. We’ve covered the setup process, defining data models, creating API endpoints, and handling database interactions. FastAPI’s intuitive syntax and powerful features make it a fantastic choice for building efficient and robust APIs. This project serves as a foundation, demonstrating how to construct APIs that facilitate data retrieval and manipulation, crucial tasks in today’s data-centric world.\nOverall, this FastAPI-based web service exemplifies the elegance and utility of modern Python frameworks in building APIs that bridge the gap between data and applications."
  },
  {
    "objectID": "posts/glm.html",
    "href": "posts/glm.html",
    "title": "My first post about statmodels - so be nice! :)",
    "section": "",
    "text": "Introduction: Play with GLMs\nDuring my study at HSLU I have learned to use mostly R when it comes to statistic and for some machine learning courses as well. When it comes to work, the most companies I have seen so far, use Python as their main programming language. My previous two employers used Python and R. This article is not going to discuss which language is better, but rather focus on how to use Python and the library statmodels which seems to produce similar outputs as it would in R.\nOne important and cool thing about the statmodels library is that it has a lot of data sets already included. You can find them here. The data set I use is a R data set from Rdatasets. In particular, the dataset from the package modeldata called car_prices is used.\n## Data imports\ndf = (\n    statsmodels.api # imported as sm in following code\n    .datasets\n    .get_rdataset(\"car_prices\", package='modeldata')\n    .data\n)\nAnother important thing about this blog post is the usage of Quarto. Quarto allowed me to write this blog post directly from a Jupyter-Notebook without any fancy and complicated transformations. For more information, please visit the quarto website.\nHowever, let’s start with the analysis. The first thing we do is to import the necessary libraries and the data set.\n::: {#code .cell  code-fold=‘false’ execution_count=1}\n\nShow the code\n## General imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\n## Data manipulation imports\nimport pandas as pd\nimport numpy as np\n\n## Display imports\nfrom IPython.display import display, Markdown\n\n## statmodels import\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.genmod.families.family as fam\nfrom patsy import dmatrices\n\n## Plot imports\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (5,5/2.5)\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_theme()\nsns.set_context(\n    \"paper\", \n    rc={\n        \"figsize\"       :   plt.rcParams['figure.figsize'],\n        'font_scale'    :   1.25,\n    }\n)\n\n\n## Data imports\ndf = (\n    sm\n    .datasets\n    .get_rdataset(\n        \"car_prices\", \n        package='modeldata',\n    )\n    .data\n)\n\n:::\n\n\nExplanatory Data Exploration\nThe data exploration is a crucial step before fitting a model. It allows to understand the data and to identify potential problems. In this notebook, we will explore very briefly the data, as the focus of this article is to understand and apply statmodels.\n\n\nThe data set contains 804 observations (rows) and 18 predictors (columns). The data set contains information about car prices and the variables are described as follows: Price; Mileage; Cylinder; Doors; Cruise; Sound; Leather; Buick; Cadillac; Chevy; Pontiac; Saab; Saturn; convertible; coupe; hatchback; sedan; and wagon.\n\n\nFirst, two approximately continous variables Price and Mileage are investigated by means of graphical analysis. The following bar plots show the distribution of the two variables.\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng1 = sns.displot(\n    data = df,\n    x = 'Price',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of car price')\nplt.xlabel('Price')\nplt.ylabel('Count of occurance')\nplt.show(g1)\n\ng2 = sns.displot(\n    data = df,\n    x = 'Mileage',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Count of occurance')\nplt.show(g2)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of the variable Price\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of the variable Mileage\n\n\n\n\n\n\n\nFigure 1: Distribution of price and mileage variables.\n\n\n\nFigure 1 (a) shows on the left a right-skewed distribution with a peak around 15k$ and price values ranging from 8k dollars to 70k dollars. On the other hand, figure 1 (b) shows on the right a more ballanced distribution with a peak around 20k$ and price values ranging from 266 miles up to 50k miles.\nProceeding to the next two variables, Cylinder and Doors, one can see less possible values, ranging from\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng = sns.displot(\n    data = df,\n    x = 'Cylinder',\n    discrete = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Cylinder')\nplt.xlabel('Cylinder')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n# plt.figure()\ng = sns.displot(\n    data = df,\n    x = 'Doors',\n    discrete = True,\n    shrink = 0.5,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Doors')\nplt.xlabel('Doors')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of the variable cylinder\n\n\n\n\n\n\n\n\n\n\n\n(b) Distribution of the variable doors\n\n\n\n\n\n\n\nFigure 2: Distribution of cylinder and doors variables.\n\n\n\nThe Figure 2 (a) surprised me quite a bit. I had anticipated the car to feature more than 8 cylinders, given that this dataset pertains to American cars. The cylinder count typically spans from 4 to 8, with the values accurately reflecting this range. It’s worth noting that the number of cylinders is expected to be even.\nAgain surprisingly, the Figure 2 (b) shows that the number of doors per car. The values are either 2 or 4, with the latter being more common. This is a bit surprising, as I would have expected the number of doors to be higher for American cars (SUV).\nNext, we check the distribution of the make of the cars in the dataset. For this analysis, we first pivot the dataframe using pd.melt() and calculate the sum by means of groupby() and sum() method as follows:\n\n\nShow the code\nbrands = (\n    df\n    [['Buick', 'Cadillac', 'Chevy', 'Pontiac', 'Saab']]\n    .melt()\n    .groupby('variable')\n    .sum()\n    .reset_index()\n)\n\nbrands.head()\n\n\n\n\nTable 1: Cars by brand after melting, grouping, and summing\n\n\n\n\n\n\n\n\n\n\nvariable\nvalue\n\n\n\n\n0\nBuick\n80\n\n\n1\nCadillac\n80\n\n\n2\nChevy\n320\n\n\n3\nPontiac\n150\n\n\n4\nSaab\n114\n\n\n\n\n\n\n\n\n\n\nAfter aggregation, the visualization of the data is as follows:\n\n\nShow the code\nsns.catplot(\n    data    = brands.sort_values('value', ascending=False),\n    x       = 'variable',\n    y       = 'value',\n    kind    = 'bar',\n    height  = 5,\n    aspect  = 2,\n)\nplt.title('Number of cars by brand')\nplt.xlabel('Brand')\nplt.ylabel('Number of cars')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Number of cars by make\n\n\n\n\n\nIn descending order the most present make is: Chevy followed by Pontiac, Saab, Buick and Cadilac.\nAt this point it should be mentioned, that the data set is not balanced and the distribution of the features is not uniform across the different makes and car features. In a normal project this would be a point to consider and to take care of. However, for the purpose of this project, this is not necessary.\n\n\nModeling\nModeling with statsmodels becomes straightforward once the formula API and provided documentation are well understood. I encountered a minor challenge in grasping the formula API, but once I comprehended it, the usage turned out to be quite intuitive.\nLet’s delve into an understanding of the formula API (statsmodels.formula.api). This feature enables us to employ R-style formulas for specifying models. To illustrate, when fitting a linear model, the following formula suffices:\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\n    formula='y ~ x1 + x2 + x3', \n    data=df\n)\nThe formula API leverages the patsy package (Patsy Package). It’s worth noting that the formula API’s potency extends to intricate models. Additionally, it’s important to mention that the formula API automatically incorporates an intercept into the formula if one isn’t explicitly specified. For cases where the intercept is undesired, a -1 can be used within the formula.\nWith the glm class, a vast array of models becomes accessible importing as follows:\nimport statsmodels.genmod.families.family as fam\nThe fam import is necessary for specifying the family of the model. The families available are:\n\n\n\n\nTable 2: GLM Families\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nFamily(link, variance[, check_link])\nThe parent class for one-parameter exponential families.\n\n\nBinomial([link, check_link])\nBinomial exponential family distribution.\n\n\nGamma([link, check_link])\nGamma exponential family distribution.\n\n\nGaussian([link, check_link])\nGaussian exponential family distribution.\n\n\nInverseGaussian([link, check_link])\nInverseGaussian exponential family.\n\n\nNegativeBinomial([link, alpha, check_link])\nNegative Binomial exponential family (corresponds to NB2).\n\n\nPoisson([link, check_link])\nPoisson exponential family.\n\n\nTweedie([link, var_power, eql, check_link])\nTweedie family.\n\n\n\n\n\n\n\n\nFitting the model and analyzing the results are the same as one would using R. First define the model, then fit it, then analyze the results. The details of the fit can be accessed using the summary method.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  Price   No. Observations:                  804\nModel:                            GLM   Df Residuals:                      789\nModel Family:                Gaussian   Df Model:                           14\nLink Function:               Identity   Scale:                      8.4460e+06\nMethod:                          IRLS   Log-Likelihood:                -7544.8\nDate:                Sun, 03 Sep 2023   Deviance:                   6.6639e+09\nTime:                        21:02:49   Pearson chi2:                 6.66e+09\nNo. Iterations:                     3   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P&gt;|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nMileage        -0.1842      0.013    -14.664      0.000      -0.209      -0.160\nCylinder     3659.4543    113.345     32.286      0.000    3437.303    3881.606\nDoors        1654.6326    174.525      9.481      0.000    1312.570    1996.695\nCruise        340.8695    295.962      1.152      0.249    -239.205     920.944\nSound         440.9169    234.484      1.880      0.060     -18.664     900.497\nLeather       790.8220    249.745      3.167      0.002     301.331    1280.313\nBuick       -1911.3752    336.292     -5.684      0.000   -2570.495   -1252.256\nCadillac      1.05e+04    409.274     25.663      0.000    9701.132    1.13e+04\nChevy       -3408.2863    213.274    -15.981      0.000   -3826.295   -2990.278\nPontiac     -4258.9628    256.358    -16.613      0.000   -4761.416   -3756.509\nSaab         9419.1227    331.211     28.438      0.000    8769.960    1.01e+04\nSaturn      -2859.0803    358.709     -7.970      0.000   -3562.137   -2156.023\nconvertible  1.258e+04    525.984     23.922      0.000    1.16e+04    1.36e+04\ncoupe        1559.7620    395.946      3.939      0.000     783.723    2335.801\nhatchback   -4977.3196    339.046    -14.680      0.000   -5641.837   -4312.803\nsedan       -3064.7176    215.007    -14.254      0.000   -3486.123   -2643.312\nwagon        1384.6400    364.920      3.794      0.000     669.410    2099.870\n===============================================================================\n\n\nAfter fitting a GLM using the glm class in statsmodels, you can obtain a summary of the model’s results using the .summary() method on the fitted model object. Here’s a general overview of the information typically included in the summary output:\n\nModel Information:\n\nThe name of the model.\nThe method used for estimation (e.g., maximum likelihood).\nThe distribution family (e.g., Gaussian, binomial, Poisson).\nThe link function used in the model (e.g., logit, identity, etc.).\nThe number of observations used in the model.\n\nModel Fit Statistics:\n\nLog-likelihood value.\nAIC (Akaike Information Criterion) and/or BIC (Bayesian Information Criterion).\nDeviance and Pearson chi-square statistics.\nDispersion parameter (if applicable).\n\nCoefficients:\n\nEstimated coefficients for each predictor variable.\nStandard errors of the coefficients.\nz-scores and p-values for testing the significance of the coefficients.\n\nConfidence Intervals:\n\nConfidence intervals for each coefficient, often at a default level like 95%.\n\nHypothesis Testing:\n\nHypothesis tests for the coefficients, typically with null hypothesis being that the coefficient is zero.\n\nGoodness of Fit:\n\nLikelihood-ratio test for overall model fit.\nTests for assessing the contribution of individual predictors to the model.\n\nDiagnostic Information:\n\nInformation about model assumptions and diagnostics, depending on the type of GLM and the method used.\n\nResiduals:\n\nInformation about residuals, which can include measures like deviance residuals, Pearson residuals, etc.\n\n\nRefer to the official documentation for the most accurate and up-to-date information about the summary output for your specific use case.\n\n\nConclusion\nStatsmodels is a powerful Python library for statistical modeling and hypothesis testing, making it an excellent transition for R users or Python users who like R to solve certain problems. It offers a familiar syntax and functionality for regression, ANOVA, and more. Its integration with Python’s data analysis ecosystem, like pandas, allows seamless data manipulation. With support for various statistical methods and a comprehensive summary output, Statsmodels facilitates effortless migration from R, enabling R users to harness its capabilities in a Python environment."
  },
  {
    "objectID": "tutorials/DC_airflow.html",
    "href": "tutorials/DC_airflow.html",
    "title": "Datacamp Airflow Course",
    "section": "",
    "text": "This post is about a Datacamp course on Airflow which I took recently. I share the course and personal notes. I will also share some of the code I wrote for the exercises. This course covers information about: (1) DAGs; (2) Web Interface; (3) Operator; (4) Task; (5) Scheduling; (6) Sensor; (7) Executor; (8) Debugging; (9) SL; (10) Template; (11) Macro; (12) Branching; and (13) Production.\nThe course is called Introduction to Airflow in Python and is part of the Data Engineering track."
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-a-workflow",
    "href": "tutorials/DC_airflow.html#what-is-a-workflow",
    "title": "Datacamp Airflow Course",
    "section": "What is a workflow?",
    "text": "What is a workflow?\nBefore we can really discuss Airflow, we need to talk about workflows. A workflow is a set of steps to accomplish a given data engineering task. These can include any given task, such as downloading a file, copying data, filtering information, writing to a database, and so forth. A workflow is of varying levels of complexity. Some workflows may only have 2 or 3 steps, while others consist of hundreds of components.\n\n\n\nWorkflow example containing 5 steps\n\n\nThe complexity of a workflow is completely dependent on the needs of the user. We show an example of a possible workflow to the right. It’s important to note that we’re defining a workflow here in a general data engineering sense. This is an informal definition to introduce the concept. As you’ll see later, workflow can have specific meaning within specific tools."
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-airflow",
    "href": "tutorials/DC_airflow.html#what-is-airflow",
    "title": "Datacamp Airflow Course",
    "section": "What is Airflow?",
    "text": "What is Airflow?\nAirflow is a platform to program workflows (general), including the creation, scheduling, and monitoring of said workflows.\nAirflow can use various tools and languages, but the actual workflow code is written with Python. Airflow implements workflows as DAGs, or Directed Acyclic Graphs. We’ll discuss exactly what this means throughout this course, but for now think of it as a set of tasks and the dependencies between them. Airflow can be accessed and controlled via code, via the command-line, or via a built-in web interface. We’ll look at all these options later on."
  },
  {
    "objectID": "tutorials/DC_airflow.html#quick-introduction-to-dags",
    "href": "tutorials/DC_airflow.html#quick-introduction-to-dags",
    "title": "Datacamp Airflow Course",
    "section": "Quick introduction to DAGs",
    "text": "Quick introduction to DAGs\nA DAG stands for a Directed Acyclic Graph. In Airflow, this represents the set of tasks that make up your workflow. It consists of the tasks and the dependencies between tasks.\n\n\n\nExample of DAG consisting of five set of tasks\n\n\nDAGs are created with various details about the DAG, including the name, start date, owner, email alerting options, etc."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-code-example",
    "href": "tutorials/DC_airflow.html#dag-code-example",
    "title": "Datacamp Airflow Course",
    "section": "DAG code example",
    "text": "DAG code example\nWe will go into further detail in the next lesson but a very simple DAG is defined using the following code. A new DAG is created with the dag_id of etl_pipeline and a default_args dictionary containing a start_date for the DAG.\netl_dag = DAG(\n    dag_id = 'etl_pipeline',    \n    default_args = {\"start_date\": \"2020-01-08\"},\n)\n\n\n\n\n\n\nNote that within any Python code, this is referred to via the variable identifier, etl_dag, but within the Airflow shell command, you must use the dag_id."
  },
  {
    "objectID": "tutorials/DC_airflow.html#running-a-workflow-in-airflow",
    "href": "tutorials/DC_airflow.html#running-a-workflow-in-airflow",
    "title": "Datacamp Airflow Course",
    "section": "Running a workflow in Airflow",
    "text": "Running a workflow in Airflow\nTo get started, let’s look at how to run a component of an Airflow workflow. These components are called tasks and simply represent a portion of the workflow. We’ll go into further detail in later chapters. There are several ways to run a task, but one of the simplest is using the airflow run shell command.\nairflow run &lt;dag_id&gt; &lt;task_id&gt; &lt;start_date&gt;\nAirflow run takes three arguments, a dag_id, a task_id, and a start_date. All of these arguments have specific meaning and will make more sense later in the course. For our example, we’ll use a dag_id of example-etl, a task named download-file, and a start date of 2020-01-10. This task would simply download a specific file, perhaps a daily update from a remote source. Our command as such is airflow run example-etl download-file 2020-01-10. This will then run the specified task within Airflow."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises",
    "href": "tutorials/DC_airflow.html#exercises",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\nWe’ve looked at Airflow and some of the basic aspects of why you’d use it. We’ve also looked at how to run a task within Airflow from the command-line. Let’s practice what we’ve learned.\n\nExercise 1: Running a task in Airflow\nYou’ve just started looking at using Airflow within your company and would like to try to run a task within the Airflow platform. You remember that you can use the airflow run command to execute a specific task within a workflow.\n\n\n\n\n\n\nNote that an error while using airflow run will return airflow.exceptions.AirflowException: on the last line of output.\n\n\n\nAn Airflow DAG is set up for you with a dag_id of etl_pipeline. The task_id is download_file and the start_date is 2020-01-08. All other components needed are defined for you.\nWhich command would you enter in the console to run the desired task?\n\nairflow run dag task 2020-01-08\n\nairflow run etl_pipeline task 2020-01-08\n\nairflow run etl_pipeline download_file 2020-01-08\n\n\n\nExercise 2: Examining Airflow commands\nWhile researching how to use Airflow, you start to wonder about the airflow command in general. You realize that by simply running airflow you can get further information about various sub-commands that are available.\nWhich of the following is NOT an Airflow sub-command?\n\nlist_dags\nedit_dag\ntest\nscheduler"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-in-airflow",
    "href": "tutorials/DC_airflow.html#dag-in-airflow",
    "title": "Datacamp Airflow Course",
    "section": "DAG in Airflow",
    "text": "DAG in Airflow\nAs we’re working with Airflow, let’s look at its implementation of the DAG concept. Within Airflow, DAGs are written in Python, but can use components written in other languages or technologies. This means we’ll define the DAG using Python, but we could include Bash scripts, other executables, Spark jobs, and so on. Airflow DAGs are made up of components to be executed, such as operators, sensors, etc. Airflow typically refers to these as tasks. We’ll cover these in much greater depth later on, but for now think of a task as a thing within the workflow that needs to be done. Airflow DAGs contain dependencies that are defined, either explicitly or implicitly. These dependencies define the execution order so Airflow knows which components should be run at what point within the workflow. For example, you would likely want to copy a file to a server prior to trying to import it to a database."
  },
  {
    "objectID": "tutorials/DC_airflow.html#define-a-dag",
    "href": "tutorials/DC_airflow.html#define-a-dag",
    "title": "Datacamp Airflow Course",
    "section": "Define a DAG",
    "text": "Define a DAG\nLet’s look at defining a simple DAG within Airflow. When defining the DAG in Python, you must first import the DAG object from airflow dot models. Once imported, we create a default arguments dictionary consisting of attributes that will be applied to the components of our DAG. These attributes are optional, but provide a lot of power to define the runtime behavior of Airflow.\nfrom airflow.models import DAG\nfrom datetime import datetime\n\ndefault_arguments = {\n    'owner'         : 'jdoe',\n    'email'         : 'jdoe@email.com', \n    'start_date'    : datetime(2020, 1, 20)\n}\n\netl_dag = DAG(\n    'etl_workflow', \n    default_args = default_arguments\n)\nHere we define the owner name as jdoe, an email address for any alerting, and specify the start date of the DAG. The start date represents the earliest datetime that a DAG could be run. Finally, we define our DAG object with the first argument using a name for the DAG, etl underscore workflow, and assign the default arguments dictionary to the default underscore args argument. There are many other optional configurations we will use later on.\n\n\n\n\n\n\nNote that the entire DAG is assigned to a variable called etl underscore dag. This will be used later when defining the components of the DAG, but the variable name etl underscore dag does not actually appear in the Airflow interfaces. Note, DAG is case sensitive in Python code."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-on-the-command-line",
    "href": "tutorials/DC_airflow.html#dags-on-the-command-line",
    "title": "Datacamp Airflow Course",
    "section": "DAGs on the command line",
    "text": "DAGs on the command line\nWhen working with DAGs (and Airflow in general), you’ll often want to use the airflow command line tool. The airflow command line program contains many subcommands that handle various aspects of running Airflow. You’ve used a couple of these already in previous exercises. Use the\nairflow -h\ncommand for help and descriptions of the subcommands. Many of these subcommands are related to DAGs. You can use the\nairflow list_dags\noption to see all recognized DAGs in an installation. When in doubt, try a few different commands to find the information you’re looking for."
  },
  {
    "objectID": "tutorials/DC_airflow.html#command-line-vs-python",
    "href": "tutorials/DC_airflow.html#command-line-vs-python",
    "title": "Datacamp Airflow Course",
    "section": "Command line vs Python",
    "text": "Command line vs Python\nYou may be wondering when to use the Airflow command line tool vs writing Python.\n\nCommand line vs Python\n\n\nCommand line\nPython\n\n\n\n\nStart Airflow processes\nCreate a DAGs\n\n\nManually run DAGs / tasks\nEdit individual prop of DAG\n\n\nReview logging information\n\n\n\n\nIn general, the airflow command line program is used to start Airflow processes (ie, webserver or scheduler), manually run DAGs or tasks, and review logging information. Python code itself is usually used in the creation and editing of a DAG, not to mention the actual data processing code itself."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-1",
    "href": "tutorials/DC_airflow.html#exercises-1",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Defining a simple DAG\nYou’ve spent some time reviewing the Airflow components and are interested in testing out your own workflows. To start you decide to define the default arguments and create a DAG object for your workflow.\nThe DateTime object has been imported for you.\n\nImport the Airflow DAG object. Note that it is case-sensitive.\nDefine the default_args dictionary with a key owner and a value of ‘dsmith’. Add a start_date of January 14, 2020 to default_args using the value 1 for the month of January. Add a retries count of 2 to default_args.\nInstantiate the DAG object to a variable called etl_dag with a DAG named example_etl. Add the default_args dictionary to the appropriate argument.\n\n\n# Import the DAG object\nfrom datetime import datetime\nfrom airflow.models import DAG\n\n# Define the default_args dictionary\ndefault_args = {\n  'owner'       : 'dsmith',\n  'start_date'  : datetime(2020, 1, 14),\n  'retries'     : 2\n}\n\n# Instantiate the DAG object\netl_dag = DAG(\n    'example_etl', \n    default_args=default_args\n)\n\n\n\nExercise 2: Working with DAGs and the Airflow shell\nWhile working with Airflow, sometimes it can be tricky to remember what DAGs are defined and what they do. You want to gain some further knowledge of the Airflow shell command so you’d like to see what options are available.\nMultiple DAGs are already defined for you. How many DAGs are present in the Airflow system from the command-line?\n\n!airflow dags list\n\n\nPlease confirm database upgrade (or wait 4 seconds to skip it). Are you sure? [y/N]\nNo data found"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dags-view-dags",
    "href": "tutorials/DC_airflow.html#dags-view-dags",
    "title": "Datacamp Airflow Course",
    "section": "DAGs view DAGs",
    "text": "DAGs view DAGs\nIt provides a quick status of the number of DAGs / workflows available.\n\n\nIt shows us the schedule for the DAG (in date or cron format).\n\nWe can see the owner of the DAG.\n\nwhich of the most recent tasks have run,\n\nwhen the last run started,\n\nand the last three DAG runs.\n\nThe links area on the right gives us quick access to many of the DAG specific views.\n\nDon’t worry about those for now - instead we’ll click on the “example_dag” link which takes us to our DAG detail page."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-detail-view",
    "href": "tutorials/DC_airflow.html#dag-detail-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG detail view",
    "text": "DAG detail view\nThe DAG detail view gives us specific access to information about the DAG itself, including several views of information (Graph, Tree, and Code) illustrating the tasks and dependencies in the code. We also get access to the Task duration, task tries, timings, a Gantt chart view, and specific details about the DAG. We have the ability to trigger the DAG (to start), refresh our view, and delete the DAG if we desire. The detail view defaults to the Tree view, showing the specific named tasks, which operators are in use, and any dependencies between tasks. The circles in front of the words represent the state of the task / DAG. In the case of our specific DAG, we see that we have one task called generate_random_number."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-graph-view",
    "href": "tutorials/DC_airflow.html#dag-graph-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG graph view",
    "text": "DAG graph view\nThe DAG graph view arranges the tasks and dependencies in a chart format - this provides another view into the flow of the DAG. You can see the operators in use and the state of the tasks at any point in time. The tree and graph view provide different information depending on what you’d like to know. Try moving between them when examining a DAG to obtain further details. For this view we again see that we have a task called generate_random_number. We can also see that it is of the type BashOperator in the middle left of the image."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-code-view",
    "href": "tutorials/DC_airflow.html#dag-code-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG code view",
    "text": "DAG code view\nThe DAG code view does exactly as it sounds - it provides a copy of the Python code that makes up the DAG. The code view provides easy access to exactly what defines the DAG without clicking in various portions of the UI. As you use Airflow, you’ll determine which tools work best for you. It is worth noting that the code view is read-only. Any DAG code changes must be done via the actual DAG script. In this view, we can finally see the code making up the generate_random_number task and that it runs the bash command echo $RANDOM."
  },
  {
    "objectID": "tutorials/DC_airflow.html#logs",
    "href": "tutorials/DC_airflow.html#logs",
    "title": "Datacamp Airflow Course",
    "section": "Logs",
    "text": "Logs\nThe Logs page, under the Browse menu option, provides troubleshooting and audit ability while using Airflow. This includes items such as starting the Airflow webserver, viewing the graph or tree nodes, creating users, starting DAGs, etc. When using Airflow, look at the logs often to become more familiar with the types of information included, and also what happens behind the scenes of an Airflow install.\n\n\n\n\n\n\nNote that you’ll often refer to the Event type present on the Logs view when searching (such as graph, tree, cli scheduler)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#web-ui-vs-command-line",
    "href": "tutorials/DC_airflow.html#web-ui-vs-command-line",
    "title": "Datacamp Airflow Course",
    "section": "Web UI vs command line",
    "text": "Web UI vs command line\nIn most circumstances, you can choose between using the Airflow web UI or the command line tool based on your preference. The web UI is often easier to use overall. The command line tool may be simpler to access depending on settings (via SSH, etc.)"
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-2",
    "href": "tutorials/DC_airflow.html#exercises-2",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Examining DAGs with the Airflow UI\nYou’ve become familiar with the basics of an Airflow DAG and the basics of interacting with Airflow on the command-line. Your boss would like you to show others on your team how to examine any available DAGs. In this instance, she would like to know which operator is NOT in use with the DAG called update_state, as your team is trying to verify the components used in production workflows.\nRemember that the Airflow UI allows various methods to view the state of DAGs. The Tree View lists the tasks and any ordering between them in a tree structure, with the ability to compress / expand the nodes. The Graph View shows any tasks and their dependencies in a graph structure, along with the ability to access further details about task runs. The Code view provides full access to the Python code that makes up the DAG.\nRemember to select the operator NOT used in this DAG.\n\n\nBashOperator\nPythonOperator\nJdbcOperator\nSimpleHttpOperator"
  },
  {
    "objectID": "tutorials/DC_airflow.html#bashoperator",
    "href": "tutorials/DC_airflow.html#bashoperator",
    "title": "Datacamp Airflow Course",
    "section": "BashOperator",
    "text": "BashOperator\nThe BashOperator executes a given Bash command or script.\nBashOperator(    \n    task_id='bash_example',    \n    bash_command='echo \"Example!\"',    \n    dag=ml_dag\n)\n\nBashOperator(    \n    task_id='bash_script_example',    \n    bash_command='runcleanup.sh',    \n    dag=ml_dag\n)\nThis command can be pretty much anything Bash is capable of that would make sense in a given workflow. The BashOperator requires three arguments: the task id which is the name that shows up in the UI, the bash command (the raw command or script), and the dag it belongs to. The BashOperator runs the command in a temporary directory that gets automatically cleaned up afterwards. It is possible to specify environment variables for the bash command to try to replicate running the task as you would on a local system. If you’re unfamiliar with environment variables, these are run-time settings interpreted by the shell. It provides flexibility while running scripts in a generalized way. The first example runs a bash command to echo Example exclamation mark to standard out. The second example uses a predefined bash script for its command, runcleanup.sh."
  },
  {
    "objectID": "tutorials/DC_airflow.html#bashoperator-examples",
    "href": "tutorials/DC_airflow.html#bashoperator-examples",
    "title": "Datacamp Airflow Course",
    "section": "BashOperator examples",
    "text": "BashOperator examples\nBefore using the BashOperator, it must be imported:\nfrom airflow.operators.bash_operator import BashOperator\n\n...\n\nexample_task = BashOperator(\n    task_id = 'example',\n    bash_command = 'echo 1',\n    dag = dag,\n)\nThe first example creates a BashOperator that takes a task_id, runs the bash command “echo 1”, and assigns the operator to the dag. ::: {.callout-caution collapse=“true” apperance=‘simple’} Note that we’ve previously defined the dag in an earlier exercise. :::\nThe second example is a BashOperator to run a quick data cleaning operation using cat and awk.\nbash_task = BashOperator(\n    task_id = 'clean_addresses',\n    bash_command = 'cat addresses.txt | awk \"NF==10\" &gt; cleaned.txt',\n    dag = dag,\n)\nDon’t worry if you don’t understand exactly what this is doing. This is a common scenario when running workflows - you may not know exactly what a command does, but you can still run it in a reliable way."
  },
  {
    "objectID": "tutorials/DC_airflow.html#operator-gotchas",
    "href": "tutorials/DC_airflow.html#operator-gotchas",
    "title": "Datacamp Airflow Course",
    "section": "Operator gotchas",
    "text": "Operator gotchas\nThere are some general gotchas when using Operators. The biggest is that individual operators are not guaranteed to run in the same location or environment. This means that just because one operator ran in a given directory with a certain setup, it does not necessarily mean that the next operator will have access to that same information. If this is required, you must explicitly set it up. You may need to set up environment variables, especially for the BashOperator.\n\nFor example, it’s common in bash to use the tilde character to represent a home directory. This is not defined by default in Airflow.\n\n\nAnother example of an environment variable could be AWS credentials, database connectivity details, or other information specific to running a script.\n\nFinally, it can also be tricky to run tasks with any form of elevated privilege. This means that any access to resources must be setup for the specific user running the tasks. If you’re uncertain what elevated privileges are, think of running a command as root or the administrator on a system."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-3",
    "href": "tutorials/DC_airflow.html#exercises-3",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExericse 1: Defining a BashOperator task\nThe BashOperator allows you to specify any given Shell command or script and add it to an Airflow workflow. This can be a great start to implementing Airflow in your environment.\nAs such, you’ve been running some scripts manually to clean data (using a script called cleanup.sh) prior to delivery to your colleagues in the Data Analytics group. As you get more of these tasks assigned, you’ve realized it’s becoming difficult to keep up with running everything manually, much less dealing with errors or retries. You’d like to implement a simple script as an Airflow operator.\nThe Airflow DAG analytics_dag is already defined for you and has the appropriate configurations in place.\n\nImport the BashOperator object.\nDefine a BashOperator called cleanup with the task_id of cleanup_task.\nUse the command cleanup.sh.\nAdd the operator to the DAG.\n\n# Import the BashOperator\nfrom airflow.operators.bash_operator import BashOperator\n\n# Define the BashOperator \ncleanup = BashOperator(\n    task_id='cleanup_task',\n    # Define the bash_command\n    bash_command='cleanup.sh',\n    # Add the task to the dag\n    dag=analytics_dag,\n)\n\n\nExercise 2: Multiple BashOperators\nAirflow DAGs can contain many operators, each performing their defined tasks.\nYou’ve successfully implemented one of your scripts as an Airflow task and have decided to continue migrating your individual scripts to a full Airflow DAG. You now want to add more components to the workflow. In addition to the cleanup.sh used in the previous exercise you have two more scripts, consolidate_data.sh and push_data.sh. These further process your data and copy to its final location.\nThe DAG analytics_dag is available as before, and your cleanup task is still defined. The BashOperator is already imported.\n\nDefine a BashOperator called consolidate, to run consolidate_data.sh with a task_id of consolidate_task.\nAdd a final BashOperator called push_data, running push_data.sh and a task_id of pushdata_task.\n\n# Define a second operator to run the `consolidate_data.sh` script\nconsolidate = BashOperator(\n    task_id = 'consolidate_task',\n    bash_command = 'consolidate_data.sh',\n    dag = analytics_dag,\n)\n\n# Define a final operator to execute the `push_data.sh` script\npush_data = BashOperator(\n    task_id = 'pushdata_task',\n    bash_command = 'push_data.sh',\n    dag = analytics_dag,\n)"
  },
  {
    "objectID": "tutorials/DC_airflow.html#task-dependencies",
    "href": "tutorials/DC_airflow.html#task-dependencies",
    "title": "Datacamp Airflow Course",
    "section": "Task dependencies",
    "text": "Task dependencies\nTask dependencies in Airflow define an order of task completion. While not required, task dependencies are usually present. If task dependencies are not defined, task execution is handled by Airflow itself with no guarantees of order. Task dependencies are referred to as upstream or downstream tasks. An upstream task means that it must complete prior to any downstream tasks. Since Airflow 1.8, task dependencies are defined using the bitshift operators. The upstream operator is two greater-than symbols. The downstream operator is two less-than symbols."
  },
  {
    "objectID": "tutorials/DC_airflow.html#upstream-vs-downstream",
    "href": "tutorials/DC_airflow.html#upstream-vs-downstream",
    "title": "Datacamp Airflow Course",
    "section": "Upstream vs Downstream",
    "text": "Upstream vs Downstream\nIt’s easy to get confused on when to use an upstream or downstream operator. The simplest analogy is that upstream means before and downstream means after. This means that any upstream tasks would need to complete prior to any downstream ones."
  },
  {
    "objectID": "tutorials/DC_airflow.html#simple-task-dependency",
    "href": "tutorials/DC_airflow.html#simple-task-dependency",
    "title": "Datacamp Airflow Course",
    "section": "Simple task dependency",
    "text": "Simple task dependency\nLet’s look at a simple example involving two bash operators. We define our first task, and assign it to the variable task1. We then create our second task and assign it to the variable task2. Once each operator is defined and assigned to a variable, we can define the task order using the bitshift operators. In this case, we want to run task1 before task2.\n# Define the tasks\ntask1 = BashOperator(\n    task_id='first_task',                     \n    bash_command='echo 1',                     \n    dag=example_dag)\n\ntask2 = BashOperator(\n    task_id='second_task',                     \n    bash_command='echo 2',                     \n    dag=example_dag)\n    \n# Set first_task to run before second_task\ntask1 &gt;&gt; task2   # or task2 &lt;&lt; task1\nThe most readable method for this is using the upstream operator, two greater-than symbols, as task1 upstream operator task2. ::: {.callout-caution collapse=“true” apperance=‘simple’} Note that you could also define this in reverse using the downstream operator to accomplish the same thing. In this case, it’d be task2 two less-than symbols task1. :::"
  },
  {
    "objectID": "tutorials/DC_airflow.html#task-dependencies-in-the-airflow-ui",
    "href": "tutorials/DC_airflow.html#task-dependencies-in-the-airflow-ui",
    "title": "Datacamp Airflow Course",
    "section": "Task dependencies in the Airflow UI",
    "text": "Task dependencies in the Airflow UI\nLet’s take a look at what the Airflow UI shows for tasks and their dependencies. In this case, we’re looking at the graph view within the Airflow web interface. ::: {.callout-caution collapse=“true” apperance=‘simple’} Note that in the task area, our two tasks, first_task and second_task, are both present, but there is no order to the task execution. This is the DAG prior to setting the task dependency using the bitshift operator. :::\n\nNow let’s look again at the view with a defined order via the bitshift operators. The view is similar but we can see the order of tasks indicated by the directed arrow between first underscore task and second underscore task."
  },
  {
    "objectID": "tutorials/DC_airflow.html#multiple-dependencies",
    "href": "tutorials/DC_airflow.html#multiple-dependencies",
    "title": "Datacamp Airflow Course",
    "section": "Multiple dependencies",
    "text": "Multiple dependencies\nDependencies can be as complex as required to define the workflow to your needs. We can chain a dependency, in this case setting task1 upstream of task2 upstream of task3 upstream of task4. The Airflow graph view shows a dependency view indicating this order.\ntask1 &gt;&gt; task2 &gt;&gt; task3 &gt;&gt; task4\n\nYou can also mix upstream and downstream bitshift operators in the same workflow. If we define task1 upstream of task2 then downstream of task3, we get a configuration different than what we might expect.\ntask1 &gt;&gt; task2 &lt;&lt; task3\n\n## or\ntask1 &gt;&gt; task2\ntask3 &gt;&gt; task2\n\nThis creates a DAG where first underscore task and third underscore task must finish prior to second underscore task. This means we could define the same dependency graph on two lines, in a possibly clearer form. task1 upstream of task2. task3 upstream of task2.\n\n\n\n\n\n\nNote that because we don’t require it, either task1 or task3 could run first depending on Airflow’s scheduling."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-4",
    "href": "tutorials/DC_airflow.html#exercises-4",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Define order of BashOperators\nNow that you’ve learned about the bitshift operators, it’s time to modify your workflow to include a pull step and to include the task ordering. You have three currently defined components, cleanup, consolidate, and push_data.\nThe DAG analytics_dag is available as before and the BashOperator is already imported.\n\nDefine a BashOperator called pull_sales with a bash command of wget https://salestracking/latestinfo?json.\nSet the pull_sales operator to run before the cleanup task.\nConfigure consolidate to run next, using the downstream operator.\nSet push_data to run last using either bitshift operator.\n\n# Define a new pull_sales task\npull_sales = BashOperator(\n    task_id = 'pullsales_task',\n     bash_command = 'wget https://salestracking/latestinfo?json',\n    dag = analytics_dag,\n)\n\n# Set pull_sales to run prior to cleanup\npull_sales &gt;&gt; cleanup\n\n# Configure consolidate to run after cleanup\ncleanup &gt;&gt; consolidate\n\n# Set push_data to run last\nconsolidate &gt;&gt; push_data\n\n\nExercise 2: Determining the order of tasks\nWhile looking through a colleague’s workflow definition, you’re trying to decipher exactly in which order the defined tasks run. The code in question shows the following:\npull_data &lt;&lt; initialize_process\npull_data &gt;&gt; clean &gt;&gt; run_ml_pipeline\ngenerate_reports &lt;&lt; run_ml_pipeline\nOrder the tasks in the sequence defined by the bitshift code, with the first task to run on top and the last task to run on the bottom.\n\ninitialize_process\npull_data\nclean\nrun_ml_pipeline\ngenerate_reports\n\n\n\nExercise 3: Troubleshooting DAG dependencies\nYou’ve created a DAG with intended dependencies based on your workflow but for some reason Airflow won’t load / execute the DAG. Try using the terminal to:\n\nList the DAGs.\nDecipher the error message.\nUse cat workspace/dags/codependent.py to view the Python code.\nDetermine which of the following lines should be removed from the Python code. You may want to consider the last line of the file.\n\n\n\ntask1 &gt;&gt; task2\ntask2 &gt;&gt; task3\ntask3 &gt;&gt; task1"
  },
  {
    "objectID": "tutorials/DC_airflow.html#pythonoperator",
    "href": "tutorials/DC_airflow.html#pythonoperator",
    "title": "Datacamp Airflow Course",
    "section": "PythonOperator",
    "text": "PythonOperator\nThe PythonOperator is similar to the BashOperator, except that it runs a Python function or callable method. Much like the BashOperator, it requires a taskid, a dag entry, and most importantly a python underscore callable argument set to the name of the function in question. You can also pass arguments or keyword style arguments into the Python callable as needed. Our first example shows a simple printme function that writes a message to the task logs.\nfrom airflow.operators.python_operator import PythonOperator\n\ndef printme():\n    print(\"This goes in the logs!\")\n\npython_task = PythonOperator(\n    task_id = 'simple_print',\n    python_callable = printme,\n    dag = example_dag,\n)\nWe must first import the PythonOperator from the airflow dot operators dot python underscore operator library. Afterwards, we create our function printme, which will write a quick log message when run. Once defined, we create the PythonOperator instance called python underscore task and add the necessary arguments."
  },
  {
    "objectID": "tutorials/DC_airflow.html#arguments",
    "href": "tutorials/DC_airflow.html#arguments",
    "title": "Datacamp Airflow Course",
    "section": "Arguments",
    "text": "Arguments\nThe PythonOperator supports adding arguments to a given task. This allows you to pass arguments that can then be passed to the Python function assigned to python callable. The PythonOperator supports both positional and keyword style arguments as options to the task. &gt; For this course, we’ll focus on using keyword arguments only for the sake of clarity.\nTo implement keyword arguments with the PythonOperator, we define an argument on the task called op_kwargs. This is a dictionary consisting of the named arguments for the intended Python function."
  },
  {
    "objectID": "tutorials/DC_airflow.html#op_kwargs-example",
    "href": "tutorials/DC_airflow.html#op_kwargs-example",
    "title": "Datacamp Airflow Course",
    "section": "op_kwargs example",
    "text": "op_kwargs example\nLet’s create a new function called sleep, which takes a length of time argument. It uses this argument to call the time dot sleep method. Once defined, we create a new task called sleep underscore task, with the taskid, dag, and python callable arguments added as before. This time we’ll add our op underscore kwargs dictionary with the length of time variable and the value of 5.\n## Function to sleep (needs argument to be passed)\ndef sleep(length_of_time):\n    time.sleep(length_of_time)\n\nsleep_task = PythonOperator(\n    task_id = 'sleep',\n    python_callable = sleep,\n    ## Pass argument to callable\n    op_kwargs = {'length_of_time': 5},\n    dag = example_dag,\n)\n\n\n\n\n\n\nNote that the dictionary key must match the name of the function argument. If the dictionary contains an unexpected key, it will be passed to the Python function and typically cause an unexpected keyword argument error."
  },
  {
    "objectID": "tutorials/DC_airflow.html#emailoperator",
    "href": "tutorials/DC_airflow.html#emailoperator",
    "title": "Datacamp Airflow Course",
    "section": "EmailOperator",
    "text": "EmailOperator\nThere are many other operators available within the Airflow ecosystem. The primary operators are in the airflow.operators or airflow.contrib.operators libraries.\nAnother useful operator is the EmailOperator, which as expected sends an email from within an Airflow task. It can contain the typical components of an email, including HTML content and attachments.\n\n\n\n\n\n\nNote that the Airflow system must be configured with the email server details to successfully send a message."
  },
  {
    "objectID": "tutorials/DC_airflow.html#emailoperator-example",
    "href": "tutorials/DC_airflow.html#emailoperator-example",
    "title": "Datacamp Airflow Course",
    "section": "EmailOperator example",
    "text": "EmailOperator example\nA quick example for sending an email would be sending a generated sales report upon completion of a workflow. We first must import the EmailOperator object from airflow dot operators dot email underscore operator. We can then create our EmailOperator instance with the task id, the to, subject, and content fields and a list of any files to attach.\nfrom airflow.operators.email_operator import EmailOperator\n\nemail_task = EmailOperator(\n    task_id = 'email_sales_report',\n    to = 'sales_manager@example.com',\n    subject = 'Automated Sales Report',\n    html_content = 'Attached is the latest sales report',\n    files = 'latest_sales.xlsx',\n    dag = example_dag\n)\n\n\n\n\n\n\nNote that in this case we assume the file latest underscore sales dot xlsx was previously generated - later in the course we’ll see how to verify that first. Finally we add it to our dag as usual."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-5",
    "href": "tutorials/DC_airflow.html#exercises-5",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Using the PythonOperator\nYou’ve implemented several Airflow tasks using the BashOperator but realize that a couple of specific tasks would be better implemented using Python. You’ll implement a task to download and save a file to the system within Airflow.\nThe requests library is imported for you, and the DAG process_sales_dag is already defined.\n\nDefine a function called pull_file with two parameters, URL and savepath.\nUse the print() function and Python f-strings to write a message to the logs.\nImport the necessary object to use the Python Operator.\nCreate a new task assigned to the variable pull_file_task, with the id pull_file.\nAdd the pull_file(URL, savepath) function defined previously to the operator.\nDefine the arguments needed for the task.\n\n# Define the method\ndef pull_file(URL, savepath):\n    r = requests.get(URL)\n    with open(savepath, 'wb') as f:\n        f.write(r.content)    \n    # Use the print method for logging\n    print(f\"File pulled from {URL} and saved to {savepath}\")\n\n## Import the PythonOperator\nfrom airflow.operators.python_operator import PythonOperator\n\n# Create the task\npull_file_task = PythonOperator(\n    task_id = 'pull_file',\n    # Add the callable\n    python_callable = pull_file,\n    # Define the arguments\n    op_kwargs = {\n        'URL' : 'http://dataserver/sales.json', \n        'savepath' : 'latestsales.json',\n    },\n    dag = process_sales_dag\n)\n\n\nExercise 2: More PythonOperators\nTo continue implementing your workflow, you need to add another step to parse and save the changes of the downloaded file. The DAG process_sales_dag is defined and has the pull_file task already added. In this case, the Python function is already defined for you, parse_file(inputfile, outputfile).\nNote that often when implementing Airflow tasks, you won’t necessarily understand the individual steps given to you. As long as you understand how to wrap the steps within Airflow’s structure, you’ll be able to implement a desired workflow.\n\nDefine the Python task to the variable parse_file_task with the id parse_file.\nAdd the parse_file(inputfile, outputfile) to the Operator.\nDefine the arguments to pass to the callable.\nAdd the task to the DAG.\n\n# Add another Python task\nparse_file_task = PythonOperator(\n    task_id='parse_file',\n    # Set the function to call\n    python_callable = parse_file,\n    # Add the arguments\n    op_kwargs = {\n        'inputfile':'latestsales.json', \n        'outputfile':'parsedfile.json',\n    },\n    # Add the DAG\n    dag = process_sales_dag,\n)\n    \n\n\nExercise 3: EmailOperator and dependencies\nNow that you’ve successfully defined the PythonOperators for your workflow, your manager would like to receive a copy of the parsed JSON file via email when the workflow completes. The previous tasks are still defined and the DAG process_sales_dag is configured.\n\nImport the class to send emails.\nDefine the Operator and add the appropriate arguments (to, subject, files).\nSet the task order so the tasks run sequentially (Pull the file, parse the file, then email your manager).\n\n# Import the Operator\nfrom airflow.operators.email_operator import EmailOperator\n\n# Define the task\nemail_manager_task = EmailOperator(\n    task_id = 'email_manager',\n    to = 'manager@datacamp.com',\n    subject = 'Latest sales JSON',\n    html_content = 'Attached is the latest sales JSON file as requested.',\n    files = 'parsedfile.json',\n    dag = process_sales_dag,\n)\n\n# Set the order of tasks\npull_file_task &gt;&gt; parse_file_task &gt;&gt; email_manager_task"
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-runs-view",
    "href": "tutorials/DC_airflow.html#dag-runs-view",
    "title": "Datacamp Airflow Course",
    "section": "DAG Runs view",
    "text": "DAG Runs view\nWithin the Airflow UI, you can view all DAG runs under the Browse: DAG Runs menu option. This provides the assorted details about any DAGs that have run within the current Airflow instance.\n\nAs mentioned, you can view the state of a DAG run within this page, illustrating whether the DAG run was successful or not."
  },
  {
    "objectID": "tutorials/DC_airflow.html#schedule-details",
    "href": "tutorials/DC_airflow.html#schedule-details",
    "title": "Datacamp Airflow Course",
    "section": "Schedule details",
    "text": "Schedule details\nWhen scheduling a DAG, there are many attributes to consider depending on your scheduling needs. The start_date value specifies the first time the DAG could be scheduled. This is typically defined with a Python datetime object. The end_date represents the last possible time to schedule the DAG. max_tries represents how many times to retry before fully failing the DAG run. The schedule_interval represents how often to schedule the DAG for execution. There are many nuances to this which we’ll cover in a moment."
  },
  {
    "objectID": "tutorials/DC_airflow.html#schedule-interval",
    "href": "tutorials/DC_airflow.html#schedule-interval",
    "title": "Datacamp Airflow Course",
    "section": "Schedule interval",
    "text": "Schedule interval\nThe schedule interval represents how often to schedule the DAG runs. The scheduling occurs between the start date and the potential end date. Note that this is not when the DAGs will absolutely run, but rather a minimum and maximum value of when they could be scheduled. The schedule interval can be defined by a couple methods - with a cron style syntax or via built-in presets."
  },
  {
    "objectID": "tutorials/DC_airflow.html#cron-syntax",
    "href": "tutorials/DC_airflow.html#cron-syntax",
    "title": "Datacamp Airflow Course",
    "section": "cron syntax",
    "text": "cron syntax\nThe cron syntax3 is the same as the format for scheduling jobs using the Unix cron tool.\n\nIt consists of five fields separated by a space, starting with the minute value (0 through 59), the hour (0 through 23), the day of the month (1 through 31), the month (1 through 12), and the day of week (0 through 6). An asterisk in any of the fields represents running for every interval (for example, an asterisk in the minute field means run every minute) A list of values can be given on a field via comma separated values."
  },
  {
    "objectID": "tutorials/DC_airflow.html#cron-examples",
    "href": "tutorials/DC_airflow.html#cron-examples",
    "title": "Datacamp Airflow Course",
    "section": "cron examples",
    "text": "cron examples\n0 12 * * *              # Run daily at Noon (12:00)\n* * 25 2 *              # Run once per minute, but only on February 25th\n0,15,30,45 * * * *      # Run every 15 minutes\nThe cron entry 0 12 asterisk asterisk asterisk means run daily at Noon (12:00) asterisk asterisk 25 2 asterisk represents running once per minute, but only on February 25th. 0 comma 15 comma 30 comma 45 asterisk asterisk asterisk asterisk means to run every 15 minutes."
  },
  {
    "objectID": "tutorials/DC_airflow.html#airflow-scheduler-presets",
    "href": "tutorials/DC_airflow.html#airflow-scheduler-presets",
    "title": "Datacamp Airflow Course",
    "section": "Airflow scheduler presets",
    "text": "Airflow scheduler presets\nAirflow has several presets, or shortcut syntax options representing often used time intervals.\n\n\n\nPreset\ncron equivalent\n\n\n\n\n@hour\n0 * * * *\n\n\n@daily\n0 0 * * *\n\n\n@weekly\n0 0 * * 0\n\n\n@monthly\n0 0 1 * *\n\n\n@yearly\n0 0 1 1 *\n\n\n\nThe @hourly preset means run once an hour at the beginning of the hour. It’s equivalent to 0 asterisk asterisk asterisk asterisk in cron. The @daily, @weekly, @monthly, and @yearly presets behave similarly.\n\n\n\n\n\n\nNote: “0 12 * * ” != “ 12 * * *”\n“0 12 * * *” This cron expression specifies a specific minute and hour. It means the task should run at 12:00 PM (noon) every day. The 0 in the first position represents the minute, and 12 in the second position represents the hour.\n“* 12 * * *” This cron expression uses an asterisk in the minute position, which means the task will run every minute of the 12th hour (noon) of every day. In other words, it will run once every minute between 12:00 PM and 12:59 PM."
  },
  {
    "objectID": "tutorials/DC_airflow.html#special-presets",
    "href": "tutorials/DC_airflow.html#special-presets",
    "title": "Datacamp Airflow Course",
    "section": "Special presets",
    "text": "Special presets\nAirflow also has two special presets for schedule intervals.\n\n@none means don’t ever schedule the DAG and is used for manually triggered workflows.\n@once means to only schedule a DAG once.\n\n\n\n\n\n\n\nScheduling DAGs has an important nuance to consider. When scheduling DAG runs, Airflow will use the start date as the earliest possible value, but not actually schedule anything until at least one schedule interval has passed beyond the start date. Given a start_date of February 25, 2020 and a @daily schedule interval, Airflow would then use the date of February 26, 2020 for the first run of the DAG. This can be tricky to consider when adding new DAG schedules, especially if they have longer schedule intervals."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-6",
    "href": "tutorials/DC_airflow.html#exercises-6",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Schedule a DAG via Python\nYou’ve learned quite a bit about creating DAGs, but now you would like to schedule a specific DAG on a specific day of the week at a certain time. You’d like the code include this information in case a colleague needs to reinstall the DAG to a different server.\nThe Airflow DAG object and the appropriate datetime methods have been imported for you.\n\nSet the start date of the DAG to November 1, 2019.\nConfigure the retry_delay to 20 minutes. You will learn more about the timedelta object in Chapter 3. For now, you just need to know it expects an integer value.\nUse the cron syntax to configure a schedule of every Wednesday at 12:30pm.\n\n# Update the scheduling arguments as defined\ndefault_args = {\n  'owner': 'Engineering',\n  'start_date': datetime(2019, 11, 1),\n  'email': ['airflowresults@datacamp.com'],\n  'email_on_failure': False,\n  'email_on_retry': False,\n  'retries': 3,\n  'retry_delay': timedelta(minutes=20)\n}\n\ndag = DAG(\n    'update_dataflows', \n    default_args=default_args, \n    schedule_interval = '30 12 * * 3',\n)\n\n\nExercise 2: Deciphering Airflow schedules\nGiven the various options for Airflow’s schedule_interval, you’d like to verify that you understand exactly how intervals relate to each other, whether it’s a cron format, timedelta object, or a preset.\n\nOrder the schedule intervals from least to greatest amount of time.\n\n\n\n\nExercise 3: Troubleshooting DAG runs\nYou’ve scheduled a DAG called process_sales which is set to run on the first day of the month and email your manager a copy of the report generated in the workflow. The start_date for the DAG is set to February 15, 2020. Unfortunately it’s now March 2nd and your manager did not receive the report and would like to know what happened.\nUse the information you’ve learned about Airflow scheduling to determine what the issue is.\n\n\nThe schedule_interval has not yet passed since the start_date.\nThe email_manager_task is not downstream of the other tasks.\nThe DAG run has an error.\nThe op_kwargs are incorrect for the EmailOperator."
  },
  {
    "objectID": "tutorials/DC_airflow.html#sensor-details",
    "href": "tutorials/DC_airflow.html#sensor-details",
    "title": "Datacamp Airflow Course",
    "section": "Sensor details",
    "text": "Sensor details\nAll sensors are derived from the airflow.sensors.base_sensor_operator class. There are some default arguments available to all sensors, including mode, poke_interval, and timeout. The mode tells the sensor how to check for the condition and has two options, poke or reschedule.\n\npoke: The default is poke, and means to continue checking until complete without giving up a worker slot.\nreschedule: Reschedule means to give up the worker slot and wait for another slot to become available.\n\nWe’ll discuss worker slots in the next lesson, but for now consider a worker slot to be the capability to run a task.\n\npoke_interval: The poke_interval is used in the poke mode, and tells Airflow how often to check for the condition. This is should be at least 1 minute to keep from overloading the Airflow scheduler.\ntimeout: The timeout field is how long to wait (in seconds) before marking the sensor task as failed. To avoid issues, make sure your timeout is significantly shorter than your schedule interval.\n\n\n\n\n\n\n\nNote that as sensors are operators, they also include normal operator attributes such as task_id and dag."
  },
  {
    "objectID": "tutorials/DC_airflow.html#file-sensor",
    "href": "tutorials/DC_airflow.html#file-sensor",
    "title": "Datacamp Airflow Course",
    "section": "File sensor",
    "text": "File sensor\nA useful sensor is the FileSensor, found in the airflow.contrib.sensors library. The FileSensor checks for the existence of a file at a certain location in the file system. It can also check for any files within a given directory. A quick example is importing the FileSensor object, then defining a task called file underscore sensor underscore task. We set the task_id and dag entries as usual.\nfrom airflow.contrib.sensors.file_sensor import FileSensor\n\nfile_sensor_task = FileSensor(\n    task_id='file_sense',\n    filepath='salesdata.csv',\n    poke_interval=300,\n    dag=sales_report_dag,\n)\n    \ninit_sales_cleanup &gt;&gt; file_sensor_task &gt;&gt; generate_report\nThe filepath argument is set to salesdata.csv, looking for a file with this filename to exist before continuing. We set the poke_interval to 300 seconds, or to repeat the check every 5 minutes until true.\nFinally, we use the bitshift operators to define the sensor’s dependencies within our DAG. In this case, we must run init_sales_cleanup, then wait for the file_sensor_task to finish, then we run generate_report."
  },
  {
    "objectID": "tutorials/DC_airflow.html#other-sensors",
    "href": "tutorials/DC_airflow.html#other-sensors",
    "title": "Datacamp Airflow Course",
    "section": "Other sensors",
    "text": "Other sensors\nThere are many types of sensors available within Airflow.\n\nExternalTaskSensor: The ExternalTaskSensor waits for a task in a separate DAG to complete. This allows a loose connection to other workflow tasks without making any one workflow too complex.\nHttpSensor: The HttpSensor will request a web URL and allow you define the content to check for.\nSqlSensor: The SqlSensor runs a SQL query to check for content.\n\nMany other sensors are available in the airflow.sensors and airflow.contrib.sensors libraries."
  },
  {
    "objectID": "tutorials/DC_airflow.html#why-sensors",
    "href": "tutorials/DC_airflow.html#why-sensors",
    "title": "Datacamp Airflow Course",
    "section": "Why sensors?",
    "text": "Why sensors?\nYou may be wondering when to use a sensor vs an operator.\nFor the most part, you’ll want to use a normal operator unless you have any of the following requirements:\n\nYou’re uncertain when a condition will be true.\nIf you know something will complete that day but it might vary by an hour or so, you can use a sensor to check until it is.\nIf you want to continue to check for a condition but not necessarily fail the entire DAG immediately. This provides some flexibility in defining your DAG.\nFinally, if you want to repeatedly run a check without adding cycles to your DAG, sensors are a good choice."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-7",
    "href": "tutorials/DC_airflow.html#exercises-7",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Sensors vs operators\nAs you’ve just learned about sensors, you want to verify you understand what they have in common with normal operators and where they differ.\nMove each entry into the Sensors, Operators, or Both bucket.\n\n\n\nExercise 2: Sensory deprivation\nYou’ve recently taken over for another Airflow developer and are trying to learn about the various workflows defined within the system. You come across a DAG that you can’t seem to make run properly using any of the normal tools.\nTry exploring the DAG for any information about what it might be looking for before continuing.\n\nThe DAG is waiting for the file salesdata_ready.csv to be present.\nThe DAG expects a response from the SimpleHttpOperator before starting.\npart1 needs a dependency added."
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-is-an-executor",
    "href": "tutorials/DC_airflow.html#what-is-an-executor",
    "title": "Datacamp Airflow Course",
    "section": "What is an executor?",
    "text": "What is an executor?\nIn Airflow, an executor is the component that actually runs the tasks defined within your workflows. Each executor has different capabilities and behaviors for running the set of tasks. Some may run a single task at a time on a local system, while others might split individual tasks among all the systems in a cluster. As mentioned in the previous lesson, this is often referred to as the number of worker slots available. We’ll discuss some of these in more detail soon, but a few examples of executors are: - SequentialExecutor - LocalExecutor - CeleryExecutor\nThis is not an exhaustive list, and you can also create your own executor if required (though we won’t cover that in this course)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#sequentialexecutor",
    "href": "tutorials/DC_airflow.html#sequentialexecutor",
    "title": "Datacamp Airflow Course",
    "section": "SequentialExecutor",
    "text": "SequentialExecutor\nThe SequentialExecutor is the default execution engine for Airflow. It runs only a single task at a time. This means having multiple workflows scheduled around the same timeframe may cause things to take longer than expected. The SequentialExecutor is useful for debugging as it’s fairly simple to follow the flow of tasks and it can also be used with some integrated development environments (though we won’t cover that here).\nThe most important aspect of the SequentialExecutor is that while it’s very functional for learning and testing, it’s not really recommended for production due to the limitations of task resources."
  },
  {
    "objectID": "tutorials/DC_airflow.html#localexecutor",
    "href": "tutorials/DC_airflow.html#localexecutor",
    "title": "Datacamp Airflow Course",
    "section": "LocalExecutor",
    "text": "LocalExecutor\nThe LocalExecutor is another option for Airflow that runs entirely on a single system. It basically treats each task as a process on the local system, and is able to start as many concurrent tasks as desired / requested / and permitted by the system resources (ie, CPU cores, memory, etc). This concurrency is the parallelism of the system, and it is defined by the user in one of two ways - either unlimited, or limited to a certain number of simultaneous tasks.\nDefined intelligently, the LocalExecutor is a good choice for a single production Airflow system and can utilize all the resources of a given host system."
  },
  {
    "objectID": "tutorials/DC_airflow.html#celeryexecutor",
    "href": "tutorials/DC_airflow.html#celeryexecutor",
    "title": "Datacamp Airflow Course",
    "section": "CeleryExecutor",
    "text": "CeleryExecutor\nThe last executor we’ll look at is the Celery executor. If you’re not familiar with Celery, it’s a general queuing system written in Python that allows multiple systems to communicate as a basic cluster. Using a CeleryExecutor, multiple Airflow systems can be configured as workers for a given set of workflows / tasks. You can add extra systems at any time to better balance workflows. The power of the CeleryExecutor is significantly more difficult to setup and configure. It requires a working Celery configuration prior to configuring Airflow, not to mention some method to share DAGs between the systems (ie, a git server, Network File System, etc). While it is more difficult to configure, the CeleryExecutor is a powerful choice for anyone working with a large number of DAGs and / or expects their processing needs to grow."
  },
  {
    "objectID": "tutorials/DC_airflow.html#determine-your-executor",
    "href": "tutorials/DC_airflow.html#determine-your-executor",
    "title": "Datacamp Airflow Course",
    "section": "Determine your executor",
    "text": "Determine your executor\nSometimes when developing Airflow workflows, you may want to know the executor being used. If you have access to the command line, you can determine this by: Looking at the appropriate airflow.cfg file. Search for the executor equal line, and it will specify the executor in use.\n\n\n\n\n\n\nNote that we haven’t discussed the airflow.cfg file in depth as we assume a configured Airflow instance in this course. The airflow.cfg file is where most of the configuration and settings for the Airflow instance are defined, including the type of executor."
  },
  {
    "objectID": "tutorials/DC_airflow.html#determine-your-executor-2",
    "href": "tutorials/DC_airflow.html#determine-your-executor-2",
    "title": "Datacamp Airflow Course",
    "section": "Determine your executor #2",
    "text": "Determine your executor #2\nYou can also determine the executor by running airflow list_dags from the command line. Within the first few lines, you should see an entry for which executor is in use (In this case, it’s the SequentialExecutor).\n\n! cat ~/airflow/airflow.cfg | grep \"executor = \"\n\nexecutor = SequentialExecutor"
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-8",
    "href": "tutorials/DC_airflow.html#exercises-8",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Determining the executor\nWhile developing your DAGs in Airflow, you realize you’re not certain the configuration of the system. Using the commands you’ve learned, determine which of the following statements is true.\n\nThis system can run 12 tasks at the same time.\nThis system can run one task at a time.\nThis system can run as many tasks as needed at a time.\n\n\n! cat ~/airflow/airflow.cfg | grep \"executor = \"\n\nexecutor = SequentialExecutor\n\n\nThe airflow.cfg file is configured to use the SequentialExecutor -&gt; one per time\n\n\nExercise 2: Executor implications\nYou’re learning quite a bit about running Airflow DAGs and are gaining some confidence at developing new workflows. That said, your manager has mentioned that on some days, the workflows are taking a lot longer to finish and asks you to investigate. She also mentions that the salesdata_ready.csv file is taking longer to generate these days and the time of day it is completed is variable.\nThis exercise requires information from the previous two lessons - remember the implications of the available arguments and modify the workflow accordingly.\n\n\n\n\n\n\nNote that for this exercise, you’re expected to modify one line of code, not add any extra code.\n\n\n\n\nDetermine the level of parallelism available on this system. You can do this by listing dags (airflow list_dags).\nLook at the source for the DAG file and fix which entry is causing the problem.\n\nairflow dags list\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='poke',\n    dag=report_dag\n)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck &gt;&gt; generate_report_task\nuse FileSensor with mode=‘reschedule’ instead of ‘poke’\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='reschedule',\n    dag=report_dag\n)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck &gt;&gt; generate_report_task"
  },
  {
    "objectID": "tutorials/DC_airflow.html#typical-issues",
    "href": "tutorials/DC_airflow.html#typical-issues",
    "title": "Datacamp Airflow Course",
    "section": "Typical issues…",
    "text": "Typical issues…\nThere are several common issues you may run across while working with Airflow - it helps to have an idea of what these might be and how best handle them. - The first common issue is a DAG or DAGs that won’t run on schedule. - The next is a DAG that simply won’t load into the system. - The last common scenario involves syntax errors.\nLet’s look at these more closely."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-wont-run-on-schedule",
    "href": "tutorials/DC_airflow.html#dag-wont-run-on-schedule",
    "title": "Datacamp Airflow Course",
    "section": "DAG won’t run on schedule",
    "text": "DAG won’t run on schedule\nThe most common reason why a DAG won’t run on schedule is the scheduler is not running. Airflow contains several components that accomplish various aspects of the system. The Airflow scheduler handles DAG run and task scheduling. If it is not running, no new tasks can run. You’ll often see this error within the web UI if the scheduler component is not running.\n\nYou can easily fix this issue by running airflow scheduler from the command-line."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-wont-run-on-schedule-1",
    "href": "tutorials/DC_airflow.html#dag-wont-run-on-schedule-1",
    "title": "Datacamp Airflow Course",
    "section": "DAG won’t run on schedule",
    "text": "DAG won’t run on schedule\n\nAs we’ve covered before, another common issue with scheduling is the scenario where at least one schedule interval period has not passed since either the start date or the last DAG run. There isn’t a specific fix for this, but you might want to modify the start date or schedule interval to meet your requirements.\nThe last scheduling issue you’ll often see is related to what we covered in the previous lesson - the executor does not have enough free slots to run tasks. There are basically three ways to alleviate this problem - by changing the executor type to something capable of more tasks (LocalExecutor or CeleryExecutor), by adding systems or system resources (RAM, CPUs), or finally by changing the scheduling of your DAGs."
  },
  {
    "objectID": "tutorials/DC_airflow.html#dag-wont-load",
    "href": "tutorials/DC_airflow.html#dag-wont-load",
    "title": "Datacamp Airflow Course",
    "section": "DAG won’t load",
    "text": "DAG won’t load\nYou’ll often see an issue where a new DAG will not appear in your DAG view of the web UI or in the airflow list_dags output. The first thing to check is that the python file is in the expected DAGs folder or directory. You can determine the current DAGs folder setting by examining the airflow.cfg file. The line dags underscore folder will indicate where Airflow expects to find your Python DAG files.\n\n\n\n\n\n\n\nNote that the folder path must be an absolute path."
  },
  {
    "objectID": "tutorials/DC_airflow.html#syntax-errors",
    "href": "tutorials/DC_airflow.html#syntax-errors",
    "title": "Datacamp Airflow Course",
    "section": "Syntax errors",
    "text": "Syntax errors\nProbably the most common reason a DAG workflow won’t appear in your DAG list is one or more syntax errors in your python code. These are sometimes difficult to find, especially in an editor not setup for Python / Airflow (such as a base Vim install). I tend to prefer using Vim with some Python tools loaded, or VSCode but it’s really up to your preference. There are two quick methods to check for these issues - airflow list_dags, and running your DAG script with python."
  },
  {
    "objectID": "tutorials/DC_airflow.html#airflow-list_dags",
    "href": "tutorials/DC_airflow.html#airflow-list_dags",
    "title": "Datacamp Airflow Course",
    "section": "airflow list_dags",
    "text": "airflow list_dags\nThe first is to run airflow space list underscore dags.\n\nAs we’ve seen before, Airflow will output some debugging information and the list of DAGs it’s processed. If there are any errors, those will appear in the output, helping you to troubleshoot further."
  },
  {
    "objectID": "tutorials/DC_airflow.html#running-the-python-interpreter",
    "href": "tutorials/DC_airflow.html#running-the-python-interpreter",
    "title": "Datacamp Airflow Course",
    "section": "Running the Python interpreter",
    "text": "Running the Python interpreter\nAnother method to verify Python syntax is to run the actual python3 interpreter against the file. You won’t see any output normally as there’s nothing for the interpreter to do, but it can check for any syntax errors in your code. If there are errors, you’ll get an appropriate error message. If there are no errors, you’ll be returned to the command prompt."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-9",
    "href": "tutorials/DC_airflow.html#exercises-9",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: DAGs in the bag\nYou’ve taken over managing an Airflow cluster that you did not setup and are trying to learn a bit more about the system. Which of the following is true?\n\n\nThe DAG is scheduled for hourly processing.\nThe Airflow user does not have proper permissions.\nThe dags_folder is set to /home/repl/workspace/dags.\n\n\n\nExercise 2: Missing DAG\nYour manager calls you before you’re about to leave for the evening and wants to know why a new DAG workflow she’s created isn’t showing up in the system. She needs this DAG called execute_report to appear in the system so she can properly schedule it for some tests before she leaves on a trip.\nAirflow is configured using the ~/airflow/airflow.cfg file.\n\nExamine the DAG for any errors and fix those.\nDetermine if the DAG has loaded after fixing the errors.\nIf not, determine why the DAG has not loaded and fix the final issue.\n\nfrom airflow.models import DAG\n# from airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='poke',\n    dag=report_dag)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck &gt;&gt; generate_report_task\nremove the comment from the line 2\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\"\n)\n\nprecheck = FileSensor(\n    task_id='check_for_datafile',\n    filepath='salesdata_ready.csv',\n    start_date=datetime(2020,2,20),\n    mode='poke',\n    dag=report_dag)\n\ngenerate_report_task = BashOperator(\n    task_id='generate_report',\n    bash_command='generate_report.sh',\n    start_date=datetime(2020,2,20),\n    dag=report_dag\n)\n\nprecheck &gt;&gt; generate_report_task"
  },
  {
    "objectID": "tutorials/DC_airflow.html#sla-misses",
    "href": "tutorials/DC_airflow.html#sla-misses",
    "title": "Datacamp Airflow Course",
    "section": "SLA Misses",
    "text": "SLA Misses\nTo view any given SLA miss, you can access it in the web UI, via the Browse: SLA Misses link. It provides you general information about what task missed the SLA and when it failed. It also indicates if an email has been sent when the SLA failed."
  },
  {
    "objectID": "tutorials/DC_airflow.html#defining-slas",
    "href": "tutorials/DC_airflow.html#defining-slas",
    "title": "Datacamp Airflow Course",
    "section": "Defining SLAs",
    "text": "Defining SLAs\nThere are several ways to define an SLA but we’ll only look at two for this course. The first is via an sla argument on the task itself. This takes a timedelta object with the amount of time to pass.\ntask1 = BashOperator(\n    task_id = 'sla_task',\n    bash_command = 'runcode.sh',\n    sla = timedelta(seconds = 30),\n    dag = dag,\n)\nThe second way is using the default_args dictionary and defining an sla key.\ndefault_args = {\n    'sla': timedelta(minutes=20),\n    'start_date': datetime(2020,2,20)\n},\n\ndag = DAG(\n    'sla_dag', \n    default_args = default_args\n)\nThe dictionary is then passed into the default_args argument of the DAG and applies to any tasks internally."
  },
  {
    "objectID": "tutorials/DC_airflow.html#timedelta-object",
    "href": "tutorials/DC_airflow.html#timedelta-object",
    "title": "Datacamp Airflow Course",
    "section": "timedelta object",
    "text": "timedelta object\nWe haven’t covered the timedelta object yet so let’s look at some of the details. It’s found in the datetime library, along with the datetime object. Most easily accessed with an import statement of from datetime import timedelta. Takes arguments of days, seconds, minutes, hours, and weeks.\ntimedelta(seconds=30)\ntimedelta(weeks=2)\ntimedelta(days=4, hours=10, minutes=20, seconds=30)\nIt also has milliseconds and microseconds available, but those wouldn’t apply to Airflow. To create the object, you simply call timedelta with the argument or arguments you wish to reference. To create a 30 second time delta, call it with seconds equals 30. Or weeks equals 2. Or you can combine it into a longer mix of any of the arguments you wish (in this case, 4 days, 10 hours, 20 minutes, and 30 seconds)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#general-reporting",
    "href": "tutorials/DC_airflow.html#general-reporting",
    "title": "Datacamp Airflow Course",
    "section": "General reporting",
    "text": "General reporting\nFor reporting purposes you can use email alerting built into Airflow. There are a couple ways to do this. Airflow has built-in options for sending messages:\n\nsuccess\nfailure\nerror\nretry\n\nThese are handled via keys in the default_args dictionary that gets passed on DAG creation. The required component is the list of emails assigned to the email key. Then there are boolean options for email underscore on underscore failure, email underscore on underscore retry, and email underscore on underscore success.\ndefault_args = {\n    'email': ['address@domain.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'email_on_success': True,\n    ...\n}\nIn addition, we’ve already looked at the EmailOperator earlier but this is useful for sending emails outside of one of the defined Airflow options. Note that sending an email does require configuration within Airflow that is outside the scope of this course. The Airflow documentation provides information on how to set up the global email configuration."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-10",
    "href": "tutorials/DC_airflow.html#exercises-10",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Defining an SLA\nYou’ve successfully implemented several Airflow workflows into production, but you don’t currently have any method of determining if a workflow takes too long to run. After consulting with your manager and your team, you decide to implement an SLA at the DAG level on a test workflow.\nAll appropriate Airflow libraries have been imported for you.\n\nImport the timedelta object.\nDefine an SLA of 30 minutes.\nAdd the SLA to the DAG.\n\n# Import the timedelta object\nfrom datetime import timedelta\n\n# Create the dictionary entry\ndefault_args = {\n  'start_date': datetime(2020, 2, 20),\n  'sla': timedelta(minutes=30),\n}\n\n# Add to the DAG\ntest_dag = DAG(\n  'test_workflow',\n  default_args=default_args,\n  schedule_interval='@None',\n)\n\n\nExercise 2: Defining a task SLA\nAfter completing the SLA on the entire workflow, you realize you really only need the SLA timing on a specific task instead of the full workflow.\nThe appropriate Airflow libraries are imported for you.\n\nImport the timedelta object.\nAdd a 3 hour SLA to the task object\n\n# Import the timedelta object\nfrom datetime import timedelta\n\ntest_dag = DAG(\n    'test_workflow', \n    start_date=datetime(2020,2,20), \n    schedule_interval='@None'\n)\n\n# Create the task with the SLA\ntask1 = BashOperator(\n    task_id = 'first_task',\n    sla = timedelta(hours = 3),\n    bash_command = 'initialize_data.sh',\n    dag = test_dag\n)\n\n\nExercise 3: Generate and email a report\nAirflow provides the ability to automate almost any style of workflow. You would like to receive a report from Airflow when tasks complete without requiring constant monitoring of the UI or log files. You decide to use the email functionality within Airflow to provide this message.\nAll the typical Airflow components have been imported for you, and a DAG is already defined as dag.\n\nDefine the proper operator for the email_report task.\nFill the missing details for the Operator. Use the file named monthly_report.pdf.\nSet the email_report task to occur after the generate_report task.\n\n# Define the email task\nemail_report = EmailOperator(\n        task_id = 'email_report',\n        to = 'airflow@datacamp.com',\n        subject = 'Airflow Monthly Report',\n        html_content = \"\"\"\n        Attached is your monthly workflow report - please refer to it for more detail\n        \"\"\",\n        files = ['monthly_report.pdf'],\n        dag = report_dag,\n)\n\n# Set the email task to run after the report is generated\nemail_report &lt;&lt; generate_report\n\n\nExercise 4: Adding status emails\nYou’ve worked through most of the Airflow configuration for setting up your workflows, but you realize you’re not getting any notifications when DAG runs complete or fail. You’d like to setup email alerting for the success and failure cases, but you want to send it to two addresses.\n\nEdit the execute_report_dag.py workflow.\nAdd the emails airflowalerts@datacamp.com and airflowadmin@datacamp.com to the appropriate key in default_args.\nSet the failure email option to True.\nConfigure the success email to send you messages as well.\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom datetime import datetime\n\ndefault_args={\n    'email': [\n        'airflowalerts@datacamp.com',\n        'airflowadmin@datacamp.com',\n        ...\n    ],\n    'email_on_failure': True,\n    'email_on_success': True,\n}\n\nreport_dag = DAG(\n    dag_id = 'execute_report',\n    schedule_interval = \"0 0 * * *\",\n    default_args = default_args\n)\n\nprecheck = FileSensor(\n    task_id = 'check_for_datafile',\n    filepath = 'salesdata_ready.csv',\n    start_date = datetime(2020,2,20),\n    mode = 'reschedule',\n    dag = report_dag\n)\n\ngenerate_report_task = BashOperator(\n    task_id = 'generate_report',\n    bash_command = 'generate_report.sh',\n    start_date = datetime(2020,2,20),\n    dag = report_dag,\n)\n\nprecheck &gt;&gt; generate_report_task"
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-are-templates",
    "href": "tutorials/DC_airflow.html#what-are-templates",
    "title": "Datacamp Airflow Course",
    "section": "What are templates?",
    "text": "What are templates?\nYou may be wondering what templates are and what they do in the case of Airflow.\n\nTemplates allow substitution of information during a DAG run. In other words, every time a DAG with templated information is executed, information is interpreted and included with the DAG run.\nTemplates provide added flexibility when defining tasks. We’ll see examples of this shortly.\nTemplates are created using the Jinja templating language. A full explanation of Jinja is out of scope for this course, but we’ll cover some basics in the coming slides."
  },
  {
    "objectID": "tutorials/DC_airflow.html#non-templated-bashoperator-example",
    "href": "tutorials/DC_airflow.html#non-templated-bashoperator-example",
    "title": "Datacamp Airflow Course",
    "section": "Non-Templated BashOperator example",
    "text": "Non-Templated BashOperator example\nBefore we get specifically into a templated example, let’s consider what we would do for the following requirement. Your manager has asked you to simply echo the word “Reading” and a list of files to a log / output / etc. If we were to do this with what we currently know about Airflow, we would likely create multiple tasks using the BashOperator. Our first task would setup the task with the intended bash command - in this case echo Reading file1 dot txt, as an argument to the BashOperator.\nt1 = BashOperator(\n    task_id = 'first_task',\n    bash_command = 'echo \"Reading file1.txt\"',\n    dag = dag,\n)\nIf we had a second file, we would create a second task using the bash command echo Reading file2 dot txt.\nt2 = BashOperator(\n    task_id = 'second_task',\n    bash_command = 'echo \"Reading file2.txt\"',\n    dag = dag,\n)\nThis type of code would continue for the entire list of files. Consider what this would look like if we had 5, 10, or even 100+ files we needed to process. There would be a lot of repetitive code. Not to mention what if you needed to change the command being used / etc."
  },
  {
    "objectID": "tutorials/DC_airflow.html#templated-bashoperator-example",
    "href": "tutorials/DC_airflow.html#templated-bashoperator-example",
    "title": "Datacamp Airflow Course",
    "section": "Templated BashOperator example",
    "text": "Templated BashOperator example\nLet’s take a look at how we would accomplish the same behavior with templates. First, we need to create a variable containing our template - which is really just a string with some specialized formatting.\nOur string is the actual bash command echo and instead of the file name, we’re using two open curly braces, the term params dot filename, and then two closing curly braces. The curly braces when used in this manner represent information to be substituted. This will make more sense in a moment. If you’ve done any web development or worked with any reporting tools, you’ve likely worked with something similar. Next, we create our Airflow task as we have previously. We assign a task_id and dag argument as usual, but our bashcommand looks a little different. We set the bashcommand to use the templated command string we defined earlier. We also have an additional argument called params. In this case, params is a dictionary containing a single key filename with the value file1 dot txt. Now, if you look back at the templated command, you’ll notice that the term in the curly braces is params.filename.\ntemplated_command = \"\"\"\n    echo \"Reading {{ params.filename }}\"\n\"\"\"\nt_i = BashOperator(\n    task_id = 'templated_task',\n    bash_command = templated_command,\n    params = {\n        'filename': 'file1.txt',\n    },\n    dag = dag,\n)\nAt runtime, Airflow will execute the BashOperator by reading the templated command and replacing params dot filename with the value stored in the params dictionary for the filename key. In other words, it would pass the BashOperator echo Reading file1 dot txt. The actual log output would be Reading file1 dot txt (after the BashOperator executed the command)."
  },
  {
    "objectID": "tutorials/DC_airflow.html#templated-bashoperator-example-continued",
    "href": "tutorials/DC_airflow.html#templated-bashoperator-example-continued",
    "title": "Datacamp Airflow Course",
    "section": "Templated BashOperator example (continued)",
    "text": "Templated BashOperator example (continued)\nNow, let’s consider one way to use templates for our earlier task of outputting Reading file1 dot txt and Reading file2 dot txt.\ntemplated_command=\"\"\"  \n    echo \"Reading {{ params.filename }}\"\n\"\"\"\nt1 = BashOperator(\n    task_id='template_task',\n    bash_command=templated_command,\n    params={'filename': 'file1.txt'}\n    dag=example_dag\n)\n\nt2 = BashOperator(\n    task_id='template_task',\n    bash_command=templated_command,\n    params={'filename': 'file2.txt'}\n    dag=example_dag\n)\nFirst, we create our templated command as before. Next, create the first task and pass the params dict with a filename key and the value file1 dot txt. To pass another entry, we can create a second task and modify the params dict accordingly. This time the filename would contain file2 dot txt and Airflow would substitute that value instead. The resulting output would be as expected.\n\n\n\n\n\n\nNote, you may be wondering what templates do for you here. You’ll see more in the coming exercises and lessons."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-11",
    "href": "tutorials/DC_airflow.html#exercises-11",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Creating a templated BashOperator\nYou’ve successfully created a BashOperator that cleans a given data file by executing a script called cleandata.sh. This works, but unfortunately requires the script to be run only for the current day. Some of your data sources are occasionally behind by a couple of days and need to be run manually.\nYou successfully modify the cleandata.sh script to take one argument - the date in YYYYMMDD format. Your testing works at the command-line, but you now need to implement this into your Airflow DAG. For now, use the term { ds_nodash } in your template - you’ll see exactly what this is means later on.\n\nCreate a templated command to execute the cleandata.sh script with the current execution date given by Airflow. Assign this command to a variable called templated_command.\nModify the BashOperator to use the templated command.\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime\n\ndefault_args = {\n  'start_date': datetime(2020, 4, 15),\n}\n\ncleandata_dag = DAG(\n    'cleandata',\n    default_args = default_args,\n    schedule_interval = '@daily'\n)\n\n# Create a templated command to execute\ntemplated_command = \"\"\"\n    bash {{ params.filename }} {{ ds_nodash }}\n\"\"\"\n\n# Modify clean_task to use the templated command\nclean_task = BashOperator(\n    task_id = 'cleandata_task',\n    bash_command = templated_command,\n    params = {\n        'filename': 'cleandata.sh', \n    },\n    dag = cleandata_dag,\n)\n\n\nExercise 2: Templates with multiple arguments\nYou wish to build upon your previous DAG and modify the code to support two arguments - the date in YYYYMMDD format, and a file name passed to the cleandata.sh script.\n\nModify the templated command to handle a second argument called filename.\nChange the first BashOperator to pass the filename salesdata.txt to the command.\nAdd a new BashOperator called clean_task2 to use a second filename supportdata.txt.\nSet clean_task2 downstream of clean_task.\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime\n\ndefault_args  =  {\n  'start_date': datetime(2020, 4, 15),\n}\n\ncleandata_dag = DAG(\n    'cleandata',\n    default_args = default_args,\n    schedule_interval = '@daily')\n\n# Modify the templated command to handle a\n# second argument called filename.\ntemplated_command = \"\"\"\n  bash cleandata.sh {{ ds_nodash }} {{ params.filename }}\n\"\"\"\n\n# Modify clean_task to pass the new argument\nclean_task = BashOperator(\n    task_id = 'cleandata_task',                      \n    bash_command = templated_command,                         \n    params = {\n        'filename': 'salesdata.txt',\n    },                      \n    dag = cleandata_dag\n)\n\n# Create a new BashOperator clean_task2\nclean_task2 = BashOperator(\n    task_id = 'cleandata_task2',\n    bash_command = templated_command,\n    params = {\n        'filename': 'supportdata.txt',\n    },\n    dag = cleandata_dag,\n)\n                           \n# Set the operator dependencies\nclean_task &gt;&gt; clean_task2"
  },
  {
    "objectID": "tutorials/DC_airflow.html#more-advanced-template",
    "href": "tutorials/DC_airflow.html#more-advanced-template",
    "title": "Datacamp Airflow Course",
    "section": "More advanced template",
    "text": "More advanced template\nJinja templates can be considerably more powerful than we’ve used so far. It is possible to use a for construct to allow us to iterate over a list and output content accordingly. Let’s change our templated command to the following. We start with an open curly brace and the percent symbol, then use a normal python command of for filename in params dot filenames then percent close brace. Then we modify our output line to be echo Reading open curly braces filename close curly braces. We then use a Jinja entry to represent the end of the for loop, open curly brace percent endfor percent close curly brace.\ntemplated_command=\"\"\"\n{% for filename in params.filenames %}  \n    echo \"Reading {{ filename }}\"\n{% endfor %}\n\"\"\"\nt1 = BashOperator(\n    task_id = 'template_task',\n    bash_command = templated_command,\n    params = {'filenames': ['file1.txt', 'file2.txt']}\n    dag = example_dag,\n)\n\n\n\n\n\n\nNote that this is required to define the end of the loop, as opposed to Python’s typical whitespace indention. Now let’s look at our BashOperator task. It looks similar except we’ve modified the params key to be filenames, and the value is now a list with two strings, file1 dot txt and file2 dot txt. When Airflow executes the BashOperator, it will iterate over the entries in the filenames list and substitute them in accordingly. Our output is the same as before, with a single task instead of two. Consider too the difference in code if you had 100 files in the list?"
  },
  {
    "objectID": "tutorials/DC_airflow.html#variables",
    "href": "tutorials/DC_airflow.html#variables",
    "title": "Datacamp Airflow Course",
    "section": "Variables",
    "text": "Variables\nAs part of the templating system, Airflow provides a set of built-in runtime variables. These provide assorted information about DAG runs, individual tasks, and even the system configuration.\n\n\n\n\n\n\n\n Description\nWhat does\n\n\n\n\nExe. date: {{ ds }}\nYYYY-MM-DD\n\n\nExe. date, no dashes: {{ ds_nodash }}\n YYYYMMDD\n\n\nPrevious exe. date: {{ prev_ds }}\nYYYY-MM-DD\n\n\nPrevious exe. date, no dashes: {{ prev_ds_nodash }}\nYYYYMMDD\n\n\nDAG object: {{ dag }}\nDAG(‘example_dag’)\n\n\nAirflow configuration: {{ conf }}\n{‘core’: {‘task_log_reader’: ’file.task_runner…}}\n\n\n\nTemplate examples include the execution date, which is ds in the double curly brace pairs. It returns the date in a 4 digit year dash 2 digit month dash 2 digit day format. There’s also a ds underscore nodash variety to get the same info without dashes.\n\n\n\n\n\n\nNote that this is a string, not a python datetime object.\n\n\n\nAnother variable available is the prev underscore ds, which gives the date of the previous DAG run in the same format as ds. The nodash variety is here as well. You can also access the full DAG object using the dag variable. Or you can use the conf object to access the current Airflow configuration within code. There are many more variables available - you can refer to the documentation for more information4.\n\n\n\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\n\n{{ data_interval_start }}\npendulum.DateTime\nStart of the data interval. Added in version 2.2.\n\n\n{{ data_interval_end }}\npendulum.DateTime\nEnd of the data interval. Added in version 2.2.\n\n\n{{ ds }}\nstr\nThe DAG run’s logical date as YYYY-MM-DD. Same as {{ dag_run.logical_date\n\n\n{{ ds_nodash }}\nstr\nSame as {{ dag_run.logical_date\n\n\n{{ ts }}\nstr\nSame as {{ dag_run.logical_date\n\n\n{{ ts_nodash_with_tz }}\nstr\nSame as {{ dag_run.logical_date\n\n\n{{ ts_nodash }}\nstr\nSame as {{ dag_run.logical_date\n\n\n{{ prev_data_interval_start_success }}\npendulum.DateTime\nNone\n\n\n{{ prev_data_interval_end_success }}\npendulum.DateTime\nNone\n\n\n{{ prev_start_date_success }}\npendulum.DateTime\nNone\n\n\n{{ dag }}\nDAG\nThe currently running DAG. You can read more about DAGs in DAGs.\n\n\n{{ task }}\nBaseOperator\nThe currently running BaseOperator. You can read more about Tasks in Operators\n\n\n{{ macros }}\nnan\nA reference to the macros package. See Macros below.\n\n\n{{ task_instance }}\nTaskInstance\nThe currently running TaskInstance.\n\n\n{{ ti }}\nTaskInstance\nSame as {{ task_instance }}.\n\n\n{{ params }}\ndict[str, Any]\nThe user-defined params. This can be overridden by the mapping passed to trigger_dag -c if dag_run_conf_overrides_params is enabled in airflow.cfg.\n\n\n{{ var.value }}\nnan\nAirflow variables. See Airflow Variables in Templates below.\n\n\n{{ var.json }}\nnan\nAirflow variables. See Airflow Variables in Templates below.\n\n\n{{ conn }}\nnan\nAirflow connections. See Airflow Connections in Templates below.\n\n\n{{ task_instance_key_str }}\nstr\nA unique, human-readable key to the task instance. The format is {dag_id}{task_id}{ds_nodash}.\n\n\n{{ conf }}\nAirflowConfigParser\nThe full configuration object representing the content of your airflow.cfg. See airflow.configuration.conf.\n\n\n{{ run_id }}\nstr\nThe currently running DagRun run ID.\n\n\n{{ dag_run }}\nDagRun\nThe currently running DagRun.\n\n\n{{ test_mode }}\nbool\nWhether the task instance was run by the airflow test CLI.\n\n\n{{ expanded_ti_count }}\nint\nNone"
  },
  {
    "objectID": "tutorials/DC_airflow.html#macros",
    "href": "tutorials/DC_airflow.html#macros",
    "title": "Datacamp Airflow Course",
    "section": "Macros",
    "text": "Macros\nIn addition to the other Airflow variables, there is also a macros variable. The macros package provides a reference to various useful objects or methods for Airflow templates.\n\n\n\nMacros object\nPython object\n\n\n\n\n{{ macros.datetime}}\n datetime.datetime\n\n\n{{ macros.timedelta}}\ndatetime.timedelta\n\n\n{{ macros.uuid}}\nuuid.UUID\n\n\n{{ macros.ds_add(ds, days)}}\n datetime\n\n\n\nSome examples of these include the macros dot datetime, which is the Python datetime dot datetime object. The macros dot timedelta template references the timedelta object. A macros dot uuid is the same as Python’s uuid object. Finally, there are also some added functions available, such as macros dot ds underscore add. It provides an easy way to perform date math within a template. It takes two arguments, a YYYYMMDD datestring and an integer representing the number of days to add (or subtract if the number is negative). Our example here would return April 20, 2020. These are not all the available macros objects - refer to the Airflow documentation for more info."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-12",
    "href": "tutorials/DC_airflow.html#exercises-12",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Using lists with templates\nOnce again, you decide to make some modifications to the design of your cleandata workflow. This time, you realize that you need to run the command cleandata.sh with the date argument and the file argument as before, except now you have a list of 30 files. You do not want to create 30 tasks, so your job is to modify the code to support running the argument for 30 or more files.\nThe Python list of files is already created for you, simply called filelist.\n\nModify the templated command to iterate over a list of filenames.\nPass the filelist to the templated command in the operator.\n\nfrom airflow.models import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom datetime import datetime\n\nfilelist = [f'file{x}.txt' for x in range(30)]\n\ndefault_args = {\n  'start_date': datetime(2020, 4, 15),\n}\n\ncleandata_dag = DAG('cleandata',\n                    default_args=default_args,\n                    schedule_interval='@daily')\n\n# Modify the template to handle multiple files in a \n# single run.\ntemplated_command = \"\"\"\n  &lt;% for filename in params.filenames %&gt;\n  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n  &lt;% endfor %&gt;\n\"\"\"\n\n# Modify clean_task to use the templated command\nclean_task = BashOperator(task_id='cleandata_task',\n                          bash_command=templated_command,\n                          params={'filenames': filelist},\n                          dag=cleandata_dag)\n\n\nExercise 2: Understanding parameter options\nYou’ve used a few different methods to add templates to your workflows. Considering the differences between options, why would you want to create individual tasks (ie, BashOperators) with specific parameters vs a list of files?\nFor example, why would you choose\nt1 = BashOperator(task_id='task1', bash_command=templated_command, params={'filename': 'file1.txt'}, dag=dag)\nt2 = BashOperator(task_id='task2', bash_command=templated_command, params={'filename': 'file2.txt'}, dag=dag)\nt3 = BashOperator(task_id='task3', bash_command=templated_command, params={'filename': 'file3.txt'}, dag=dag)\nover using a loop form such as\nt1 = BashOperator(\n    task_id = 'task1',                   \n    bash_command = templated_command,                   \n    params = {'filenames': ['file1.txt', 'file2.txt', 'file3.txt']},                  \n    dag = dag\n)\nPossible Answers\n\nUsing a loop form is slower.\nUsing specific tasks allows better monitoring of task state and possible parallel execution.\nThe params object can only handle lists of a few items.\n\n\n\nExercise 3: Sending templated emails\nWhile reading through the Airflow documentation, you realize that various operations can use templated fields to provide added flexibility. You come across the docs for the EmailOperator and see that the content can be set to a template. You want to make use of this functionality to provide more detailed information regarding the output of a DAG run.\n\nCreate a Python string that represents the email content you wish to send. Use the substitutions for the current date string (with dashes) and a variable called username.\nCreate the EmailOperator task using the template string for the html_content.\nSet the subject field to a macro call using macros.uuid.uuid4(). This simply provides a string of a universally unique identifier as the subject field.\nAssign the params dictionary as appropriate with the username of testemailuser.\n\nfrom airflow.models import DAG\nfrom airflow.operators.email_operator import EmailOperator\nfrom datetime import datetime\n\n# Create the string representing the html email content\nhtml_email_str = \"\"\"\nDate: {{ ds }}\nUsername: {{ params.username }}\n\"\"\"\n\nemail_dag = DAG(\n    'template_email_test',                \n    default_args = {\n        'start_date': datetime(2020, 4, 15),\n    },\n    schedule_interval = '@weekly',\n)\n\nemail_task = EmailOperator(\n    task_id = 'email_task',\n    to = 'testuser@datacamp.com',\n    subject = \"{{ macros.uuid.uuid4() }}\",\n    html_content = html_email_str,\n    params = {\n        'username': 'testemailuser',\n    },\n    dag = email_dag\n)"
  },
  {
    "objectID": "tutorials/DC_airflow.html#branching-example",
    "href": "tutorials/DC_airflow.html#branching-example",
    "title": "Datacamp Airflow Course",
    "section": "Branching example",
    "text": "Branching example\nFor our branching example, let’s assume we’ve already defined our DAG and imported all the necessary libraries. Our first task is to create the function used with the python_callable for the BranchPythonOperator.\ndef branch_test(**kwargs):\n    if int(kwargs['ds_nodash']) % 2 == 0:\n        return 'even_day_task'\n    else:\n        return 'odd_day_task'\nYou’ll note that the asterisk asterisk kwargs argument is the only component passed in. Remember that this is a reference to a keyword dictionary passed into the function. In the function we first access the ds underscore nodash key from the kwargs dictionary. If you remember from our previous lesson, this is the template variable used to return the date in YYYYMMDD format. We take this value, convert it to an integer, and then run a check if modulus 2 equals 0. Basically, we’re checking if a number is fully divisible by 2. If so, it’s even, otherwise, it’s odd. As such, we return either even underscore day underscore task, or odd underscore day underscore task."
  },
  {
    "objectID": "tutorials/DC_airflow.html#branching-example-1",
    "href": "tutorials/DC_airflow.html#branching-example-1",
    "title": "Datacamp Airflow Course",
    "section": "Branching example",
    "text": "Branching example\nThe next part of our code creates the BranchPythonOperator. This is like the normal PythonOperator, except we pass in the provide underscore context argument and set it to True. This is the component that tells Airflow to provide access to the runtime variables and macros to the function. This is what gets referenced via the kwargs dictionary object in the function definition.\nbranch_task = BranchPythonOperator(\n    task_id = 'branch_task',\n    python_callable = branch_test,\n    provide_context = True,\n    dag = dag,\n)\nNow we don’t show the code here, but let’s assume we’ve created two tasks for even days, and two tasks for odd numbered days.\ntask_even = PythonOperator(...)\ntask_uneven = PythonOperator(...)\nFinally, we need to set the dependencies. We want the branch task to run first, then the even day task or the odd day task depending on the result of the branch task. We need to set the dependencies using the bitshift operators.\nstart_task &gt;&gt; branch_task &gt;&gt; even_day_task &gt;&gt; even_day_task2branch_task &gt;&gt; odd_day_task &gt;&gt; odd_day_task2\nbranch_task &gt;&gt; odd_day_task &gt;&gt; odd_day_task2\nFirst, we configure the dependency order for start task, branch task, then even day task and even day task2. Now we need to set the dependency order for the odd day tasks. As we’ve already defined the dependency for the start and branch tasks, we can set odd day task to follow the branch task, and the odd day task2 to follow that. You may be wondering why you’d set these dependencies if one set is not going to run. If you didn’t set these dependencies, all the tasks would run as normal, regardless of what the branch operator returned."
  },
  {
    "objectID": "tutorials/DC_airflow.html#branching-graph-view",
    "href": "tutorials/DC_airflow.html#branching-graph-view",
    "title": "Datacamp Airflow Course",
    "section": "Branching graph view",
    "text": "Branching graph view\nLet’s look at the DAG in the graph view of the Airflow UI.\n\nYou’ll notice that we have a start task upstream of the branch task. The branch task then shows two paths, one to the odd day tasks, and the other to the even day tasks."
  },
  {
    "objectID": "tutorials/DC_airflow.html#branching-even-days",
    "href": "tutorials/DC_airflow.html#branching-even-days",
    "title": "Datacamp Airflow Course",
    "section": "Branching even days",
    "text": "Branching even days\nLet’s look first at what happens if we run on an even numbered day.\n\nThe start task executes as normal, then the branch task checks the ds_nodash value and determines this is an even day. It returns the value even underscore day underscore task, which is then executed by Airflow followed by the even day task2.\n\n\n\n\n\n\nNote that the odd day tasks are marked in light pink, which refers to them being skipped."
  },
  {
    "objectID": "tutorials/DC_airflow.html#branching-odd-days",
    "href": "tutorials/DC_airflow.html#branching-odd-days",
    "title": "Datacamp Airflow Course",
    "section": "Branching odd days",
    "text": "Branching odd days\nFor completeness, let’s look at the output from a run on an odd day.\n\nThe process is the same, except that the branch task selects odd day task instead and the even branch is marked skipped."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-13",
    "href": "tutorials/DC_airflow.html#exercises-13",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Define a BranchPythonOperator\nAfter learning about the power of conditional logic within Airflow, you wish to test out the BranchPythonOperator. You’d like to run a different code path if the current execution date represents a new year (ie, 2020 vs 2019).\nThe DAG is defined for you, along with the tasks in question. Your current task is to implement the BranchPythonOperator.\n\nIn the function year_check, configure the code to determine if the year of the current execution date is different than the previous execution date (ie, is the year different between the appropriate Airflow template variables.)\nFinish the BranchPythonOperator by adding the appropriate arguments.\nSet the dependencies on current_year_task and new_year_task.\n\n# Create a function to determine if years are different\ndef year_check(**kwargs):\n    current_year = int(kwargs['ds_nodash'][0:4])\n    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n    if current_year == previous_year:\n        return 'current_year_task'\n    else:\n        return 'new_year_task'\n\n# Define the BranchPythonOperator\nbranch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n                                   python_callable=year_check, provide_context=True)\n# Define the dependencies\nbranch_dag &gt;&gt; current_year_task\nbranch_dag &gt;&gt; new_year_task\n\n\nExercise 2: Branch troubleshooting\nWhile working with a workflow defined by a colleague, you notice that a branching operator executes, but there’s never any change in the DAG results. You realize that regardless of the state defined by the branching operator, all other tasks complete, even as some should be skipped.\nUse what you’ve learned to determine the most likely reason that the branching operator is ineffective.\n\nThe branch_test method does not return the correct value.\nThe DAG does not run often enough for the callable to work properly.\nThe dependency is missing between the branch_task and even_day_task and odd_day_task."
  },
  {
    "objectID": "tutorials/DC_airflow.html#running-dags-tasks",
    "href": "tutorials/DC_airflow.html#running-dags-tasks",
    "title": "Datacamp Airflow Course",
    "section": "Running DAGs & Tasks",
    "text": "Running DAGs & Tasks\nYou may remember way back in chapter 1, we discussed how to run a task. If not, here’s a quick reminder - use airflow run dag id task id and execution date from the command line.\nairflow run &lt;dag_id&gt; &lt;task_id&gt; &lt;execution_date&gt;\nThis will execute a specific DAG task as though it were running on the date specified.\nTo run a full DAG, you can use the airflow trigger underscore dag dash e then the execution date and dag_id.\nairflow trigger_dag -e &lt;execution_date&gt; &lt;dag_id&gt;\nThis executes the full DAG as though it were running on the specified date."
  },
  {
    "objectID": "tutorials/DC_airflow.html#operators-reminder",
    "href": "tutorials/DC_airflow.html#operators-reminder",
    "title": "Datacamp Airflow Course",
    "section": "Operators reminder",
    "text": "Operators reminder\nWe’ve been working with operators and sensors through most of this course, but let’s take a quick look at some of the most common ones we’ve used.\n\nThe BashOperator behaves like most operators, but expects a bash_command parameter which is a string of the command to be run.\nThe PythonOperator requires a python_callable argument with the name of the Python function to execute.\n\nThe BranchPythonOperator is similar to the PythonOperator, but the python_callable must be a function that accepts a **kwargs entry. As such, the provide_context = True attribute must be set to true.\n\nThe FileSensor requires a filepath argument of a string, and might need mode or poke_interval attributes.\n\nYou can refer to previous chapters for further detail if required."
  },
  {
    "objectID": "tutorials/DC_airflow.html#template-reminders",
    "href": "tutorials/DC_airflow.html#template-reminders",
    "title": "Datacamp Airflow Course",
    "section": "Template reminders",
    "text": "Template reminders\nA quick reminder is that many objects in Airflow can use templates. Only certain fields can accept templated strings while others do not. It can be tricky to remember which ones support templates on what fields. One way to check is to use the built-in python documentation via a live python interpreter. To use this method, open a python3 interpreter at the command line.\npython\nImport any necessary libraries (ie, the BashOperator) At the prompt, run help with the name of the Airflow object as the lone argument.\nfrom airflow.operators.bash import BashOperator\n\nhelp(BashOperator)\nLook for a line referencing template underscore fields. This line will specify if and which fields can use templated strings."
  },
  {
    "objectID": "tutorials/DC_airflow.html#template-documentation-example",
    "href": "tutorials/DC_airflow.html#template-documentation-example",
    "title": "Datacamp Airflow Course",
    "section": "Template documentation example",
    "text": "Template documentation example\nThis is an example of checking for help in the python interpreter. Notice the output with the template fields entry - in this case, the bash underscore command and the env fields can accept templated values.\n\n\n\n\n\n\n\nA final note before working through our last exercises - as a data engineer, your job is not to necessarily understand every component of a workflow. You may not fully understand all of a machine learning process, or perhaps how an Apache Spark job works. Your task is to implement any of those tasks in a repeatable and reliable fashion. Let’s practice implementing workflows for the last time in this course now."
  },
  {
    "objectID": "tutorials/DC_airflow.html#exercises-14",
    "href": "tutorials/DC_airflow.html#exercises-14",
    "title": "Datacamp Airflow Course",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1: Creating a production pipeline #1\nYou’ve learned a lot about how Airflow works - now it’s time to implement your workflow into a production pipeline consisting of many objects including sensors and operators. Your boss is interested in seeing this workflow become automated and able to provide SLA reporting as it provides some extra leverage for closing a deal the sales staff is working on. The sales prospect has indicated that once they see updates in an automated fashion, they’re willing to sign-up for the indicated data service.\nFrom what you’ve learned about the process, you know that there is sales data that will be uploaded to the system. Once the data is uploaded, a new file should be created to kick off the full processing, but something isn’t working correctly.\nRefer to the source code of the DAG to determine if anything extra needs to be added.\nfrom airflow.models import DAG\nfrom airflow.contrib.sensors.file_sensor import FileSensor\n\n# Import the needed operators\nfrom airflow.operators.____ import ____\n____\nfrom datetime import date, datetime\n\ndef process_data(**context):\n  file = open('/home/repl/workspace/processed_data.tmp', 'w')\n  file.write(f'Data processed on {date.today()}')\n  file.close()\n\n    \ndag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2020,4,1)})\n\nsensor = FileSensor(task_id='sense_file', \n                    filepath='/home/repl/workspace/startprocess.txt',\n                    poke_interval=5,\n                    timeout=15,\n                    dag=dag)\n\nbash_task = BashOperator(task_id='cleanup_tempfiles', \n                         bash_command='rm -f /home/repl/*.tmp',\n                         dag=dag)\n\npython_task = PythonOperator(task_id='run_processing', \n                             python_callable=process_data,\n                             dag=dag)\n\nsensor &gt;&gt; bash_task &gt;&gt; python_task\n\nUpdate the DAG in pipeline.py to import the needed operators.\n\nSolution\nfrom airflow.models import DAG\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom dags.process import process_data\nfrom datetime import timedelta, datetime\n\n# Update the default arguments and apply them to the DAG\ndefault_args = {\n  'start_date': datetime(2019,1,1),\n  'sla': timedelta(minutes=90),\n}\n\ndag = DAG(\n    dag_id = 'etl_update', \n    default_args = default_args, \n)\n\nsensor = FileSensor(\n    task_id = 'sense_file', \n    filepath = '/home/repl/workspace/startprocess.txt',\n    poke_interval = 45,\n    dag = dag\n)\n\nbash_task = BashOperator(\n    task_id = 'cleanup_tempfiles', \n    bash_command = 'rm -f /home/repl/*.tmp',\n    dag = dag\n)\n\npython_task = PythonOperator(\n    task_id = 'run_processing', \n    python_callable = process_data,\n    provide_context = True,\n    dag = dag,\n)\n\nsensor &gt;&gt; bash_task &gt;&gt; python_task\n\nRun the sense_file task from the command line and look for any errors. Use the command airflow test and the appropriate arguments to run the command. For the last argument, use a -1 instead of a specific date.\n\n\n\n\nThe file is not found and timeout is triggered after 15 seconds. we have first to create the file\n\n\n\nDetermine why the sense_file task does not complete and remedy this using the editor.\nRe-test the sense_file task and verify the problem is fixed.\n\n\n\nExercise 2: Creating a production pipeline #2\nContinuing on your last workflow, you’d like to add some additional functionality, specifically adding some SLAs to the code and modifying the sensor components.\nRefer to the source code of the DAG to determine if anything extra needs to be added. The default_args dictionary has been defined for you, though it may require further modification.\nfrom airflow.models import DAG\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom dags.process import process_data\nfrom datetime import timedelta, datetime\n\n# Update the default arguments and apply them to the DAG\ndefault_args = {\n  'start_date': datetime(2019,1,1),\n  ____\n}\n\ndag = DAG(dag_id='etl_update', default_args=default_args)\n\nsensor = FileSensor(task_id='sense_file', \n                    filepath='/home/repl/workspace/startprocess.txt',\n                    ____,\n                    dag=dag)\n\nbash_task = BashOperator(task_id='cleanup_tempfiles', \n                         bash_command='rm -f /home/repl/*.tmp',\n                         dag=dag)\n\npython_task = PythonOperator(task_id='run_processing', \n                             python_callable=process_data,\n                             ____\n                             dag=dag)\n\nsensor &gt;&gt; bash_task &gt;&gt; python_task\n\nAdd an SLA of 90 minutes to the DAG.\nUpdate the FileSensor object to check for files every 45 seconds.\nModify the python_task to send Airflow variables to the callable. Note that the callable is configured to accept the variables using the provide_context argument.\n\nfrom airflow.models import DAG\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom dags.process import process_data\nfrom datetime import timedelta, datetime\n\n# Update the default arguments and apply them to the DAG\ndefault_args = {\n  'start_date': datetime(2019,1,1),\n  'sla': timedelta(minutes=90),\n}\n\ndag = DAG(\n    dag_id='etl_update', \n    default_args=default_args, \n)\n\nsensor = FileSensor(\n    task_id='sense_file', \n    filepath='/home/repl/workspace/startprocess.txt',\n    poke_interval=45,\n    dag=dag\n)\n\nbash_task = BashOperator(\n    task_id='cleanup_tempfiles', \n    bash_command='rm -f /home/repl/*.tmp',\n    dag=dag\n)\n\npython_task = PythonOperator(\n    task_id='run_processing', \n    python_callable=process_data,\n    provide_context = True,\n    dag=dag\n)\n\nsensor &gt;&gt; bash_task &gt;&gt; python_task\n\n\nExercise 3: Adding the final changes to your pipeline\nTo finish up your workflow, your manager asks that you add a conditional logic check to send a sales report via email, only if the day is a weekday. Otherwise, no email should be sent. In addition, the email task should be templated to include the date and a project name in the content.\nThe branch callable is already defined for you.\n\nImport the necessary operators.\nConfigure the EmailOperator to provide the specific data to the callable.\nComplete the branch callable as necessary to point to the email_report_task or no_email_task.\nConfigure the branch operator to properly check for the condition.\n\nfrom airflow.models import DAG\nfrom airflow.contrib.sensors.file_sensor import FileSensor\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.python_operator import BranchPythonOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators.email_operator import EmailOperator\nfrom dags.process import process_data\nfrom datetime import datetime, timedelta\n\n# Update the default arguments and apply them to the DAG.\n\ndefault_args = {\n  'start_date': datetime(2019,1,1),\n  'sla': timedelta(minutes=90)\n}\n    \ndag = DAG(dag_id='etl_update', default_args=default_args)\n\nsensor = FileSensor(\n  task_id='sense_file', \n  filepath='/home/repl/workspace/startprocess.txt',\n  poke_interval=45,\n  dag=dag\n)\n\nbash_task = BashOperator(\n  task_id='cleanup_tempfiles', \n  bash_command='rm -f /home/repl/*.tmp',\n  dag=dag\n)\n\npython_task = PythonOperator(\n  task_id='run_processing', \n  python_callable=process_data,\n  provide_context=True,\n  dag=dag\n)\n\n\nemail_subject=\"\"\"\n  Email report for {{ params.department }} on {{ ds_nodash }}\n\"\"\"\n\n\nemail_report_task = EmailOperator(\n  task_id='email_report_task',\n  to='sales@mycompany.com',\n  subject=email_subject,\n  html_content='',\n  params={'department': 'Data subscription services'},\n  dag=dag\n)\n\n\nno_email_task = DummyOperator(task_id='no_email_task', dag=dag)\n\n\ndef check_weekend(**kwargs):\n    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n    if (dt.weekday() &lt; 5):\n        return 'email_report_task'\n    else:\n        return 'no_email_task'\n    \n    \nbranch_task = BranchPythonOperator(\n  task_id='check_if_weekend',\n  python_callable = check_weekend,\n  provide_context=True,\n  dag=dag\n)\n    \nsensor &gt;&gt; bash_task &gt;&gt; python_task\n\npython_task &gt;&gt; branch_task &gt;&gt; [email_report_task, no_email_task]"
  },
  {
    "objectID": "tutorials/DC_airflow.html#what-weve-learned",
    "href": "tutorials/DC_airflow.html#what-weve-learned",
    "title": "Datacamp Airflow Course",
    "section": "What we’ve learned",
    "text": "What we’ve learned\nLet’s review everything we’ve worked with during this course. We started with learning about workflows and DAGs in Airflow. We’ve learned what an operator is and how to use several of the available ones. We learned about tasks and how they are defined by various types of operators. In addition, we learned about dependencies between tasks and how to set them with bitshift operators. We’ve used sensors to react to workflow conditions and state. We’ve scheduled DAGs in various ways. We used SLAs and alerting to maintain visibility on our workflows. We learned about the power of templating in our workflows for maximum flexibility when defining tasks. We’ve learned how to use branching to add conditional logic to our DAGs. Finally, we’ve learned about the Airflow interfaces (command line and UI), about Airflow executors, and a bit about how to debug and troubleshoot various issues with Airflow and our own workflows."
  },
  {
    "objectID": "tutorials/DC_airflow.html#next-steps",
    "href": "tutorials/DC_airflow.html#next-steps",
    "title": "Datacamp Airflow Course",
    "section": "Next steps",
    "text": "Next steps\nA few suggestions for next steps include setting up your own environment for practice. You can follow the installation instructions in the Airflow documentation or use a cloud-based Airflow service. Look into other operators or sensors - there are operators available for Amazon’s S3, Postgresql operators, HDFS sensors, and so forth. Experiment with dependencies with a large number of tasks. Consider how you expect the workflow to progress and always try to leave as much up to the scheduler as possible to achieve the best performance. Given the length of the course, there is only so much we could cover and we left out some important parts of Airflow such as XCom, connections, and managing the UI further. Refer to the documentation for more ideas. Finally and most importantly, keep building workflows. When you’re uncertain how something works, try to build an example that covers what you’d like to accomplish. Look at the views within the Airflow UI to better understand how the system interprets your DAG code. The more you experiment, the better your understanding will grow."
  },
  {
    "objectID": "tutorials/DC_airflow.html#thank-you",
    "href": "tutorials/DC_airflow.html#thank-you",
    "title": "Datacamp Airflow Course",
    "section": "Thank you!",
    "text": "Thank you!\nFinally, thank you for taking this course and giving me the opportunity to introduce you to Airflow. Good luck on your future learning opportunities!"
  },
  {
    "objectID": "tutorials/DC_airflow.html#footnotes",
    "href": "tutorials/DC_airflow.html#footnotes",
    "title": "Datacamp Airflow Course",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.m.wikipedia.org/wiki/Directed_acyclic_graph↩︎\nhttps://airflow.apache.org/docs/stable/scheduler.html↩︎\nhttps://bradymholt.github.io/cron-expression-descriptor/↩︎\nhttps://airflow.apache.org/docs/stable/macros-ref.html↩︎"
  },
  {
    "objectID": "WorkInProgress/ExtractTextPDF2.html",
    "href": "WorkInProgress/ExtractTextPDF2.html",
    "title": "Bernardo Cruz",
    "section": "",
    "text": "Show the code\nimport PyPDF2\nfrom pprint import pp\n\n\n\n\nShow the code\ndef extract_text_page_by_page(pdf_path):\n    with open(pdf_path, 'rb') as file:\n        # Initialize PDF reader\n        pdf_reader = PyPDF2.PdfReader(file, strict=True)\n                \n        # Iterate over all the pages and extract text\n        for page_num in range(len(pdf_reader.pages)):\n            page = pdf_reader.pages[page_num]\n            yield page.extract_text()\n\npdf_path = '../data/CS25Amendment27.pdf'\n\n# Save the extracted text to text.txt\nout_file = '../data/CS25Amendment27.txt'\nwith open(out_file, 'w') as file_obj:\n    for page_text in extract_text_page_by_page(pdf_path):\n        file_obj.write(page_text)\n\nprint(f\"Text saved to {out_file}\")\n\n\nText saved to ../data/CS25Amendment27.txt\n\n\n\n\nShow the code\nwith open(out_file, 'r') as file_obj:\n    text = [line.strip() for line in file_obj.readlines()]\n\n\n\n\nShow the code\ntext[:200]\n\n\n['',\n 'Annex to ED Decision 2021/015/R   Page 1 of 1389',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n 'Certification Specifications and Acceptable',\n 'Means of Compliance for Large Aeroplanes',\n '(CS-25)',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n 'Amendment  27',\n '',\n '24 November  20211',\n '',\n '',\n '',\n '',\n '',\n '',\n '',\n '1 For the date of entry into force of Amendment  27, please refer to ED Decision 20 21/015/R in the  Official Publication  of EASA .  CS-25 Am endment  27',\n 'Table of c ontents',\n '',\n 'Annex to ED Decision 2021/015/R   Page 2 of 1389',\n 'TABLE OF CONTENTS',\n 'Table of contents  ................................ ................................ ...........  2',\n 'Preamb le  ................................ ................................ .....................  28',\n 'SUBPART A – GENERAL  ................................ ................................ . 51',\n 'CS 25.1 Applicability  ................................ ................................ ................................ .........  51',\n 'SUBPART B – FLIGHT  ................................ ................................ .... 52',\n 'GENERAL  ................................ ................................ ................................ ..........................  52',\n 'CS 25.20 Scope  ................................ ................................ ................................ .................  52',\n 'CS 25.21 Proof of compliance ................................ ................................ ...........................  52',\n 'AMC 25.21(d) Proof of compliance  ................................ ................................ ........  53',\n 'AMC 25.21(g) Performance and handling characteristics in icing conditions  .......  54',\n 'Appendix 1 – Airframe Ice Accretion  ................................ ................................ .....................  79',\n 'Appendix 2 – Artificial Ice Shapes  ................................ ................................ ..........................  89',\n 'Appendix 3 – Design Features  ................................ ................................ ................................  90',\n 'Appendix 4 – Examples of Aeroplane Flight Manual Limitations and Operating Procedures for',\n 'Operations in Supercooled Large Drop Icing Conditions  ................................ ........................  91',\n 'Appendix 5 – Related Acceptable Means of Compliance (AMC) and FAA Advisory Circulars (AC)',\n '................................ ................................ ................................ ................................ ...............  95',\n 'Appendix 6 – Acronyms and definitions ................................ ................................ .................  95',\n 'CS 25.23 Load distribution limits  ................................ ................................ ......................  96',\n 'CS 25.25 Weight Limits  ................................ ................................ ................................ ..... 96',\n 'CS 25.27 Centre of gravity limits  ................................ ................................ ......................  96',\n 'CS 25.29 Emp ty weight and corresponding centre of gravity  ................................ ..........  97',\n 'CS 25.31 Removable ballast  ................................ ................................ .............................  97',\n 'CS 25.33 Propeller speed and pitch limits  ................................ ................................ ........  97',\n 'PERFORMANCE  ................................ ................................ ................................ ................  97',\n 'CS 25.101 General  ................................ ................................ ................................ ............  97',\n 'AMC 25.101 General  ................................ ................................ ..............................  98',\n 'AMC No. 1 to CS 25.101(c) Extrapolation of Performance with Weight  ...............  98',\n 'AMC No. 2 to CS 25.101(c) General  ................................ ................................ .......  99',\n 'AMC 25.101(g) Go -around  ................................ ................................ ...................  101',\n 'AMC 25.101(h)(3) General  ................................ ................................ ...................  104',\n 'AMC 25.101(i) Performance determination with worn brakes  ...........................  105',\n 'CS 25.103 Stall speed ................................ ................................ ................................ ...... 105',\n 'AMC 25.103 (b) Stalling speed  ................................ ................................ ..............  106',\n 'AMC 25.103(c) Stall speed  ................................ ................................ ...................  106',\n 'AMC 25.103(d) Stall speed  ................................ ................................ ...................  106',\n 'CS 25.105 Take -off ................................ ................................ ................................ ..........  106  CS-25 Am endment  27',\n 'Table of contents',\n '',\n 'Annex to ED Decision 2021/015/R   Page 3 of 1389',\n 'CS 25.107 Take -off speeds  ................................ ................................ .............................  107',\n 'AMC 25.107(d) Take -off speeds  ................................ ................................ ...........  109',\n 'AMC 25.107(e)(1)(iv) Take -off speeds  ................................ ................................ . 109',\n 'AMC 25.107(e)(3) Take -off speeds  ................................ ................................ ...... 110',\n 'AMC No. 1 to CS 25.107(e)(4) Take -off speeds  ................................ ....................  110',\n 'AMC No. 2 to CS 25.107(e)(4) Take -off speeds  ................................ ....................  110',\n 'CS 25.109 Accelerate -stop distance  ................................ ................................ ...............  110',\n 'AMC 25.109(a) and (b) Accelerate -stop distance  ................................ ................  113',\n 'AMC 25.109(c)(2) Accelerate -stop distance: anti -skid sys tem efficiency  ............  113',\n 'AMC 25.109(d)(2) Accelerate -stop distance: anti -skid efficiency on grooved and',\n 'porous friction course (PFC) runways.  ................................ ................................ . 124',\n 'AMC 25.109(f) Accelerate -stop distance: credit for reverse thrust.  ....................  125',\n 'CS 25.111 Take -off path  ................................ ................................ ................................ . 126',\n 'AMC 25.111 Take -off path  ................................ ................................ ...................  128',\n 'AMC 25.111(b) Take -off path  ................................ ................................ ...............  128',\n 'CS 25.113 Take -off distance and take -off run  ................................ ................................  128',\n 'AMC 25.113(a)(2), (b)(2) and (c)(2) Take -off distance and take -off run  ..............  129',\n 'CS 25.115 Take -off flight path  ................................ ................................ ........................  129',\n 'CS 25.117 Climb: general  ................................ ................................ ................................  129',\n 'CS 25.119 Landing climb: all engines operating  ................................ .............................  129',\n 'AMC 25.119 Landing climb: all -engines -operating  ................................ ..............  130',\n 'CS 25.121 Climb: one -engine -inoperative  ................................ ................................ ...... 130',\n 'AMC 25.121 Climb: One -engine -inoperative  ................................ .......................  132',\n 'AMC 25.121(a) Climb: One -engine -inoperative  ................................ ...................  132',\n 'AMC 25.121(a)(1) Climb: One -engine -inoperative  ................................ ..............  133',\n 'AMC 25.121(b)(1)(i) Climb: One -engine -inoperative  ................................ ...........  133',\n 'CS 25.123 En -route flight paths  ................................ ................................ ......................  133',\n 'AMC 25.123 En -route flight paths ................................ ................................ ........  134',\n 'CS 25.125 Landing  ................................ ................................ ................................ ..........  134',\n 'AMC 25.125(b)(3) Change of Configuration ................................ .........................  135',\n 'AMC 25.125(c) Landing  ................................ ................................ ........................  135',\n 'AMC 25.125(c)(2) Landing  ................................ ................................ ....................  135',\n 'CONTROLLABILITY AND MANOEUVRABILITY  ................................ ................................ ... 135',\n 'CS 25.143 General  ................................ ................................ ................................ ..........  135',\n 'AMC 25.143(a) and (b) Controllability and Manoeuvrability  ...............................  139',\n 'AMC 25.143(b)(1) Control Following Engine Failure  ................................ ............  139',\n 'AMC 25.143(b)(4) Go -around Manoeuvres  ................................ .........................  139',\n 'AMC 25.143(d) Controllability and Manoeuvrability  ................................ ...........  143',\n 'AMC No. 1 to CS 25.143(g) Controllability and Manoeuvrability  ........................  143',\n 'AMC No. 2 to CS 25.143(g) Controllability and Manoeuvrability  ........................  143',\n 'AMC 25.143(h) Manoeuvre Capability  ................................ ................................ . 145',\n 'CS 25.145 Longitudinal control ................................ ................................ .......................  145',\n 'AMC 25.145(a) Longitudinal control - Control near the stall  ...............................  147',\n 'AMC 25.145(b)(2) Longitudinal control  ................................ ...............................  148',\n 'AMC 25.145(b)(1), (b)(2) and (b)(3) Longitudinal control  ................................ .... 148  CS-25 Am endment  27',\n 'Table of contents',\n '',\n 'Annex to ED Decision 2021/015/R   Page 4 of 1389',\n 'AMC 25.145(e) Longitudinal control  ................................ ................................ .... 148',\n 'AMC 25.145(f) Longitudinal control – go-around  ................................ ................  148',\n 'CS 25.147 Directional and lateral control  ................................ ................................ .......  149',\n 'AMC 25.147(a) Directional control; general  ................................ ........................  150',\n 'AMC 25.147(d) Lateral control: Roll capability  ................................ ....................  150',\n 'AMC 25.147(f) Lateral control: All engines operating  ................................ .........  151',\n 'CS 25.149 Minimum control speed  ................................ ................................ ................  151',\n 'AMC 25.149 Minimum control speeds  ................................ ................................  154',\n 'AMC 25.149(e) Minimum control speed ................................ ..............................  154',\n 'AMC 25.149(f) Minimum Control Speed during Approach and Landing (V MCL) ... 154',\n 'AMC 25.149(g) Minimum Control Speed with Two Inoperati ve Engines during',\n 'Approach and Landing (V MCL -2) ................................ ................................ .............  155',\n 'AMC 25.149(h)(3) Minimum control speeds  ................................ .......................  156',\n 'AMC 25.149(h)(4) Minimum control speeds  ................................ .......................  156',\n 'TRIM  ................................ ................................ ................................ ..............................  156',\n 'CS 25.161 Trim  ................................ ................................ ................................ ................  156',\n 'STABILITY  ................................ ................................ ................................ .......................  157',\n 'CS 25.171 General  ................................ ................................ ................................ ..........  157',\n 'CS 25.173 Static longitudinal stability  ................................ ................................ ............  157',\n 'AMC 25.173(c) Static longitudi nal stability  ................................ ..........................  157',\n 'CS 25.175 Demonstration of static longitudinal stability  ................................ ...............  157',\n 'CS 25.177 Static directional and lateral stability  ................................ ............................  159',\n 'AMC 25.177(c) Steady, Straight Sideslips  ................................ ............................  160',\n 'AMC 25.177(d) Full Rudder Sideslips  ................................ ................................ ... 161',\n 'CS 25.181 Dynamic stability  ................................ ................................ ...........................  163',\n 'AMC 25.181 Dynamic stability  ................................ ................................ .............  163',\n 'STAL LS ................................ ................................ ................................ ...........................  163',\n 'CS 25.201 Stall demonstration  ................................ ................................ .......................  163',\n 'AMC 25.201(a)(2) Stall demonstration  ................................ ................................  164',\n 'AMC 25.201(b)(1) Stall demonstration  ................................ ................................  164',\n 'AMC 25.201(c)(2) Turning Flight Stalls At Higher Deceleration Rates  .................  164',\n 'AMC 25.201(d) Stall demonstration  ................................ ................................ .... 164',\n 'AMC 25.201(d)(3) Stall dem onstration  ................................ ................................  165',\n 'CS 25.203 Stall characteristics  ................................ ................................ ........................  165',\n 'AMC 25.203 Stall characteristics  ................................ ................................ ..........  165',\n 'CS 25.207 Stall warning  ................................ ................................ ................................ .. 166',\n 'AMC 25.207(b) Stall warning  ................................ ................................ ...............  167',\n 'AMC 25.207(c) and (d) Stall warning  ................................ ................................ ... 168',\n 'GROUND HANDLING CHARACTERISTICS  ................................ ................................ ..........  169',\n 'CS 25.231 Longitudinal stability and control  ................................ ................................ .. 169',\n 'CS 25.233 Directional stability and control  ................................ ................................ .... 169',\n 'CS 25.235 Taxying condition  ................................ ................................ ...........................  169  CS-25 Am endment  27',\n 'Table of contents',\n '',\n 'Annex to ED Decision 2021/015/R   Page 5 of 1389',\n 'CS 25.237 Wind velocities  ................................ ................................ ..............................  169',\n 'MISCELLANEOUS FLIGHT REQUIREMENTS  ................................ ................................ .......  170',\n 'CS 25.251 Vibration and buffeting  ................................ ................................ .................  170',\n 'AMC 25.251(e) Vibration and Buffeting in Cruising Flight  ................................ ... 170',\n 'CS 25.253 High -speed characteristics ................................ ................................ .............  171',\n 'AMC 25.253(a)(4) Lateral Control: Roll Capability  ................................ ...............  172',\n 'AMC 25.253(a)(5) High Speed Characteristics  ................................ .....................  173',\n 'CS 25.255 Out -of-trim characteristics  ................................ ................................ ............  173',\n 'AMC 25.255 Out -of-trim characteristics  ................................ ..............................  174',\n 'SUBPART C – STRUCTURE  ................................ ...........................  177',\n 'GENERAL  ................................ ................................ ................................ ........................  177',\n 'CS 25.301 Loads  ................................ ................................ ................................ ..............  177',\n 'AMC No. 1 to 25.301(b) Loads  ................................ ................................ .............  177',\n 'AMC No. 2 to 25.301(b) Flight Load Validation  ................................ ...................  177',\n 'CS 25.302 Interaction of systems and structures  ................................ ...........................  181',\n 'CS 25.303 Factor of safety  ................................ ................................ ..............................  182',\n 'CS 25.305 Strength and deformation  ................................ ................................ .............  182',\n 'CS 25.307 Proof of structure  ................................ ................................ ..........................  182',\n 'AMC 25.307 Proof of structure  ................................ ................................ ............  183',\n 'FLIGHT LOADS  ................................ ................................ ................................ ................  187',\n 'CS 25.321 General  ................................ ................................ ................................ ..........  187',\n 'FLIGHT MANOEUVRE AND GUST CONDITIONS  ................................ ................................ . 187',\n 'CS 25.331 Symmetric manoeuvring conditions  ................................ ..............................  187',\n 'AMC 25.331(c)(1) Maximum pitch control displacement at V A ...........................  189',\n 'AMC 25.331(c)(2) Checked manoeuvre between V A and V D................................  190',\n 'CS 25.333 Flight manoeuvring envelope  ................................ ................................ ........  190']"
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nDatacamp Airflow Course\n\n\n\n\n\n\nApache Airflow\n\n\nDAG\n\n\nPython\n\n\nWorkflow\n\n\nETL\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\n90 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Driven by a passion for innovation and insatiable curiosity, I transitioned from aeronautical engineering to data science. With seven years of engineering experience, I’ve developed a knack for creative problem-solving and adapting to challenges. I thrive on discovering and leveraging new technologies, and I am proactive and adaptable in implementing them to drive business growth forward."
  },
  {
    "objectID": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "href": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "title": "Curriculum Vitae",
    "section": "Stadler Rail AG, Data Scientist (06/2023 – present)",
    "text": "Stadler Rail AG, Data Scientist (06/2023 – present)\n\nDeveloped Python packages for internal use\nImplemented a comprehensive automated CI/CD and ETL pipeline using on-premise infrastructure, private Docker registry and Airflow\nProcessed live data from the train telemetry system using Kafka, SQL and Kernel Density Estimation (KDE)\nTrained a team of mechanical engineers in best practices for software products and development in a Linux (Debian) environment\nDeployed Grafana instance for rapid development of dashboard as a replacement of Streamlit applications\n\nTechnology: Gitlab, Gitlab CI/CD, Airflow, Docker, Private Docker Registry, Sklearn, Postgres, Debian, Grafana\nLanguages: Python, SQL"
  },
  {
    "objectID": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "href": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "title": "Curriculum Vitae",
    "section": "Prognolite AG, Data Scientist (10/2022 – 03/2023)",
    "text": "Prognolite AG, Data Scientist (10/2022 – 03/2023)\n\nDeveloped regression models to predict turnover and menu sales using customer data and external data such as weather or public holidays\nMaintained and further developed the restaurant sales forecasting process\nDeveloped efficient optimization algorithms to improve forecast models based on various influencing factors\nMaintained and further developed the data preparation process of point of sales system data\n\nTechnology: Tidymodels, Sklearn, Random Forest, XGBoost, Cubist\nLanguages: R, Python, SQL"
  },
  {
    "objectID": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "href": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "title": "Curriculum Vitae",
    "section": "Crealogix AG, Data Scientist (09/2021 – 09/2022)",
    "text": "Crealogix AG, Data Scientist (09/2021 – 09/2022)\n\nAdvice and support of the department on new products related to Data Science and Machine Learning\nCreation of prospect and upsell models\nExtraction of information from fact-sheets using a customized OCR\nDevelopment of user journeys for chatbot dialogue flow\nDevelopment of chatbot solutions using IBM Watson and Spacy\n\nTechnology: Sklearn, Spacy, Random Forest, XGBoost, RMarkdown\nLanguages: Python, SQL, R"
  },
  {
    "objectID": "cv.html#bucher-leichtbau-ag-012014-092021",
    "href": "cv.html#bucher-leichtbau-ag-012014-092021",
    "title": "Curriculum Vitae",
    "section": "Bucher Leichtbau AG (01/2014 – 09/2021)",
    "text": "Bucher Leichtbau AG (01/2014 – 09/2021)\n\nCompliance Verification Engineer (10/2017 – 09/2021)\n\nAdvised and supported all departments of Bucher Leichtbau AG in initial airworthiness certification matters.\nIndependently verified documentation for “minor” and “major changes” against certification requirements.\n\nTechnology: FEMAP, Mechanical Engineering\n\n\nCertification Engineer (01/2014 – 10/2017)\n\nConducted FEM calculations/verification and prepared internal and external test reports.\nCommunicated with customers, official bodies, and authorities.\nConsulted development department for design optimization.\n\nTechnology: FEMAP, Mechanical Engineering"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nDevOps with Docker & Gitlab\n\n\n\n\n\n\nDocker\n\n\nContainerization\n\n\nDeployment\n\n\nGitlab\n\n\nDevOps\n\n\nCI/CD\n\n\nAutomation\n\n\nGithub Pages\n\n\n\n\n\n\n\n\n\nAug 1, 2024\n\n\n11 min\n\n\n\n\n\n\n\nFastAPI\n\n\n\n\n\n\nAPI Development\n\n\nFastAPI\n\n\nPython\n\n\nSQL\n\n\nCRUD Operations\n\n\nSQLAlchemy\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\n13 min\n\n\n\n\n\n\n\nPandas Method Chaining\n\n\n\n\n\n\nPandas\n\n\nReadability\n\n\nMethod Chaining\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\n4 min\n\n\n\n\n\n\n\nMy first post about statmodels - so be nice! :)\n\n\n\n\n\n\nstatmodels\n\n\nSeaborn\n\n\nPandas\n\n\nPython vs. R\n\n\nFormula API\n\n\nRDatasets in Python\n\n\nPatsy in Python\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I am Bernardo, an aeronautical engineer with a BSc in Aeronautical Engineering and a MSc in Data Science. My passion lies in both coding and engineering. I am from Zurich 🇨🇭 and have over seven years of experience working as an aeronautical engineer and switched to data science back in 2021.\nI would like to invite you go through my CV and blog to learn more about me 🤓. Please don’t hesitate to contact me if you have any questions or would like to collaborate on a project 🤝."
  },
  {
    "objectID": "index.html#hi-there",
    "href": "index.html#hi-there",
    "title": "Welcome!",
    "section": "",
    "text": "I am Bernardo, an aeronautical engineer with a BSc in Aeronautical Engineering and a MSc in Data Science. My passion lies in both coding and engineering. I am from Zurich 🇨🇭 and have over seven years of experience working as an aeronautical engineer and switched to data science back in 2021.\nI would like to invite you go through my CV and blog to learn more about me 🤓. Please don’t hesitate to contact me if you have any questions or would like to collaborate on a project 🤝."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Welcome!",
    "section": "👨🏽‍💻 Experience",
    "text": "👨🏽‍💻 Experience\nExplore my journey through various industries and roles in the world of data science and beyond! From predictive maintenance in the railway sector 🚂🚂🚂, to crafting models for turnover prediction in the food industry 🍔🍕🍝 and upsell models banking 💵💰💳, my experiences have been diverse and impactful. Delve into my work with Gaussian Mixture Models, Multi-Output Regression, OCR tools, and more. For a detailed account of my professional voyage, head to Section CV."
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Welcome!",
    "section": "🏝️ 🏙️ 💻 📺 Hobbies",
    "text": "🏝️ 🏙️ 💻 📺 Hobbies\nI love to travel 🛩️ 🚂 ⛰️ 🏝️ 🏙️ to code 💻 and binge watch 📺 in my free time. In particular I discovered a passion for web development and this blog 🤓😎."
  },
  {
    "objectID": "index.html#tech-stack",
    "href": "index.html#tech-stack",
    "title": "Welcome!",
    "section": "💻 Tech Stack:",
    "text": "💻 Tech Stack:"
  },
  {
    "objectID": "index.html#github-trophies",
    "href": "index.html#github-trophies",
    "title": "Welcome!",
    "section": "🏆 GitHub Trophies",
    "text": "🏆 GitHub Trophies"
  },
  {
    "objectID": "index.html#wisdom-of-the-day",
    "href": "index.html#wisdom-of-the-day",
    "title": "Welcome!",
    "section": "🧐 Wisdom of the Day",
    "text": "🧐 Wisdom of the Day"
  },
  {
    "objectID": "index.html#farewell-message",
    "href": "index.html#farewell-message",
    "title": "Welcome!",
    "section": "🙌🏽 Farewell message",
    "text": "🙌🏽 Farewell message\nsee you soon"
  },
  {
    "objectID": "WorkInProgress/Udemy_DockerCompose.html",
    "href": "WorkInProgress/Udemy_DockerCompose.html",
    "title": "Docker Compose Course",
    "section": "",
    "text": "This course notes corresponds to course Docker Compose in Depth on Udemy.\nDocker has taken the development world by storm in recent years, being the first effective tool that wraps up a piece of software in a complete file system package, installs it on a server, and runs it repeatedly. However, until recently it was difficult to do this with micro-architectures composed of numerous containers that all need to work in conjunction with one another. Enter Docker Compose, the handiest tool to hit the tech world since Docker. Here’s everything you need to know…\n\nDefine multi-container application environments\nCreate flexible, customisable environments and networks\nTransform an existing application into a fully Docker-ised environment\nEnhance your Docker experience\n\n\n\nFirst you’ll cover the basic features using a sample environment, gaining an understanding of restarts, dependencies, and persisting the database with a volume.\nAfter that you’ll progress to networks. You’ll take an in-depth look at isolating containers, aliases and container names, links, using external networks, and how updates affect networking. Then it’s on to the really good stuff; a section each is dedicated to volumes, logging, the Compose CLI, and ‘Composing Compose’ (don’t worry, it won’t be as complicated as it sounds by the time you get there). Finally you’ll learn about Compose in Production.\n\n\n\nDocker Compose is a useful tool from the people at Docker. It makes defining and running application environments made up of multiple Docker containers even easier and more efficient. Up until now, starting any more than one or two Docker containers was extremely complicated. With Docker Compose, the entire process just got infinitely better."
  },
  {
    "objectID": "WorkInProgress/Udemy_DockerCompose.html#make-your-docker-experience-even-more-stress-free",
    "href": "WorkInProgress/Udemy_DockerCompose.html#make-your-docker-experience-even-more-stress-free",
    "title": "Docker Compose Course",
    "section": "",
    "text": "First you’ll cover the basic features using a sample environment, gaining an understanding of restarts, dependencies, and persisting the database with a volume.\nAfter that you’ll progress to networks. You’ll take an in-depth look at isolating containers, aliases and container names, links, using external networks, and how updates affect networking. Then it’s on to the really good stuff; a section each is dedicated to volumes, logging, the Compose CLI, and ‘Composing Compose’ (don’t worry, it won’t be as complicated as it sounds by the time you get there). Finally you’ll learn about Compose in Production."
  },
  {
    "objectID": "WorkInProgress/Udemy_DockerCompose.html#about-docker-compose",
    "href": "WorkInProgress/Udemy_DockerCompose.html#about-docker-compose",
    "title": "Docker Compose Course",
    "section": "",
    "text": "Docker Compose is a useful tool from the people at Docker. It makes defining and running application environments made up of multiple Docker containers even easier and more efficient. Up until now, starting any more than one or two Docker containers was extremely complicated. With Docker Compose, the entire process just got infinitely better."
  },
  {
    "objectID": "WorkInProgress/Udemy_DockerCompose.html#basics",
    "href": "WorkInProgress/Udemy_DockerCompose.html#basics",
    "title": "Docker Compose Course",
    "section": "Basics",
    "text": "Basics\nNext, we are going to dive deeper into the basics of Docker Compose. We will start analyzing the docker compose wordpress example. This example contains the following information:\nversion: '2'\n\nservices:\n   db:\n     image: mysql:5.7                   #&lt;1.1&gt;\n     volumes:                           #&lt;1.2&gt;\n       - \"./.data/db:/var/lib/mysql\"\n     restart: always                    #&lt;1.3&gt;\n     environment:                       #&lt;1.4&gt;\n       MYSQL_ROOT_PASSWORD: wordpress\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wordpress\n       MYSQL_PASSWORD: wordpress\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:4.5                #&lt;2.1&gt;\n     links:                              #&lt;2.2&gt;\n       - db\n     ports:                              #&lt;2.3&gt;\n       - \"8000:80\"\n     restart: always\n     environment:                        #&lt;2.4&gt;\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_PASSWORD: wordpress\nwhere:\n\ndenotes the first service called db,\n\ndenotes the image used for the environment as a mysql database\n\ncreates a folder to persist the data in ./.data/db\nrestarts the container if it stops\ndefines the environment variables for the database environment\n\ndenotes the second service called wordpress, which defines the wordpress environment\n\ndenotes the image used for the wordpress environment\n\ndenotes the links to the other service called db\nexposes port 80 of the container to port 8000 of the host\ndefines the environment variables for the wordpress environment\n\n\nTaking a look at the setup provided by docker-compose file, the following schema is obtained:\n\n\n\n\nflowchart LR\n    subgraph Docker[Docker Container]\n        direction TB\n\n        subgraph Service1[Service: db]\n            DB[(\"DB\n            \n            Persists data\n            Restarts if it stops\n            Defines environment variables\n            Exposes port 3306\")] \n            \n        end\n        \n        subgraph Service2[Service: wordpress]\n            Wordpress[\"Wordpress\n            \n            Depends on db\n            Links to db\n            Defines environment variables\n            Exposes port 80\"]\n        end\n\n        Service1 &lt;--&gt; Service2\n        Service1 -- data is persisted in --&gt; Volume[Volume]\n    end\n\n    subgraph Host\n        direction LR\n        Browser[Browser Port 8000]\n    end\n\n    Docker -- Maps port 80 to 8000 --&gt; Host\n    \n\n\nSchematics of the Docker compose example showing\n\n\n\nSummarizing, we have review our first Docker compose file, which defines two services, a database and a wordpress environment. The database is linked to the wordpress environment and the wordpress environment is exposed to the host on port 8000. Furthermore, the database is persisted in the host in the folder ./.data/db. Next, we are going to dive deeper into the docker compose file and analyze how to persist data in a volume."
  },
  {
    "objectID": "WorkInProgress/Udemy_DockerCompose.html#persiting-database-with-a-volume",
    "href": "WorkInProgress/Udemy_DockerCompose.html#persiting-database-with-a-volume",
    "title": "Docker Compose Course",
    "section": "Persiting Database with a Volume",
    "text": "Persiting Database with a Volume\nversion: '2'\n\nservices:\n  wordpress:\n    image: wordpress:4.5\n    depends_on:\n      - db\n    links:\n      - db\n    ports:\n      - \"8080:80\"\n    environment:\n      WORDPRESS_DB_HOST: db:3306\n      WORDPRESS_DB_USER: wordpress\n      WORDPRESS_DB_PASSWORD: wordpress\n    restart: always\n\n  db:\n    image: mariadb:10.1\n    volumes:\n1      - \"./volumes/db:/var/lib/mysql\"\n2    restart: always\n    environment:\n      MYSQL_ROOT_PASSWORD: example\n      MYSQL_DATABASE: wordpress\n      MYSQL_USER: wordpress\n      MYSQL_PASSWORD: wordpress\n\n1\n\nCreates a folder to persist the data in ./volumes/db (host path) to /var/lib/mysql (container path).\n\n2\n\nRestarts the container if it stops - more on this in the next part.\n\n\nLet us now discuss restart policies as we’ve defined one of them for each of our containers (but we don’t quite know exactly what that means yet). In the case of two separate containers each with their own restart policies you’ll find that restarts occur independently:\nSo for example: if the database container went down in this environment our restart policy would cause it to restart its own but independently of Wordpress would then be up and things might be running again.\n\n\n\n\nflowchart LR\n    subgraph Docker[\"Case 1: without dependencies\"]\n        direction LR\n        wordpress[\"wordpress\"]\n        db[\"db\"]\n\n        wordpress &lt;-.-&gt; db\n        db -. restarts always .- db\n    end\n\n\nIndependent restarts\n\n\n\nMany applications that depend on a data layer like this may just stop working if their database disappears and/or is replaced. Therefore, we define a hierarchy between containers. We can do that by using the depends_on keyword. So in this case we’re going to say that the Wordpress container depends on the database container.\n\n\n\n\nflowchart LR\n    subgraph Docker[\"Case 2: with depends_on\"]\n        direction LR\n        wordpress[\"wordpress\"]\n        db[\"db\"]\n\n        wordpress -- depends on --&gt; db\n        db -. restarts always .- db\n        wordpress -. restart in case db restarts .-&gt; wordpress\n    end\n\n\nRestart using depends_on\n\n\n\nThe depends_on does not propagate the in the other direction. In case the wordpress container goes down, the database container will not be restarted."
  },
  {
    "objectID": "WorkInProgress/Udemy_DockerCompose.html#footnotes",
    "href": "WorkInProgress/Udemy_DockerCompose.html#footnotes",
    "title": "Docker Compose Course",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://gdevillele.github.io/compose/wordpress/↩︎"
  },
  {
    "objectID": "posts/pandas_method_chaning.html",
    "href": "posts/pandas_method_chaning.html",
    "title": "Pandas Method Chaining",
    "section": "",
    "text": "Introduction\nMethod chaining is a way to combine multiple pandas operations together into a single line or statement. It is not necessary to use method chaining, but it can make your code more concise and easier to read. In this post, we will look at how to use method chaining to clean and transform a dataset.\nHonestly, since I started using method chaining, I have found it difficult to go back to the old way of writing pandas code. I hope that by the end of this post, you will feel the same way. :)\nLet’s get started! First I show how to use method chaining to import a dataset as follows:\n\n## Data imports\n\ndf = (\n    # use sm \n    sm                          \n    # get datasets attribute\n    .datasets                   \n    # use get_rdataset method\n    .get_rdataset(              \n        \"car_prices\", \n        package='modeldata'\n    )\n    # get data attribute\n    .data\n)\n\nChaining in Python uses () in order to chain methods together. This allows the user to write multiple statements on different lines as showed in the code above. On big advantage of chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Not using chaining would require the user to write the code as follows:\ndf = sm.datasets.get_rdataset(\"car_prices\", package='modeldata').data\nUsing chaining makes the code more readable right?\nWhat I like most about chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Especially when working with large datasets, it is very useful to be able to comment out one line at a time.\nBefore continuing with the next section, let’s have a look at the data:\n\n(\n    df\n    .head()\n)\n\n\n\nTable 1: Data Preview\n\n\n\n\n\n\n\n\n\n\nPrice\nMileage\nCylinder\nDoors\nCruise\nSound\nLeather\nBuick\nCadillac\nChevy\nPontiac\nSaab\nSaturn\nconvertible\ncoupe\nhatchback\nsedan\nwagon\n\n\n\n\n0\n22661.05\n20105\n6\n4\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n21725.01\n13457\n6\n2\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n2\n29142.71\n31655\n4\n2\n1\n1\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n3\n30731.94\n22479\n4\n2\n1\n0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n4\n33358.77\n17590\n4\n2\n1\n1\n1\n0\n0\n0\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n\n\nThis dataset describes the price of sold cars by make, mileage and other features. Let us assume we would like to convert the data into a tidy format. We start with the car features.\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\nTable 2: Tidy format of car features\n\n\n\n\n\n\n\n\n\n\nPrice\nMileage\nBuick\nCadillac\nChevy\nPontiac\nSaab\nSaturn\nFeature\nValue\n\n\n\n\n3783\n8638.93\n25216\n0\n0\n1\n0\n0\n0\nLeather\n0\n\n\n5391\n8638.93\n25216\n0\n0\n1\n0\n0\n0\ncoupe\n0\n\n\n567\n8638.93\n25216\n0\n0\n1\n0\n0\n0\nCylinder\n4\n\n\n2175\n8638.93\n25216\n0\n0\n1\n0\n0\n0\nCruise\n0\n\n\n6999\n8638.93\n25216\n0\n0\n1\n0\n0\n0\nsedan\n1\n\n\n\n\n\n\n\n\n\n\nLet’s now continue and see how we can use the assign method to create new columns in our dataframe. We will use the assign method to create a new column called Make which will a vector representation of the car make.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\nTable 3: Continued from the previous chain I\n\n\n\n\n\n\n\n\n\n\nPrice\nMileage\nFeature\nValue\nMake\n\n\n\n\n3783\n8638.93\n25216\nLeather\n0\n[0, 0, 1, 0, 0, 0]\n\n\n5391\n8638.93\n25216\ncoupe\n0\n[0, 0, 1, 0, 0, 0]\n\n\n567\n8638.93\n25216\nCylinder\n4\n[0, 0, 1, 0, 0, 0]\n\n\n2175\n8638.93\n25216\nCruise\n0\n[0, 0, 1, 0, 0, 0]\n\n\n6999\n8638.93\n25216\nsedan\n1\n[0, 0, 1, 0, 0, 0]\n\n\n\n\n\n\n\n\n\n\nLet’s continue and use groupby and aggregate the Feature column by the agg method. This will give us the mean of each feature appears in the dataset.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .groupby(\n        by = 'Feature',\n    )\n    .agg(\n        {\n            'Value': ['mean'],\n        }\n    )\n)\n\n\n\nTable 4: Continued from the previous chain II\n\n\n\n\n\n\n\n\n\n\nValue\n\n\n\nmean\n\n\nFeature\n\n\n\n\n\nCruise\n0.752488\n\n\nCylinder\n5.268657\n\n\nDoors\n3.527363\n\n\nLeather\n0.723881\n\n\nSound\n0.679104\n\n\nconvertible\n0.062189\n\n\ncoupe\n0.174129\n\n\nhatchback\n0.074627\n\n\nsedan\n0.609453\n\n\nwagon\n0.079602\n\n\n\n\n\n\n\n\n\n\nSummarizing, we could go on forever and adding more steps to our method chain. But I think you get the point. We can do a lot with method chaining and it is a great way to write clean and readable code.\n\n\nConclusion\nMethod chaining in Pandas is a powerful technique that offers several advantages for data manipulation and analysis workflows. It involves combining multiple operations on a DataFrame or Series into a single, concise chain of method calls. This approach enhances code readability, maintainability, and efficiency.\nFirstly, method chaining reduces the need for intermediate variables, streamlining code and making it more readable. By stringing together operations, such as filtering, transforming, and aggregating, the code becomes a clear and sequential representation of the data transformation process.\nSecondly, it encourages the use of functionally composed operations, leading to more modular and reusable code. This modular nature facilitates changes and updates, as adjustments can be made within the chain without affecting other parts of the code.\nFurthermore, method chaining promotes better memory usage and performance optimization. Pandas optimizes these chains under the hood, reducing the creation of unnecessary intermediate copies of data frames, which leads to improved execution speed and reduced memory overhead.\nLastly, method chaining aligns well with the “tidy data” philosophy, as it emphasizes a more structured, organized approach to data manipulation. This promotes consistency and clarity in the analysis process, aiding in collaboration and code maintenance.\nI hope you enjoyed this post and learned something new. If you have any questions contact me. Thanks for reading!"
  },
  {
    "objectID": "posts/devops.html",
    "href": "posts/devops.html",
    "title": "DevOps with Docker & Gitlab",
    "section": "",
    "text": "My intention with this article is to provide a hands-on demonstration of what I have accomplished so far in DevOps. Rather than just discussing theory, I will show you the following:\n\nHow to leverage a Dockerfile to create a Docker image.\nHow to push and pull the image to and from a private Docker registry and not from public Docker Hub.\nHow to push the output of a process to GitHub in order use Github Pages to publish a website.\n\nWe are going to use GitLab CI/CD to automate all the steps mentioned above.\nPlease note that I am not an expert in DevOps and am still learning. I am sharing this article to show you what I have done so far and to get feedback from you. :)"
  },
  {
    "objectID": "posts/devops.html#requirements",
    "href": "posts/devops.html#requirements",
    "title": "DevOps with Docker & Gitlab",
    "section": "Requirements",
    "text": "Requirements\nIn order to be able to follow along with this article, you need to have the following installed/registered:\n\nDocker\nGitLab account\nGithub account\n\nOptional:\n\nGitLab Runner installed on your server/computer\n\nThese are the only requirements you need to follow along with this article. Next, we will discuss the requirements for the project we are going to work on."
  },
  {
    "objectID": "posts/devops.html#choose-a-project",
    "href": "posts/devops.html#choose-a-project",
    "title": "DevOps with Docker & Gitlab",
    "section": "Choose a Project",
    "text": "Choose a Project\nI have chosen a simple project that I have been working on—this blog 😜\n\n\n\n\n\n\nThink about a project that you have been working on and would like to automate the process of building and pushing. It doesn’t really matter what project you choose. The important thing is to understand the process of building, pushing, and pulling a Docker image and how to leverage GitLab CI/CD to automate the process.\n\n\n\nOur plan is as follows:\nFirst, we push our code to GitLab and then use GitLab CI/CD to build and push a Docker image to our private Docker registry. We will then pull the image and “do something” with it. In my case, I am going to render this blog using ̀Quarto and push it to GitHub. Finally, we are going to use GitHub Actions to automate the process of pushing the output of the process to GitHub Pages."
  },
  {
    "objectID": "posts/devops.html#dockerfile",
    "href": "posts/devops.html#dockerfile",
    "title": "DevOps with Docker & Gitlab",
    "section": "Dockerfile",
    "text": "Dockerfile\nAgain, the Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Using Dockerfile, we can automate the process of building a Docker image.\nHere, I demonstrate how I utilized a Dockerfile to build a Docker image for this blog. I used Quarto framework to render HTML pages. I chose to build the image from scratch, starting with the Ubuntu base image and installing all necessary dependencies along with the Quarto application.\n# Base image\nFROM ubuntu:jammy\n\n# Install the required packages\nRUN apt-get update && \\\n    apt-get install -y \\\n    bash \\\n    curl \\\n    git \\\n    jq \\\n    openssh-client \\\n    ca-certificates \\\n    dpkg\n\n# Install quarto  \nRUN curl -sL -o installer.deb \\\n    https://github.com/quarto-dev/quarto-cli/releases/download/v1.5.56/quarto-1.5.56-linux-amd64.deb && \\\n    dpkg -i installer.deb && \\\n    rm installer.deb\n\n# Create a directory for the app\nRUN mkdir -p /app\n\n# Set the working directory\nWORKDIR /app\n\n# CHMOD app - give permission to the app directory\nRUN chmod -R 777 /app\n\n# Copy the content of the local src directory to the working directory\nCOPY . .\n# Make a directory for the output (empty for now)\nRUN mkdir -p ./_output\nWe start with the base image ubuntu:jammy, which is a Docker image containing the Ubuntu operating system. First, we install the necessary dependencies and download the Quarto installer from GitHub, then proceed with its installation. Next, we create a directory for the app and set the working directory to /app. We copy the contents of the current directory into the /app directory and create an _output directory. This _output directory will store the process output, which will eventually be pushed to GitHub. Currently, _output is empty."
  },
  {
    "objectID": "posts/devops.html#docker-registry",
    "href": "posts/devops.html#docker-registry",
    "title": "DevOps with Docker & Gitlab",
    "section": "Docker Registry",
    "text": "Docker Registry\nAs we decided to create a private Docker registry to store our Docker images, we use the Docker registry image to create a private Docker registry and also a convenient UI to manage the registry. The easiest way to do this is by using Docker Compose, which is a tool for defining and running multi-container Docker applications.\nservices:\n\n  registry-server:\n    image: registry:2.8\n    container_name: registry-server\n    ports:\n    - \"5000:5000\"\n    restart: always\n    volumes:\n      - ./registry-data:/var/lib/registry\n    \n  registry-ui:\n    image: joxit/docker-registry-ui:2.5-debian\n    container_name: registry-ui\n    restart: always\n    ports:\n      - \"9000:80\"\n    environment:\n      SINGLE_REGISTRY: true\n      REGISTRY_TITLE: Own Docker Registry UI\n      DELETE_IMAGES: true\n      SHOW_CONTENT_DIGEST: true\n      NGINX_PROXY_PASS_URL: http://registry-server:5000\n      SHOW_CATALOG_NB_TAGS: true\n      CATALOG_MIN_BRANCHES: 1\n      CATALOG_MAX_BRANCHES: 1\n      TAGLIST_PAGE_SIZE: 100\n      REGISTRY_SECURED: false\n      CATALOG_ELEMENTS_LIMIT: 1000\n      THEME: dark\nFirst, we define the services that we are going to use. We define two services: registry-server and registry-ui. The registry-server is the private Docker registry that we are going to use to store the Docker image. The registry-ui is a Docker image that provides a UI for the private Docker registry. We can use the UI to see the images that are stored in the private Docker registry."
  },
  {
    "objectID": "posts/devops.html#gitlab-cicd",
    "href": "posts/devops.html#gitlab-cicd",
    "title": "DevOps with Docker & Gitlab",
    "section": "Gitlab CI/CD",
    "text": "Gitlab CI/CD\nHaving a Dockerfile and Registry means we are almost ready to deploy.\nThe final step in our pipeline is to wrap everything up in a pipeline. This is where Gitlab CI/CD comes into play. The .gitlab-ci.yml file is where we define the pipeline and the steps that are going to be executed when we push code to the repository. Stages, variables and jobs are defined in the .gitlab-ci.yml file.\nstages:\n  - build\n  - render\n  - deploy\n\nvariables: \n  REGISTRY_HOST: '&lt;host-url&gt;'\n  REGISTRY_USER: '&lt;host-user&gt;'\n  APP_NAME: '$CI_PROJECT_NAME'\n  APP_VERSION: '$CI_COMMIT_BRANCH'\n  \n## Build the Docker image and push it to the private Docker registry\nbuild:\n  stage: build\n  image: docker\n  services:\n    - name: docker:27.1.1-dind\n      alias: docker\n      command: [ \"--tls=false\",\"--insecure-registry=&lt;host-url&gt;\"]\n  script:\n    - docker build -t $REGISTRY_HOST/$REGISTRY_USER/$APP_NAME:$APP_VERSION .\n    - docker push $REGISTRY_HOST/$REGISTRY_USER/$APP_NAME:$APP_VERSION\n  tags: [your_tag]\n\n## Render the bookdown project using the previous build image\n## content is rendered to \"_output\"\nrender:\n  stage: render\n  image: $REGISTRY_HOST/$REGISTRY_USER/$APP_NAME:$APP_VERSION\n  script:\n    - quarto render\n  artifacts:\n    paths: ['_output/']\n    expire_in: 10 mins\n  tags: [your_tag]\n\n# Push the content of \"_output\" to github (for github pages)\n# Fetch output from the previous stage\npush_output_to_github:\n  stage: deploy\n  image: ubuntu:jammy\n  before_script:\n    - apt-get update && apt-get install -y git\n  script:\n    - git config --global user.email \"&lt;your-email&gt;\"\n    - git config --global user.name \"&lt;your-name&gt;\"\n    - git clone https://$GITHUB_TOKEN@github.com/&lt;your-name&gt;/&lt;repo-name&gt;.git\n    - git checkout main\n    - cd blog\n    - rm -rf *\n    - cp -r ../_output/* .\n    - git status\n    - git add .\n    - git status\n    - git commit -m \"auto-commit for deployment\"\n    - git push origin main\n  dependencies:\n    - render\n  only:\n    - main\n  tags: [your_tag]\n\n\n\n\n\n\nVariables like $GITHUB_TOKEN should be defined in the Gitlab settings. You can define them in the CI/CD settings of your project.\n\n\n\nThis YAML file defines a CI/CD pipeline with three stages: build, render, and deploy and four variables that are used in the pipeline. Let’s review the pipeline:\nVariables:\n\nREGISTRY_HOST: URL of the Docker registry.\nREGISTRY_USER: User for the Docker registry.\nAPP_NAME: Name of the application, derived from the CI project name.\nAPP_VERSION: Version of the application, derived from the CI commit branch.\n\nBuild Stage:\n\nUses the docker image.\nRuns a Docker-in-Docker service.\nBuilds a Docker image and pushes it to the private Docker registry.\n\nRender Stage:\n\nUses the Docker image built in the previous stage.\nRuns quarto render to render the bookdown project.\nSaves the rendered content to the _output directory as artifacts.\n\nDeploy Stage:\n\nUses the ubuntu:jammy image.\nInstalls Git.\nConfigures Git user details.\nClones a GitHub repository using a token.\nCopies the rendered content from _output to the repository.\nCommits and pushes the changes to the main branch.\nThe file also includes a tip that variables like $GITHUB_TOKEN should be defined in the GitLab CI/CD settings.\n\nWe have seen how to build a Docker image, push it to a private Docker registry, render the content, and push the output to GitHub.\nIn my case, where I would like to publish a static HTML website I still need to go one step further. I need enable Github Actions to publish a website using Github Pages. Github Pages is a static site hosting service that takes HTML, CSS, and JavaScript files straight from a repository on Github and publishes a website - for more information on how to enable Github Pages for your repository, please refer to the official documentation."
  }
]
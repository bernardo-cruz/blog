[
  {
    "objectID": "posts/pandas_method_chaning.html",
    "href": "posts/pandas_method_chaning.html",
    "title": "Pandas Method Chaining",
    "section": "",
    "text": "Introduction: Pandas and Method Chaining\nMethod chaining is a way to combine multiple pandas operations together into a single line or statement. It is not necessary to use method chaining, but it can make your code more concise and easier to read. In this post, we will look at how to use method chaining to clean and transform a dataset.\nHonestly, since I started using method chaining, I have found it difficult to go back to the old way of writing pandas code. I hope that by the end of this post, you will feel the same way. :)\nLet’s get started! First I show how to use method chaining to import a dataset as follows:\n\n## Data imports\n\ndf = (\n    # use sm \n    sm                          \n    # get datasets attribute\n    .datasets                   \n    # use get_rdataset method\n    .get_rdataset(              \n        \"car_prices\", \n        package='modeldata'\n    )\n    # get data attribute\n    .data\n)\n\nChaining in Python uses () in order to chain methods together. This allows the user to write multiple statements on different lines as showed in the code above. On big advantage of chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Not using chaining would require the user to write the code as follows:\ndf = sm.datasets.get_rdataset(\"car_prices\", package='modeldata').data\nUsing chaining makes the code more readable right?\nWhat I like most about chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Especially when working with large datasets, it is very useful to be able to comment out one line at a time.\nBefore continuing with the next section, let’s have a look at the data:\n\n(\n    df\n    .head()\n)\n\n\n\n\n\nTable 1:  Data Preview \n  \n    \n      \n      Price\n      Mileage\n      Cylinder\n      Doors\n      Cruise\n      Sound\n      Leather\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      convertible\n      coupe\n      hatchback\n      sedan\n      wagon\n    \n  \n  \n    \n      0\n      22661.05\n      20105\n      6\n      4\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      21725.01\n      13457\n      6\n      2\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      2\n      29142.71\n      31655\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      30731.94\n      22479\n      4\n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      33358.77\n      17590\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nThis dataset describes the price of sold cars by make, mileage and other features. Let us assume we would like to convert the data into a tidy format. We start with the car features.\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable 2:  Tidy format of car features \n  \n    \n      \n      Price\n      Mileage\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      Feature\n      Value\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Leather\n      0\n    \n    \n      5391\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      coupe\n      0\n    \n    \n      567\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cylinder\n      4\n    \n    \n      2175\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cruise\n      0\n    \n    \n      6999\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      sedan\n      1\n    \n  \n\n\n\n\n\nLet’s now continue and see how we can use the assign method to create new columns in our dataframe. We will use the assign method to create a new column called Make which will a vector representation of the car make.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable 3:  Continued from the previous chain I \n  \n    \n      \n      Price\n      Mileage\n      Feature\n      Value\n      Make\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      Leather\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      5391\n      8638.93\n      25216\n      coupe\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      567\n      8638.93\n      25216\n      Cylinder\n      4\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      2175\n      8638.93\n      25216\n      Cruise\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      6999\n      8638.93\n      25216\n      sedan\n      1\n      [0, 0, 1, 0, 0, 0]\n    \n  \n\n\n\n\n\nLet’s continue and use groupby and aggregate the Feature column by the agg method. This will give us the mean of each feature appears in the dataset.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .groupby(\n        by = 'Feature',\n    )\n    .agg(\n        {\n            'Value': ['mean'],\n        }\n    )\n)\n\n\n\n\n\nTable 4:  Continued from the previous chain II \n  \n    \n      \n      Value\n    \n    \n      \n      mean\n    \n    \n      Feature\n      \n    \n  \n  \n    \n      Cruise\n      0.752488\n    \n    \n      Cylinder\n      5.268657\n    \n    \n      Doors\n      3.527363\n    \n    \n      Leather\n      0.723881\n    \n    \n      Sound\n      0.679104\n    \n    \n      convertible\n      0.062189\n    \n    \n      coupe\n      0.174129\n    \n    \n      hatchback\n      0.074627\n    \n    \n      sedan\n      0.609453\n    \n    \n      wagon\n      0.079602\n    \n  \n\n\n\n\n\nSummarizing, we could go on forever and adding more steps to our method chain. But I think you get the point. We can do a lot with method chaining and it is a great way to write clean and readable code.\n\n\nConclusion\nMethod chaining in Pandas is a powerful technique that offers several advantages for data manipulation and analysis workflows. It involves combining multiple operations on a DataFrame or Series into a single, concise chain of method calls. This approach enhances code readability, maintainability, and efficiency.\nFirstly, method chaining reduces the need for intermediate variables, streamlining code and making it more readable. By stringing together operations, such as filtering, transforming, and aggregating, the code becomes a clear and sequential representation of the data transformation process.\nSecondly, it encourages the use of functionally composed operations, leading to more modular and reusable code. This modular nature facilitates changes and updates, as adjustments can be made within the chain without affecting other parts of the code.\nFurthermore, method chaining promotes better memory usage and performance optimization. Pandas optimizes these chains under the hood, reducing the creation of unnecessary intermediate copies of data frames, which leads to improved execution speed and reduced memory overhead.\nLastly, method chaining aligns well with the “tidy data” philosophy, as it emphasizes a more structured, organized approach to data manipulation. This promotes consistency and clarity in the analysis process, aiding in collaboration and code maintenance.\nI hope you enjoyed this post and learned something new. If you have any questions contact me. Thanks for reading!"
  },
  {
    "objectID": "posts/glm.html",
    "href": "posts/glm.html",
    "title": "My first post about statmodels - so be nice! :)",
    "section": "",
    "text": "Introduction: Play with GLMs\nDuring my study at HSLU I have learned to use mostly R when it comes to statistic and for some machine learning courses as well. When it comes to work, the most companies I have seen so far, use Python as their main programming language. My previous two employers used Python and R. This article is not going to discuss which language is better, but rather focus on how to use Python and the library statmodels which seems to produce similar outputs as it would in R.\nOne important and cool thing about the statmodels library is that it has a lot of data sets already included. You can find them here. The data set I use is a R data set from Rdatasets. In particular, the dataset from the package modeldata called car_prices is used.\n## Data imports\ndf = (\n    statsmodels.api # imported as sm in following code\n    .datasets\n    .get_rdataset(\"car_prices\", package='modeldata')\n    .data\n)\nAnother important thing about this blog post is the usage of Quarto. Quarto allowed me to write this blog post directly from a Jupyter-Notebook without any fancy and complicated transformations. For more information, please visit the quarto website.\nHowever, let’s start with the analysis. The first thing we do is to import the necessary libraries and the data set.\n\n\nShow the code\n## Data manipulation imports\nimport pandas as pd\nimport numpy as np\n\n## Display imports\nfrom IPython.display import display, Markdown\n\n## statmodels import\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.genmod.families.family as fam\nfrom patsy import dmatrices\n\n## Plot imports\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (5,5/2.5)\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_theme()\nsns.set_context(\n    \"paper\", \n    rc={\n        \"figsize\"       :   plt.rcParams['figure.figsize'],\n        'font_scale'    :   1.25,\n    }\n)\n\n\n## Data imports\ndf = (\n    sm\n    .datasets\n    .get_rdataset(\"car_prices\", package='modeldata')\n    .data\n)\n\n\n\n\nExplanatory Data Exploration\nThe data exploration is a crucial step before fitting a model. It allows to understand the data and to identify potential problems. In this notebook, we will explore very briefly the data, as the focus of this article is to understand and apply statmodels.\n\n\nThe data set contains 804 observations (rows) and 18 predictors (columns). The data set contains information about car prices and the variables are described as follows: Price; Mileage; Cylinder; Doors; Cruise; Sound; Leather; Buick; Cadillac; Chevy; Pontiac; Saab; Saturn; convertible; coupe; hatchback; sedan; and wagon.\n\n\nFirst, two approximately continous variables Price and Mileage are investigated by means of graphical analysis. The following bar plots show the distribution of the two variables.\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng1 = sns.displot(\n    data = df,\n    x = 'Price',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of car price')\nplt.xlabel('Price')\nplt.ylabel('Count of occurance')\nplt.show(g1)\n\ng2 = sns.displot(\n    data = df,\n    x = 'Mileage',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Count of occurance')\nplt.show(g2)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable Price\n\n\n\n\n\n\n\n(b) Distribution of the variable Mileage\n\n\n\n\nFigure 1: Distribution of price and mileage variables.\n\n\n\nFigure 1 (a) shows on the left a right-skewed distribution with a peak around 15k$ and price values ranging from 8k dollars to 70k dollars. On the other hand, figure 1 (b) shows on the right a more ballanced distribution with a peak around 20k$ and price values ranging from 266 miles up to 50k miles.\nProceeding to the next two variables, Cylinder and Doors, one can see less possible values, ranging from\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng = sns.displot(\n    data = df,\n    x = 'Cylinder',\n    discrete = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Cylinder')\nplt.xlabel('Cylinder')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n# plt.figure()\ng = sns.displot(\n    data = df,\n    x = 'Doors',\n    discrete = True,\n    shrink = 0.5,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Doors')\nplt.xlabel('Doors')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable cylinder\n\n\n\n\n\n\n\n(b) Distribution of the variable doors\n\n\n\n\nFigure 2: Distribution of cylinder and doors variables.\n\n\n\nThe Figure 2 (a) surprised me quite a bit. I had anticipated the car to feature more than 8 cylinders, given that this dataset pertains to American cars. The cylinder count typically spans from 4 to 8, with the values accurately reflecting this range. It’s worth noting that the number of cylinders is expected to be even.\nAgain surprisingly, the Figure 2 (b) shows that the number of doors per car. The values are either 2 or 4, with the latter being more common. This is a bit surprising, as I would have expected the number of doors to be higher for American cars (SUV).\nNext, we check the distribution of the make of the cars in the dataset. For this analysis, we first pivot the dataframe using pd.melt() and calculate the sum by means of groupby() and sum() method as follows:\n\n\nShow the code\nbrands = (\n    df\n    [['Buick', 'Cadillac', 'Chevy', 'Pontiac', 'Saab']]\n    .melt()\n    .groupby('variable')\n    .sum()\n    .reset_index()\n)\n\nbrands.head()\n\n\n\n\n\n\nTable 1:  Cars by brand after melting, grouping, and summing \n  \n    \n      \n      variable\n      value\n    \n  \n  \n    \n      0\n      Buick\n      80\n    \n    \n      1\n      Cadillac\n      80\n    \n    \n      2\n      Chevy\n      320\n    \n    \n      3\n      Pontiac\n      150\n    \n    \n      4\n      Saab\n      114\n    \n  \n\n\n\n\n\nAfter aggregation, the visualization of the data is as follows:\n\n\nShow the code\nsns.catplot(\n    data    = brands.sort_values('value', ascending=False),\n    x       = 'variable',\n    y       = 'value',\n    kind    = 'bar',\n    height  = 5,\n    aspect  = 2,\n)\nplt.title('Number of cars by brand')\nplt.xlabel('Brand')\nplt.ylabel('Number of cars')\nplt.show()\n\n\n\n\n\nFigure 3: Number of cars by make\n\n\n\n\nIn descending order the most present make is: Chevy followed by Pontiac, Saab, Buick and Cadilac.\nAt this point it should be mentioned, that the data set is not balanced and the distribution of the features is not uniform across the different makes and car features. In a normal project this would be a point to consider and to take care of. However, for the purpose of this project, this is not necessary.\n\n\nModeling\nModeling with statsmodels becomes straightforward once the formula API and provided documentation are well understood. I encountered a minor challenge in grasping the formula API, but once I comprehended it, the usage turned out to be quite intuitive.\nLet’s delve into an understanding of the formula API (statsmodels.formula.api). This feature enables us to employ R-style formulas for specifying models. To illustrate, when fitting a linear model, the following formula suffices:\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\n    formula='y ~ x1 + x2 + x3', \n    data=df\n)\nThe formula API leverages the patsy package (Patsy Package). It’s worth noting that the formula API’s potency extends to intricate models. Additionally, it’s important to mention that the formula API automatically incorporates an intercept into the formula if one isn’t explicitly specified. For cases where the intercept is undesired, a -1 can be used within the formula.\nWith the glm class, a vast array of models becomes accessible importing as follows:\nimport statsmodels.genmod.families.family as fam\nThe fam import is necessary for specifying the family of the model. The families available are:\n\n\n\n\nTable 2: GLM Families\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nFamily(link, variance[, check_link])\nThe parent class for one-parameter exponential families.\n\n\nBinomial([link, check_link])\nBinomial exponential family distribution.\n\n\nGamma([link, check_link])\nGamma exponential family distribution.\n\n\nGaussian([link, check_link])\nGaussian exponential family distribution.\n\n\nInverseGaussian([link, check_link])\nInverseGaussian exponential family.\n\n\nNegativeBinomial([link, alpha, check_link])\nNegative Binomial exponential family (corresponds to NB2).\n\n\nPoisson([link, check_link])\nPoisson exponential family.\n\n\nTweedie([link, var_power, eql, check_link])\nTweedie family.\n\n\n\n\n\n\nFitting the model and analyzing the results are the same as one would using R. First define the model, then fit it, then analyze the results. The details of the fit can be accessed using the summary method.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  Price   No. Observations:                  804\nModel:                            GLM   Df Residuals:                      789\nModel Family:                Gaussian   Df Model:                           14\nLink Function:               Identity   Scale:                      8.4460e+06\nMethod:                          IRLS   Log-Likelihood:                -7544.8\nDate:                Thu, 17 Aug 2023   Deviance:                   6.6639e+09\nTime:                        11:07:11   Pearson chi2:                 6.66e+09\nNo. Iterations:                     3   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nMileage        -0.1842      0.013    -14.664      0.000      -0.209      -0.160\nCylinder     3659.4543    113.345     32.286      0.000    3437.303    3881.606\nDoors        1654.6326    174.525      9.481      0.000    1312.570    1996.695\nCruise        340.8695    295.962      1.152      0.249    -239.205     920.944\nSound         440.9169    234.484      1.880      0.060     -18.664     900.497\nLeather       790.8220    249.745      3.167      0.002     301.331    1280.313\nBuick       -1911.3752    336.292     -5.684      0.000   -2570.495   -1252.256\nCadillac      1.05e+04    409.274     25.663      0.000    9701.132    1.13e+04\nChevy       -3408.2863    213.274    -15.981      0.000   -3826.295   -2990.278\nPontiac     -4258.9628    256.358    -16.613      0.000   -4761.416   -3756.509\nSaab         9419.1227    331.211     28.438      0.000    8769.960    1.01e+04\nSaturn      -2859.0803    358.709     -7.970      0.000   -3562.137   -2156.023\nconvertible  1.258e+04    525.984     23.922      0.000    1.16e+04    1.36e+04\ncoupe        1559.7620    395.946      3.939      0.000     783.723    2335.801\nhatchback   -4977.3196    339.046    -14.680      0.000   -5641.837   -4312.803\nsedan       -3064.7176    215.007    -14.254      0.000   -3486.123   -2643.312\nwagon        1384.6400    364.920      3.794      0.000     669.410    2099.870\n===============================================================================\n\n\nAfter fitting a GLM using the glm class in statsmodels, you can obtain a summary of the model’s results using the .summary() method on the fitted model object. Here’s a general overview of the information typically included in the summary output:\n\nModel Information:\n\nThe name of the model.\nThe method used for estimation (e.g., maximum likelihood).\nThe distribution family (e.g., Gaussian, binomial, Poisson).\nThe link function used in the model (e.g., logit, identity, etc.).\nThe number of observations used in the model.\n\nModel Fit Statistics:\n\nLog-likelihood value.\nAIC (Akaike Information Criterion) and/or BIC (Bayesian Information Criterion).\nDeviance and Pearson chi-square statistics.\nDispersion parameter (if applicable).\n\nCoefficients:\n\nEstimated coefficients for each predictor variable.\nStandard errors of the coefficients.\nz-scores and p-values for testing the significance of the coefficients.\n\nConfidence Intervals:\n\nConfidence intervals for each coefficient, often at a default level like 95%.\n\nHypothesis Testing:\n\nHypothesis tests for the coefficients, typically with null hypothesis being that the coefficient is zero.\n\nGoodness of Fit:\n\nLikelihood-ratio test for overall model fit.\nTests for assessing the contribution of individual predictors to the model.\n\nDiagnostic Information:\n\nInformation about model assumptions and diagnostics, depending on the type of GLM and the method used.\n\nResiduals:\n\nInformation about residuals, which can include measures like deviance residuals, Pearson residuals, etc.\n\n\nRefer to the official documentation for the most accurate and up-to-date information about the summary output for your specific use case.\n\n\nConclusion\nStatsmodels is a powerful Python library for statistical modeling and hypothesis testing, making it an excellent transition for R users or Python users who like R to solve certain problems. It offers a familiar syntax and functionality for regression, ANOVA, and more. Its integration with Python’s data analysis ecosystem, like pandas, allows seamless data manipulation. With support for various statistical methods and a comprehensive summary output, Statsmodels facilitates effortless migration from R, enabling R users to harness its capabilities in a Python environment."
  },
  {
    "objectID": "WorkInProgress/api.html",
    "href": "WorkInProgress/api.html",
    "title": "FastAPI",
    "section": "",
    "text": "TL;DR\nWe build a custom API service using public available data from the canton of Zurich in Switzerland. We share this data using FastAPI from a local database of type SQLite. We use the libraries pandas, fastapi, type-annotation, sqlalchemy and seaborn for plots.\nA complete example is showed how to create a custom API service considering public available data, in this case, from the canton of Zurich in Switzerland. We use pandas for data manipulation, fastapi for the schematics of our api, sqlalchemy as ORM for a SQLite database and seaborn for plots.\n\n\nMotivation & Installation\nDid you ever dream to build your own fast API written in Python?\nFastAPI is a modern, high-performance web framework for building APIs with Python. It blends the ease of use of Python with exceptional speed, making it an ideal choice for developing robust and efficient web applications.\nIts automatic validation, interactive documentation, and type hints enhance developer productivity and code reliability. Whether you’re crafting a simple REST API or a complex microservices architecture, FastAPI streamlines development, promotes clean code, and optimizes performance, making it the go-to framework for those seeking both speed and simplicity in their Python web projects.\nThe installation of FastAPI is very simple and it involves the installation of two packages: (1) FastAPI and (2) Uvicorn. The first one is the framework itself and the second one is the server that will run the API.\npip install fastapi\npip install \"uvicorn[standard]\"\nNext, we discuss the data and the corresponding database technology that we will use to store the data.\n\n\nData & Database\nSwitzerland’s unique tax system is characterized by its federal structure, granting significant fiscal autonomy to its municipalities and cantons. The country’s taxation policies are designed to maintain a balanced distribution of responsibilities and resources between the federal government, cantonal governments, and local municipalities. This results in a decentralized tax framework where both cantons and municipalities possess substantial authority over taxation, allowing them to tailor policies according to their specific needs and preferences.\nThis system encourages competition among cantons and municipalities while also fostering a sense of local ownership and control over financial matters. As a result, Switzerland’s tax landscape is complex, diverse, and reflective of the nation’s commitment to decentralized governance.\n\n\nWe are going to work in this post with data from the nicest and best canton of Switzerland called Zurich :) We have data from 201 municipalities during the period between 1990-2022. The data presents the mean income tax per municipality and year.\n\n\n\n\n\n\n\nFigure 1: Distribution of average income tax per municipality in the canton of Zurich per year.\n\n\n\n\nFigure 1 shows for each municipality the average paid income tax per person per year. It can be observed that the spread and average income tax is increasing. Since the focus of this article is in creating a custom API service we continue without futher analysis.\nThe next step involves storing the data within a database, a crucial endeavor that lays the foundation for simulating a dynamic production environment. What adds an extra layer of versatility and efficiency to our approach is the utilization of sqlalchemy: a remarkable tool empowers us to seamlessly alter the connection string (see code below, line 10), enabling us to effortlessly establish connections across a diverse array of database types. This fusion of careful data handling and technological prowess harmonizes to create a truly agile and adaptable simulation framework.\n\nimport statsmodels.api as sm\nfrom sqlalchemy import create_engine\n\n# Create a connection to the database\nengine = create_engine(              \n    ## Path to database                     \n    'sqlite:///../../data/FastAPI.db',      # <1>\n)\n\n## Create a table in new database\ndf.to_sql(\n    name = 'fastapi',\n    con = engine,\n    if_exists = 'replace',\n    index = False,\n);\n\n\n\nBuild basic API\n(use lambda statement when possible)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "",
    "text": "I am Bernardo, an aeronautical engineer with a BSc in Aeronautical Engineering and a MSc in Data Science. My passion lies in both coding and engineering. I am from Zurich 🇨🇭 and have over seven years of experience working as an aeronautical engineer and about three years as a data scientist.\nI would like to invite you go through my CV and blog to learn more about me 🤓. Please don’t hesitate to contact me if you have any questions or would like to collaborate on a project 🤝."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "👨🏽‍💻 Experience",
    "text": "👨🏽‍💻 Experience\nExplore my journey through various industries and roles in the world of data science and beyond! From predictive maintenance in the railway sector 🚂🚂🚂, to crafting models for turnover prediction in the food industry 🍔🍕🍝 and upsell models banking 💵💰💳, my experiences have been diverse and impactful. Delve into my work with Gaussian Mixture Models, Multi-Output Regression, OCR tools, and more. For a detailed account of my professional voyage, head to Section CV."
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "🏝️ 🏙️ 💻 📺 Hobbies",
    "text": "🏝️ 🏙️ 💻 📺 Hobbies\nI love to travel 🛩️ 🚂 ⛰️ 🏝️ 🏙️ to code 💻 and binge watch 📺 in my free time. In particular I discovered a passion for web development and this blog 🤓😎."
  },
  {
    "objectID": "index.html#tech-stack",
    "href": "index.html#tech-stack",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "💻 Tech Stack:",
    "text": "💻 Tech Stack:"
  },
  {
    "objectID": "index.html#github-trophies",
    "href": "index.html#github-trophies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "🏆 GitHub Trophies",
    "text": "🏆 GitHub Trophies"
  },
  {
    "objectID": "index.html#wisdom-of-the-day",
    "href": "index.html#wisdom-of-the-day",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "🧐 Wisdom of the Day",
    "text": "🧐 Wisdom of the Day"
  },
  {
    "objectID": "index.html#farewell-message",
    "href": "index.html#farewell-message",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "🙌🏽 Farewell message",
    "text": "🙌🏽 Farewell message\nsee you soon"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "I am a former aeronautical engineer with 3 years of experience as a data scientist and a recent graduate of MSc in Data Science (summer 2023). During my 7 years as an aeronautical engineer, I managed complex interdisciplinary projects in aircraft structure development and certification. As a data scientist, I developed and maintained machine learning models, demonstrating a passion for learning and innovation."
  },
  {
    "objectID": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "href": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "title": "Curriculum Vitae",
    "section": "Stadler Rail AG, Data Scientist (06/2023 – present)",
    "text": "Stadler Rail AG, Data Scientist (06/2023 – present)\n\nDeveloped machine learning models for predictive maintenance, e.g., door systems, compressor systems, HVAC systems.\nContributed to projects developing data-driven solutions for optimizing rolling stock maintenance processes.\nEstablished CI/CD pipeline for modeling activities and created Python packages for internal use. Technologies: Gitlab, Gitlab CI/CD, Sklearn, FastAPI (Python, SQL, API)"
  },
  {
    "objectID": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "href": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "title": "Curriculum Vitae",
    "section": "Prognolite AG, Data Scientist (10/2022 – 03/2023)",
    "text": "Prognolite AG, Data Scientist (10/2022 – 03/2023)\n\nDeveloped regression models predicting turnover using customer data and external factors like weather and holidays.\nCreated multi-output regression models to predict menu sales based on customer data and external factors. Technologies: Github, Tidymodels, Sklearn, Random Forest, XGBoost, Cubist (R, Python)"
  },
  {
    "objectID": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "href": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "title": "Curriculum Vitae",
    "section": "Crealogix AG, Data Scientist (09/2021 – 09/2022)",
    "text": "Crealogix AG, Data Scientist (09/2021 – 09/2022)\n\nDeveloped prospect and upsell models.\nExtracted information from fact-sheets using customized OCR.\nCreated user journeys for chatbot solutions using IBM Watson and Spacy. Technologies: Sklearn, Spacy, Random Forest, XGBoost, RMarkdown (Python, SQL, R)"
  },
  {
    "objectID": "cv.html#bucher-leichtbau-ag-012014-092021",
    "href": "cv.html#bucher-leichtbau-ag-012014-092021",
    "title": "Curriculum Vitae",
    "section": "Bucher Leichtbau AG (01/2014 – 09/2021)",
    "text": "Bucher Leichtbau AG (01/2014 – 09/2021)\n\nCompliance Verification Engineer (10/2017 – 09/2021)\n\nAdvised and supported all departments of Bucher Leichtbau AG in initial airworthiness certification matters.\nIndependently verified documentation for “minor” and “major changes” against certification requirements. Technologies: FEMAP, Mechanical Engineering\n\n\n\nCertification Engineer (01/2014 – 10/2017)\n\nConducted FEM calculations/verification and prepared internal and external test reports.\nCommunicated with customers, official bodies, and authorities.\nConsulted development department for design optimization. Technologies: FEMAP, Mechanical Engineering"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "My first post about statmodels - so be nice! :)\n\n\n\n\n\n\n\nstatmodels\n\n\nSeaborn\n\n\nPandas\n\n\nPython vs. R\n\n\nFormula API\n\n\nRDatasets in Python\n\n\nPatsy in Python\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nBernardo Freire\n\n\n\n\n\n\n\n\nPandas Method Chaining\n\n\n\n\n\n\n\nPandas\n\n\nReadability\n\n\nMethod Chaining\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nBernardo Freire\n\n\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "posts/pandas_method_chaning.html",
    "href": "posts/pandas_method_chaning.html",
    "title": "Pandas Method Chaining",
    "section": "",
    "text": "Introduction: Pandas and Method Chaining\nMethod chaining is a way to combine multiple pandas operations together into a single line or statement. It is not necessary to use method chaining, but it can make your code more concise and easier to read. In this post, we will look at how to use method chaining to clean and transform a dataset.\nHonestly, since I started using method chaining, I have found it difficult to go back to the old way of writing pandas code. I hope that by the end of this post, you will feel the same way. :)\nLet‚Äôs get started! First I show how to use method chaining to import a dataset as follows:\n\n## Data imports\n\ndf = (\n    # use sm \n    sm                          \n    # get datasets attribute\n    .datasets                   \n    # use get_rdataset method\n    .get_rdataset(              \n        \"car_prices\", \n        package='modeldata'\n    )\n    # get data attribute\n    .data\n)\n\nChaining in Python uses () in order to chain methods together. This allows the user to write multiple statements on different lines as showed in the code above. On big advantage of chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Not using chaining would require the user to write the code as follows:\ndf = sm.datasets.get_rdataset(\"car_prices\", package='modeldata').data\nUsing chaining makes the code more readable right?\nWhat I like most about chaining is that it allows the user to write more readable code (one command per line) and debugg it more easily (comment out one line at a time). Especially when working with large datasets, it is very useful to be able to comment out one line at a time.\nBefore continuing with the next section, let‚Äôs have a look at the data:\n\n(\n    df\n    .head()\n)\n\n\n\n\n\nTable¬†1:  Data Preview \n  \n    \n      \n      Price\n      Mileage\n      Cylinder\n      Doors\n      Cruise\n      Sound\n      Leather\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      convertible\n      coupe\n      hatchback\n      sedan\n      wagon\n    \n  \n  \n    \n      0\n      22661.05\n      20105\n      6\n      4\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n    \n    \n      1\n      21725.01\n      13457\n      6\n      2\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n    \n    \n      2\n      29142.71\n      31655\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      3\n      30731.94\n      22479\n      4\n      2\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n    \n      4\n      33358.77\n      17590\n      4\n      2\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n    \n  \n\n\n\n\n\nThis dataset describes the price of sold cars by make, mileage and other features. Let us assume we would like to convert the data into a tidy format. We start with the car features.\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable¬†2:  Tidy format of car features \n  \n    \n      \n      Price\n      Mileage\n      Buick\n      Cadillac\n      Chevy\n      Pontiac\n      Saab\n      Saturn\n      Feature\n      Value\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Leather\n      0\n    \n    \n      5391\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      coupe\n      0\n    \n    \n      567\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cylinder\n      4\n    \n    \n      2175\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      Cruise\n      0\n    \n    \n      6999\n      8638.93\n      25216\n      0\n      0\n      1\n      0\n      0\n      0\n      sedan\n      1\n    \n  \n\n\n\n\n\nLet‚Äôs now continue and see how we can use the assign method to create new columns in our dataframe. We will use the assign method to create a new column called Make which will a vector representation of the car make.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .head()\n)\n\n\n\n\n\nTable¬†3:  Continued from the previous chain I \n  \n    \n      \n      Price\n      Mileage\n      Feature\n      Value\n      Make\n    \n  \n  \n    \n      3783\n      8638.93\n      25216\n      Leather\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      5391\n      8638.93\n      25216\n      coupe\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      567\n      8638.93\n      25216\n      Cylinder\n      4\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      2175\n      8638.93\n      25216\n      Cruise\n      0\n      [0, 0, 1, 0, 0, 0]\n    \n    \n      6999\n      8638.93\n      25216\n      sedan\n      1\n      [0, 0, 1, 0, 0, 0]\n    \n  \n\n\n\n\n\nLet‚Äôs continue and use groupby and aggregate the Feature column by the agg method. This will give us the mean of each feature appears in the dataset.\n\n# Define a function to create the Make vector\ndef make_vector(row):\n    makes = [\n        'Buick', 'Cadillac', 'Chevy', \n        'Pontiac', 'Saab', 'Saturn',\n    ]\n    return [int(row[make]) for make in makes]\n\n(\n    df\n    .melt(\n        id_vars     = [\n            'Price','Mileage', 'Buick', 'Cadillac', \n            'Chevy', 'Pontiac', 'Saab', 'Saturn',\n        ],\n        var_name    = 'Feature',\n        value_name  = 'Value',\n    )\n    .assign(\n        Make = lambda df: df.apply(make_vector, axis = 1),\n    )\n    .drop(\n        columns = [\n            'Buick', 'Cadillac', 'Chevy', \n            'Pontiac', 'Saab', 'Saturn',\n        ],\n    )\n    .sort_values(\n        by = 'Price',\n    )\n    .groupby(\n        by = 'Feature',\n    )\n    .agg(\n        {\n            'Value': ['mean'],\n        }\n    )\n)\n\n\n\n\n\nTable¬†4:  Continued from the previous chain II \n  \n    \n      \n      Value\n    \n    \n      \n      mean\n    \n    \n      Feature\n      \n    \n  \n  \n    \n      Cruise\n      0.752488\n    \n    \n      Cylinder\n      5.268657\n    \n    \n      Doors\n      3.527363\n    \n    \n      Leather\n      0.723881\n    \n    \n      Sound\n      0.679104\n    \n    \n      convertible\n      0.062189\n    \n    \n      coupe\n      0.174129\n    \n    \n      hatchback\n      0.074627\n    \n    \n      sedan\n      0.609453\n    \n    \n      wagon\n      0.079602\n    \n  \n\n\n\n\n\nSummarizing, we could go on forever and adding more steps to our method chain. But I think you get the point. We can do a lot with method chaining and it is a great way to write clean and readable code.\n\n\nConclusion\nMethod chaining in Pandas is a powerful technique that offers several advantages for data manipulation and analysis workflows. It involves combining multiple operations on a DataFrame or Series into a single, concise chain of method calls. This approach enhances code readability, maintainability, and efficiency.\nFirstly, method chaining reduces the need for intermediate variables, streamlining code and making it more readable. By stringing together operations, such as filtering, transforming, and aggregating, the code becomes a clear and sequential representation of the data transformation process.\nSecondly, it encourages the use of functionally composed operations, leading to more modular and reusable code. This modular nature facilitates changes and updates, as adjustments can be made within the chain without affecting other parts of the code.\nFurthermore, method chaining promotes better memory usage and performance optimization. Pandas optimizes these chains under the hood, reducing the creation of unnecessary intermediate copies of data frames, which leads to improved execution speed and reduced memory overhead.\nLastly, method chaining aligns well with the ‚Äútidy data‚Äù philosophy, as it emphasizes a more structured, organized approach to data manipulation. This promotes consistency and clarity in the analysis process, aiding in collaboration and code maintenance.\nI hope you enjoyed this post and learned something new. If you have any questions contact me. Thanks for reading!"
  },
  {
    "objectID": "posts/glm.html",
    "href": "posts/glm.html",
    "title": "My first post about statmodels - so be nice! :)",
    "section": "",
    "text": "Introduction: Play with GLMs\nDuring my study at HSLU I have learned to use mostly r when it comes to statistic and for some machine learning courses as well. When it comes to work, the most companies I have seen so far, use Python as their main programming language. My previous two employers used Python and R. This article is not going to discuss which language is better, but rather focus on how to use Python and the library statmodels which seems to produce similar outputs as it would in r.\nOne important and cool thing about the statmodels library is that it has a lot of data sets already included. You can find them here. The data set I use is a r data set from Rdatasets. In particular, the dataset from the package modeldata called car_prices is used.\n## Data imports\ndf = (\n    statsmodels.api #¬†imported as sm in following code\n    .datasets\n    .get_rdataset(\"car_prices\", package='modeldata')\n    .data\n)\nAnother important thing about this blog post is the usage of Quarto. Quarto allowed me to write this blog post directly from a Jupyter-Notebook without any fancy and complicated transformations. For more information, please visit the quarto website.\nHowever, let‚Äôs start with the analysis. The first thing we do is to import the necessary libraries and the data set.\n\n\nShow the code\n## Data manipulation imports\nimport pandas as pd\nimport numpy as np\n\n## Display imports\nfrom IPython.display import display, Markdown\n\n## statmodels import\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.genmod.families.family as fam\nfrom patsy import dmatrices\n\n## Plot imports\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (5,5/2.5)\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_theme()\nsns.set_context(\n    \"paper\", \n    rc={\n        \"figsize\"       :   plt.rcParams['figure.figsize'],\n        'font_scale'    :   1.25,\n    }\n)\n\n\n## Data imports\ndf = (\n    sm\n    .datasets\n    .get_rdataset(\"car_prices\", package='modeldata')\n    .data\n)\n\n\n\n\nExplanatory Data Exploration\nThe data exploration is a crucial step before fitting a model. It allows to understand the data and to identify potential problems. In this notebook, we will explore very briefly the data, as the focus of this article is to understand and apply statmodels.\n\n\nThe data set contains 804 observations (rows) and 18 predictors (columns). The data set contains information about car prices and the variables are described as follows: Price; Mileage; Cylinder; Doors; Cruise; Sound; Leather; Buick; Cadillac; Chevy; Pontiac; Saab; Saturn; convertible; coupe; hatchback; sedan; and wagon.\n\n\nFirst, two approximately continous variables Price and Mileage are investigated by means of graphical analysis. The following bar plots show the distribution of the two variables.\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng1 = sns.displot(\n    data = df,\n    x = 'Price',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of car price')\nplt.xlabel('Price')\nplt.ylabel('Count of occurance')\nplt.show(g1)\n\ng2 = sns.displot(\n    data = df,\n    x = 'Mileage',\n    kde = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Mileage')\nplt.xlabel('Mileage')\nplt.ylabel('Count of occurance')\nplt.show(g2)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable Price\n\n\n\n\n\n\n\n(b) Distribution of the variable Mileage\n\n\n\n\nFigure¬†1: Distribution of price and mileage variables.\n\n\n\nFigure¬†1 (a) shows on the left a right-skewed distribution with a peak around 15k$ and price values ranging from 8k dollars to 70k dollars. On the other hand, figure¬†1 (b) shows on the right a more ballanced distribution with a peak around 20k$ and price values ranging from 266 miles up to 50k miles.\nProceeding to the next two variables, Cylinder and Doors, one can see less possible values, ranging from\n\n\nShow the code\nheight = plt.rcParams['figure.figsize'][0]\naspect = plt.rcParams['figure.figsize'][0]/plt.rcParams['figure.figsize'][1] / 2\n\ng = sns.displot(\n    data = df,\n    x = 'Cylinder',\n    discrete = True,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Cylinder')\nplt.xlabel('Cylinder')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n# plt.figure()\ng = sns.displot(\n    data = df,\n    x = 'Doors',\n    discrete = True,\n    shrink = 0.5,\n    height = height,\n    aspect = aspect,\n)\nplt.title('Distribution of Doors')\nplt.xlabel('Doors')\nplt.ylabel('Count of occurance')\nplt.show(g)\n\n\n\n\n\n\n\n\n(a) Distribution of the variable cylinder\n\n\n\n\n\n\n\n(b) Distribution of the variable doors\n\n\n\n\nFigure¬†2: Distribution of cylinder and doors variables.\n\n\n\nThe Figure¬†2 (a) surprised me quite a bit. I had anticipated the car to feature more than 8 cylinders, given that this dataset pertains to American cars. The cylinder count typically spans from 4 to 8, with the values accurately reflecting this range. It‚Äôs worth noting that the number of cylinders is expected to be even.\nAgain surprisingly, the Figure¬†2 (b) shows that the number of doors per car. The values are either 2 or 4, with the latter being more common. This is a bit surprising, as I would have expected the number of doors to be higher for American cars (SUV).\nNext, we check the distribution of the make of the cars in the dataset. For this analysis, we first pivot the dataframe using pd.melt() and calculate the sum by means of groupby() and sum() method as follows:\n\n\nShow the code\nbrands = (\n    df\n    [['Buick', 'Cadillac', 'Chevy', 'Pontiac', 'Saab']]\n    .melt()\n    .groupby('variable')\n    .sum()\n    .reset_index()\n)\n\nbrands.head()\n\n\n\n\n\n\nTable¬†1:  Cars by brand after melting, grouping, and summing \n  \n    \n      \n      variable\n      value\n    \n  \n  \n    \n      0\n      Buick\n      80\n    \n    \n      1\n      Cadillac\n      80\n    \n    \n      2\n      Chevy\n      320\n    \n    \n      3\n      Pontiac\n      150\n    \n    \n      4\n      Saab\n      114\n    \n  \n\n\n\n\n\nAfter aggregation, the visualization of the data is as follows:\n\n\nShow the code\nsns.catplot(\n    data    = brands.sort_values('value', ascending=False),\n    x       = 'variable',\n    y       = 'value',\n    kind    = 'bar',\n    height  = 5,\n    aspect  = 2,\n)\nplt.title('Number of cars by brand')\nplt.xlabel('Brand')\nplt.ylabel('Number of cars')\nplt.show()\n\n\n\n\n\nFigure¬†3: Number of cars by make\n\n\n\n\nIn descending order the most present make is: Chevy followed by Pontiac, Saab, Buick and Cadilac.\nAt this point it should be mentioned, that the data set is not balanced and the distribution of the features is not uniform across the different makes and car features. In a normal project this would be a point to consider and to take care of. However, for the purpose of this project, this is not necessary.\n\n\nModeling\nModeling with statsmodels becomes straightforward once the formula API and provided documentation are well understood. I encountered a minor challenge in grasping the formula API, but once I comprehended it, the usage turned out to be quite intuitive.\nLet‚Äôs delve into an understanding of the formula API (statsmodels.formula.api). This feature enables us to employ R-style formulas for specifying models. To illustrate, when fitting a linear model, the following formula suffices:\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\n    formula='y ~ x1 + x2 + x3', \n    data=df\n)\nThe formula API leverages the patsy package (Patsy Package). It‚Äôs worth noting that the formula API‚Äôs potency extends to intricate models. Additionally, it‚Äôs important to mention that the formula API automatically incorporates an intercept into the formula if one isn‚Äôt explicitly specified. For cases where the intercept is undesired, a -1 can be used within the formula.\nWith the glm class, a vast array of models becomes accessible importing as follows:\nimport statsmodels.genmod.families.family as fam\nThe fam import is necessary for specifying the family of the model. The families available are:\n\n\n\n\nTable¬†2: GLM Families\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nFamily(link,¬†variance[,¬†check_link])\nThe parent class for one-parameter exponential families.\n\n\nBinomial([link,¬†check_link])\nBinomial exponential family distribution.\n\n\nGamma([link,¬†check_link])\nGamma exponential family distribution.\n\n\nGaussian([link,¬†check_link])\nGaussian exponential family distribution.\n\n\nInverseGaussian([link,¬†check_link])\nInverseGaussian exponential family.\n\n\nNegativeBinomial([link,¬†alpha,¬†check_link])\nNegative Binomial exponential family (corresponds to NB2).\n\n\nPoisson([link,¬†check_link])\nPoisson exponential family.\n\n\nTweedie([link,¬†var_power,¬†eql,¬†check_link])\nTweedie family.\n\n\n\n\n\n\nFitting the model and analyzing the results are the same as one would using r. First define the model, then fit it, then analyze the results. The details of the fit can be accessed using the summary method.\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  Price   No. Observations:                  804\nModel:                            GLM   Df Residuals:                      789\nModel Family:                Gaussian   Df Model:                           14\nLink Function:               Identity   Scale:                      8.4460e+06\nMethod:                          IRLS   Log-Likelihood:                -7544.8\nDate:                Sun, 13 Aug 2023   Deviance:                   6.6639e+09\nTime:                        22:02:28   Pearson chi2:                 6.66e+09\nNo. Iterations:                     3   Pseudo R-squ. (CS):              1.000\nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nMileage        -0.1842      0.013    -14.664      0.000      -0.209      -0.160\nCylinder     3659.4543    113.345     32.286      0.000    3437.303    3881.606\nDoors        1654.6326    174.525      9.481      0.000    1312.570    1996.695\nCruise        340.8695    295.962      1.152      0.249    -239.205     920.944\nSound         440.9169    234.484      1.880      0.060     -18.664     900.497\nLeather       790.8220    249.745      3.167      0.002     301.331    1280.313\nBuick       -1911.3752    336.292     -5.684      0.000   -2570.495   -1252.256\nCadillac      1.05e+04    409.274     25.663      0.000    9701.132    1.13e+04\nChevy       -3408.2863    213.274    -15.981      0.000   -3826.295   -2990.278\nPontiac     -4258.9628    256.358    -16.613      0.000   -4761.416   -3756.509\nSaab         9419.1227    331.211     28.438      0.000    8769.960    1.01e+04\nSaturn      -2859.0803    358.709     -7.970      0.000   -3562.137   -2156.023\nconvertible  1.258e+04    525.984     23.922      0.000    1.16e+04    1.36e+04\ncoupe        1559.7620    395.946      3.939      0.000     783.723    2335.801\nhatchback   -4977.3196    339.046    -14.680      0.000   -5641.837   -4312.803\nsedan       -3064.7176    215.007    -14.254      0.000   -3486.123   -2643.312\nwagon        1384.6400    364.920      3.794      0.000     669.410    2099.870\n===============================================================================\n\n\nAfter fitting a GLM using the glm class in statsmodels, you can obtain a summary of the model‚Äôs results using the .summary() method on the fitted model object. Here‚Äôs a general overview of the information typically included in the summary output:\n\nModel Information:\n\nThe name of the model.\nThe method used for estimation (e.g., maximum likelihood).\nThe distribution family (e.g., Gaussian, binomial, Poisson).\nThe link function used in the model (e.g., logit, identity, etc.).\nThe number of observations used in the model.\n\nModel Fit Statistics:\n\nLog-likelihood value.\nAIC (Akaike Information Criterion) and/or BIC (Bayesian Information Criterion).\nDeviance and Pearson chi-square statistics.\nDispersion parameter (if applicable).\n\nCoefficients:\n\nEstimated coefficients for each predictor variable.\nStandard errors of the coefficients.\nz-scores and p-values for testing the significance of the coefficients.\n\nConfidence Intervals:\n\nConfidence intervals for each coefficient, often at a default level like 95%.\n\nHypothesis Testing:\n\nHypothesis tests for the coefficients, typically with null hypothesis being that the coefficient is zero.\n\nGoodness of Fit:\n\nLikelihood-ratio test for overall model fit.\nTests for assessing the contribution of individual predictors to the model.\n\nDiagnostic Information:\n\nInformation about model assumptions and diagnostics, depending on the type of GLM and the method used.\n\nResiduals:\n\nInformation about residuals, which can include measures like deviance residuals, Pearson residuals, etc.\n\n\nRefer to the official documentation for the most accurate and up-to-date information about the summary output for your specific use case.\n\n\nConclusion\nStatsmodels is a powerful Python library for statistical modeling and hypothesis testing, making it an excellent transition for R users or Python users who like R to solve certain problems. It offers a familiar syntax and functionality for regression, ANOVA, and more. Its integration with Python‚Äôs data analysis ecosystem, like pandas, allows seamless data manipulation. With support for various statistical methods and a comprehensive summary output, Statsmodels facilitates effortless migration from R, enabling R users to harness its capabilities in a Python environment."
  },
  {
    "objectID": "WorkInProgress/api.html",
    "href": "WorkInProgress/api.html",
    "title": "FastAPI",
    "section": "",
    "text": "## Data manipulation imports\nimport pandas as pd\nimport numpy as np\n\n## Display imports\nfrom IPython.display import display, Markdown\n\n## statmodels import\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.genmod.families.family as fam\nfrom patsy import dmatrices\n\n## Plot imports\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (5,5/2.5)\nimport seaborn as sns\nsns.set_style('whitegrid')\nsns.set_theme()\nsns.set_context(\n    \"paper\", \n    rc={\n        \"figsize\"       :   plt.rcParams['figure.figsize'],\n        'font_scale'    :   1.25,\n    }\n)\n\n\nMotivation\nDid you ever dream to build your own fast API? Well, as the title of this post suggests, FastAPI is really fast and easy to use, even the documentation is very well written. FastAPI is a modern, high-performance web framework for building APIs with Python. It blends the ease of use of Python with exceptional speed, making it an ideal choice for developing robust and efficient web applications. Leveraging asynchronous programming, FastAPI offers lightning-fast execution, enabling real-time applications and high-traffic services. Its automatic validation, interactive documentation, and type hints enhance developer productivity and code reliability. Whether you‚Äôre crafting a simple REST API or a complex microservices architecture, FastAPI streamlines development, promotes clean code, and optimizes performance, making it the go-to framework for those seeking both speed and simplicity in their Python web projects.\n\n\nData\nI have alway being facinated how the Switzerland works when it come to taxation individuals and companies. Switzerland‚Äôs unique tax system is characterized by its federal structure, granting significant fiscal autonomy to its municipalities and cantons. The country‚Äôs taxation policies are designed to maintain a balanced distribution of responsibilities and resources between the federal government, cantonal governments, and local municipalities. This results in a decentralized tax framework where both cantons and municipalities possess substantial authority over taxation, allowing them to tailor policies according to their specific needs and preferences. This system encourages competition among cantons and municipalities while also fostering a sense of local ownership and control over financial matters. As a result, Switzerland‚Äôs tax landscape is complex, diverse, and reflective of the nation‚Äôs commitment to decentralized governance.\nWe are going to work in this post with data from the nicest and best canton of Switzerland called Zurich :)\n\n# helper function for reading datasets with proper separator\ndef get_dataset(url):\n    if url[-3:] != \"csv\":\n        print(\"The data set URL has no proper 'csv' extension. Reading the dataset might not have worked as expected.\\nPlease check the dataset link and adjust pandas' read_csv() parameters accordingly.\")\n    data = pd.read_csv(\n        url, \n        sep = \",\", \n        on_bad_lines = 'warn', \n        encoding_errors = 'ignore', \n        low_memory = False,\n    )\n    # if dataframe only has one column or less the data is not comma separated, use \";\" instead\n    if data.shape[1] <= 1:\n        data = pd.read_csv(url, sep=';', on_bad_lines='warn', encoding_errors='ignore', low_memory=False)\n        if data.shape[1] <= 1:\n            print(\"The data wasn't imported properly. Very likely the correct separator couldn't be found.\\nPlease check the dataset manually and adjust the code.\")\n    return data\n\ndf = get_dataset('https://www.web.statistik.zh.ch/ogd/data/KANTON_ZUERICH_286.csv')\ndf.head()\n\n\n\n\n\n  \n    \n      \n      BFS_NR\n      GEBIET_NAME\n      THEMA_NAME\n      SET_NAME\n      SUBSET_NAME\n      INDIKATOR_ID\n      INDIKATOR_NAME\n      INDIKATOR_JAHR\n      INDIKATOR_VALUE\n      EINHEIT_KURZ\n      EINHEIT_LANG\n      Unnamed: 11\n    \n  \n  \n    \n      0\n      1\n      Aeugst a.A.\n      √ñffentliche Finanzen\n      Gemeindesteuern\n      Steuergrundlagen\n      286\n      √ò steuerbares Verm√∂gen nat√ºrliche Pers. [1000 ...\n      1990\n      434\n      1000 Fr.\n      Tausend Franken\n      NaN\n    \n    \n      1\n      1\n      Aeugst a.A.\n      √ñffentliche Finanzen\n      Gemeindesteuern\n      Steuergrundlagen\n      286\n      √ò steuerbares Verm√∂gen nat√ºrliche Pers. [1000 ...\n      1991\n      453\n      1000 Fr.\n      Tausend Franken\n      NaN\n    \n    \n      2\n      1\n      Aeugst a.A.\n      √ñffentliche Finanzen\n      Gemeindesteuern\n      Steuergrundlagen\n      286\n      √ò steuerbares Verm√∂gen nat√ºrliche Pers. [1000 ...\n      1992\n      453\n      1000 Fr.\n      Tausend Franken\n      NaN\n    \n    \n      3\n      1\n      Aeugst a.A.\n      √ñffentliche Finanzen\n      Gemeindesteuern\n      Steuergrundlagen\n      286\n      √ò steuerbares Verm√∂gen nat√ºrliche Pers. [1000 ...\n      1993\n      600\n      1000 Fr.\n      Tausend Franken\n      NaN\n    \n    \n      4\n      1\n      Aeugst a.A.\n      √ñffentliche Finanzen\n      Gemeindesteuern\n      Steuergrundlagen\n      286\n      √ò steuerbares Verm√∂gen nat√ºrliche Pers. [1000 ...\n      1994\n      665\n      1000 Fr.\n      Tausend Franken\n      NaN\n    \n  \n\n\n\n\n\n\nWe have data from 201 municipalities during the period between 1990-2022. The data presents the mean income tax. Let‚Äôs have a look at the data.\n\n\n\n\nRequirements\nFor this post we use python>=3.7 and fastapi==0.101.1.\n\n\nInstallation\nYou will need to install two packages: (1) FastAPI and (2) Uvicorn. The first one is the framework itself and the second one is the server that will run the API.\npip install fastapi\npip install uvicorn[standard]\n\n\nIntroduction\n\nimport statsmodels.api as sm\nfrom sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n\n# Create a connection to the database\nengine = create_engine(\n    'sqlite:///../data/FastAPI.db', echo=True\n)\n\niris = (\n    sm\n    .datasets\n    .get_rdataset('iris')\n    .data\n)\n\n\nfrom typing import Union\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n@app.get(\"/items/{item_id}\")\ndef read_item(\n    item_id: int, \n    q: Union[str, None] = None,\n):\n    return {\"item_id\": item_id, \"q\": q}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "",
    "text": "I am Bernardo, an aeronautical engineer with a BSc in Aeronautical Engineering and a MSc in Data Science. My passion lies in both coding and engineering. I am from Zurich üá®üá≠ and have over seven years of experience working as an aeronautical engineer and about three years as a data scientist.\nI would like to invite you go through my CV and blog to learn more about me ü§ì. Please don‚Äôt hesitate to contact me if you have any questions or would like to collaborate on a project ü§ù."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üë®üèΩ‚Äçüíª Experience",
    "text": "üë®üèΩ‚Äçüíª Experience\nExplore my journey through various industries and roles in the world of data science and beyond! From predictive maintenance in the railway sector üöÇüöÇüöÇ, to crafting models for turnover prediction in the food industry üçîüçïüçù and upsell models banking üíµüí∞üí≥, my experiences have been diverse and impactful. Delve into my work with Gaussian Mixture Models, Multi-Output Regression, OCR tools, and more. For a detailed account of my professional voyage, head to Section CV."
  },
  {
    "objectID": "index.html#hobbies",
    "href": "index.html#hobbies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üèùÔ∏è üèôÔ∏è üíª üì∫ Hobbies",
    "text": "üèùÔ∏è üèôÔ∏è üíª üì∫ Hobbies\nI love to travel üõ©Ô∏è üöÇ ‚õ∞Ô∏è üèùÔ∏è üèôÔ∏è to code üíª and binge watch üì∫ in my free time. In particular I discovered a passion for web development and this blog ü§ìüòé."
  },
  {
    "objectID": "index.html#tech-stack",
    "href": "index.html#tech-stack",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üíª Tech Stack:",
    "text": "üíª Tech Stack:"
  },
  {
    "objectID": "index.html#github-trophies",
    "href": "index.html#github-trophies",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üèÜ GitHub Trophies",
    "text": "üèÜ GitHub Trophies"
  },
  {
    "objectID": "index.html#wisdom-of-the-day",
    "href": "index.html#wisdom-of-the-day",
    "title": "Bernardo Freire Barboza da Cruz",
    "section": "üßê Wisdom of the Day",
    "text": "üßê Wisdom of the Day\n\nsee you soon"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "First Name:\nBernardo\n\n\nLast Name:\nFreire Barboza da Cruz\n\n\nBirthday:\n29/05/1985\n\n\nNationalities:\nSwiss & Brazilian\n\n\nPlace of Origin:\nWildberg ZH, Switzerland\n\n\nPlace of Birth:\nRio de Janeiro, RJ, Brazil\n\n\nAddress:\nFrankentalerstrasse 24, 8049 Z√ºrich, Switzerland\n\n\nPhone:\n+41 76 412 53 89\n\n\nEmail:\nberni.dacruz at gmail dot com\n\n\nGitHub:\ngithub.com/bernardo-cruz\n\n\nLinkedIn:\nlinkedin.com/in/bernardo-freire"
  },
  {
    "objectID": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "href": "cv.html#stadler-rail-ag-data-scientist-062023-present",
    "title": "Curriculum Vitae",
    "section": "Stadler Rail AG, Data Scientist (06/2023 ‚Äì present)",
    "text": "Stadler Rail AG, Data Scientist (06/2023 ‚Äì present)\n\nDeveloped machine learning models for predictive maintenance, e.g., door systems, compressor systems, HVAC systems.\nContributed to projects developing data-driven solutions for optimizing rolling stock maintenance processes.\nEstablished CI/CD pipeline for modeling activities and created Python packages for internal use. Technologies: Gitlab, Gitlab CI/CD, Sklearn, FastAPI (Python, SQL, API)"
  },
  {
    "objectID": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "href": "cv.html#prognolite-ag-data-scientist-102022-032023",
    "title": "Curriculum Vitae",
    "section": "Prognolite AG, Data Scientist (10/2022 ‚Äì 03/2023)",
    "text": "Prognolite AG, Data Scientist (10/2022 ‚Äì 03/2023)\n\nDeveloped regression models predicting turnover using customer data and external factors like weather and holidays.\nCreated multi-output regression models to predict menu sales based on customer data and external factors. Technologies: Github, Tidymodels, Sklearn, Random Forest, XGBoost, Cubist (R, Python)"
  },
  {
    "objectID": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "href": "cv.html#crealogix-ag-data-scientist-092021-092022",
    "title": "Curriculum Vitae",
    "section": "Crealogix AG, Data Scientist (09/2021 ‚Äì 09/2022)",
    "text": "Crealogix AG, Data Scientist (09/2021 ‚Äì 09/2022)\n\nDeveloped prospect and upsell models.\nExtracted information from fact-sheets using customized OCR.\nCreated user journeys for chatbot solutions using IBM Watson and Spacy. Technologies: Sklearn, Spacy, Random Forest, XGBoost, RMarkdown (Python, SQL, R)"
  },
  {
    "objectID": "cv.html#bucher-leichtbau-ag-012014-092021",
    "href": "cv.html#bucher-leichtbau-ag-012014-092021",
    "title": "Curriculum Vitae",
    "section": "Bucher Leichtbau AG (01/2014 ‚Äì 09/2021)",
    "text": "Bucher Leichtbau AG (01/2014 ‚Äì 09/2021)\n\nCompliance Verification Engineer (10/2017 ‚Äì 09/2021)\n\nAdvised and supported all departments of Bucher Leichtbau AG in initial airworthiness certification matters.\nIndependently verified documentation for ‚Äúminor‚Äù and ‚Äúmajor changes‚Äù against certification requirements. Technologies: FEMAP, Mechanical Engineering\n\n\n\nCertification Engineer (01/2014 ‚Äì 10/2017)\n\nConducted FEM calculations/verification and prepared internal and external test reports.\nCommunicated with customers, official bodies, and authorities.\nConsulted development department for design optimization. Technologies: FEMAP, Mechanical Engineering"
  },
  {
    "objectID": "data/SwissParliamentAPI.html",
    "href": "data/SwissParliamentAPI.html",
    "title": "Bernardo Cruz",
    "section": "",
    "text": "Show the code\nimport swissparlpy\nimport requests\nimport pandas as pd\nimport os\nimport urllib3\nfrom datetime import datetime\n\n\n\n\nShow the code\nurllib3.disable_warnings()\n__location__ = os.path.realpath(os.getcwd())\n\n\n\n\nSometimes it‚Äôs necessary to tweak the requests Session (e.g.¬†to provide authentication or disable SSL verification). For this purpose a custom session can be passed to SwissParlClient.\n\n\nShow the code\nsession = requests.Session()\nsession.verify = False # disable SSL verification\nclient = swissparlpy.SwissParlClient(session=session)\n\n\nFor most common cases, this is not necessary and you don‚Äôt even have to create your own SwissParlClient.\nSimply use the shorthand methods to get the data:\n\n\nShow the code\nimport swissparlpy as spp\n\ntables = spp.get_tables()\nglimpse_df = (\n    pd.DataFrame(\n        spp.get_glimpse(tables[0])\n    )\n)\nglimpse_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      PartyNumber\n      PartyName\n      PersonNumber\n      PersonIdCode\n      FirstName\n      LastName\n      GenderAsString\n      PartyFunction\n      Modified\n      PartyAbbreviation\n    \n  \n  \n    \n      0\n      1\n      DE\n      12\n      Sozialdemokratische Partei der Schweiz\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      SP\n    \n    \n      1\n      1\n      EN\n      12\n      Parti socialiste suisse\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      PSS\n    \n    \n      2\n      1\n      FR\n      12\n      Parti socialiste suisse\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      PSS\n    \n    \n      3\n      1\n      IT\n      12\n      Partito socialista svizzero\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      PSS\n    \n    \n      4\n      1\n      RM\n      12\n      Sozialdemokratische Partei der Schweiz\n      1\n      2200\n      Pierre\n      Aguet\n      m\n      Mitglied\n      2023-08-15 11:42:37.287000+00:00\n      SP\n    \n  \n\n\n\n\n\n\n\n\n\nShow the code\nclient.get_tables() # get list of all tables\n\n\n['MemberParty',\n 'Party',\n 'Person',\n 'PersonAddress',\n 'PersonCommunication',\n 'PersonInterest',\n 'Session',\n 'Committee',\n 'MemberCommittee',\n 'Canton',\n 'Council',\n 'Objective',\n 'Resolution',\n 'Publication',\n 'External',\n 'Meeting',\n 'Subject',\n 'Citizenship',\n 'Preconsultation',\n 'Bill',\n 'BillLink',\n 'BillStatus',\n 'Business',\n 'BusinessResponsibility',\n 'BusinessRole',\n 'LegislativePeriod',\n 'MemberCouncil',\n 'MemberParlGroup',\n 'ParlGroup',\n 'PersonOccupation',\n 'RelatedBusiness',\n 'BusinessStatus',\n 'BusinessType',\n 'MemberCouncilHistory',\n 'MemberCommitteeHistory',\n 'Vote',\n 'Voting',\n 'SubjectBusiness',\n 'Transcript',\n 'ParlGroupHistory',\n 'Tags',\n 'SeatOrganisationNr',\n 'PersonEmployee',\n 'Rapporteur',\n 'Mutation']\n\n\n\n\nShow the code\nclient.get_variables('Party') # get list of variables of a table\n\n\n['ID',\n 'Language',\n 'PartyNumber',\n 'PartyName',\n 'StartDate',\n 'EndDate',\n 'Modified',\n 'PartyAbbreviation']\n\n\n\n\n\n\n\nShow the code\nparties = client.get_data('Party', Language='DE')\nparties_df = pd.DataFrame(parties)\nparties_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      PartyNumber\n      PartyName\n      StartDate\n      EndDate\n      Modified\n      PartyAbbreviation\n    \n  \n  \n    \n      0\n      12\n      DE\n      12\n      Sozialdemokratische Partei der Schweiz\n      1888-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      SP\n    \n    \n      1\n      13\n      DE\n      13\n      Schweizerische Volkspartei\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      SVP\n    \n    \n      2\n      14\n      DE\n      14\n      Christlichdemokratische Volkspartei der Schweiz\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      CVP\n    \n    \n      3\n      15\n      DE\n      15\n      FDP.Die Liberalen\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      FDP-Liberale\n    \n    \n      4\n      16\n      DE\n      16\n      Liberal-Demokratische Partei\n      1848-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2010-12-26 13:05:26.430000+00:00\n      LDP\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      78\n      1582\n      DE\n      1582\n      Gr√ºne (Basels starke Alternative)\n      1995-01-01 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2015-12-03 08:48:38.250000+00:00\n      BastA\n    \n    \n      79\n      1583\n      DE\n      1583\n      Christlichdemokratische Volkspartei Oberwallis\n      2019-03-04 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2019-03-07 17:24:15.013000+00:00\n      CVPO\n    \n    \n      80\n      1584\n      DE\n      1584\n      Alternative-die Gr√ºnen Kanton Zug\n      2019-11-08 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2019-11-08 17:28:43.947000+00:00\n      Al\n    \n    \n      81\n      1585\n      DE\n      1585\n      Ensemble √† Gauche\n      2019-11-08 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2019-11-08 17:41:39.513000+00:00\n      E√†G\n    \n    \n      82\n      1586\n      DE\n      1586\n      Die Mitte\n      2021-02-02 00:00:00+00:00\n      1753-01-01 00:00:00+00:00\n      2021-08-12 07:59:22.627000+00:00\n      M-E\n    \n  \n\n83 rows √ó 8 columns\n\n\n\n\n\n\n\n\nShow the code\npersons = client.get_data(\"Person\", Language=\"DE\", LastName__startswith='Bal')\npersons.count\n\n\n12\n\n\n\n\nShow the code\nperson_df = pd.DataFrame(persons)\nperson_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      PersonNumber\n      PersonIdCode\n      Title\n      TitleText\n      LastName\n      GenderAsString\n      DateOfBirth\n      DateOfDeath\n      ...\n      MaritalStatusText\n      PlaceOfBirthCity\n      PlaceOfBirthCanton\n      Modified\n      FirstName\n      OfficialName\n      MilitaryRank\n      MilitaryRankText\n      NativeLanguage\n      NumberOfChildren\n    \n  \n  \n    \n      0\n      1442\n      DE\n      1442\n      0\n      0\n      \n      Baldinger\n      m\n      1838-06-26 00:00:00+00:00\n      1907-01-05 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:45:47.190000+00:00\n      Emil A.\n      Emil A. Baldinger\n      0\n      \n      \n      0\n    \n    \n      1\n      1443\n      DE\n      1443\n      0\n      0\n      \n      Baldinger\n      m\n      1800-11-29 00:00:00+00:00\n      1881-01-26 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 12:07:54.837000+00:00\n      Karl L.\n      Karl L. Baldinger\n      0\n      \n      \n      0\n    \n    \n      2\n      1444\n      DE\n      1444\n      0\n      0\n      \n      Baldinger\n      m\n      1810-11-30 00:00:00+00:00\n      1881-07-13 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:52:09.533000+00:00\n      Wilhelm K.\n      Wilhelm K. Baldinger\n      0\n      \n      \n      0\n    \n    \n      3\n      1445\n      DE\n      1445\n      0\n      0\n      \n      Balestra\n      m\n      1873-07-17 00:00:00+00:00\n      1970-03-30 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 12:10:58.890000+00:00\n      Luigi A.\n      Luigi A. Balestra\n      0\n      \n      \n      0\n    \n    \n      4\n      1446\n      DE\n      1446\n      0\n      0\n      \n      Balli\n      m\n      1852-09-20 00:00:00+00:00\n      1924-12-21 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:42:29.907000+00:00\n      Francesco\n      Francesco Balli\n      0\n      \n      \n      0\n    \n    \n      5\n      1447\n      DE\n      1447\n      0\n      0\n      \n      Balli\n      m\n      1796-10-09 00:00:00+00:00\n      1863-09-08 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:14:45.110000+00:00\n      Valentino Alessandro\n      Valentino Alessandro Balli\n      0\n      \n      \n      0\n    \n    \n      6\n      1448\n      DE\n      1448\n      0\n      0\n      \n      Ballmoos\n      m\n      1911-07-07 00:00:00+00:00\n      1993-07-20 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:27:08.147000+00:00\n      Walter\n      Walter Ballmoos\n      0\n      \n      \n      0\n    \n    \n      7\n      1449\n      DE\n      1449\n      0\n      0\n      \n      Bally\n      m\n      1821-10-24 00:00:00+00:00\n      1899-08-05 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:43:44.773000+00:00\n      Carl Franz\n      Carl Franz Bally\n      0\n      \n      \n      0\n    \n    \n      8\n      1450\n      DE\n      1450\n      0\n      0\n      \n      Bally\n      m\n      1847-08-11 00:00:00+00:00\n      1926-07-24 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:40:52.433000+00:00\n      Eduard\n      Eduard Bally\n      0\n      \n      \n      0\n    \n    \n      9\n      1451\n      DE\n      1451\n      0\n      0\n      \n      Bally\n      m\n      1876-12-13 00:00:00+00:00\n      1965-08-02 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:51:15.100000+00:00\n      Iwan\n      Iwan Bally\n      0\n      \n      \n      0\n    \n    \n      10\n      1452\n      DE\n      1452\n      0\n      0\n      \n      Balmer\n      m\n      1859-04-25 00:00:00+00:00\n      1936-03-13 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 12:00:11.637000+00:00\n      Josef Anton\n      Josef Anton Balmer\n      0\n      \n      \n      0\n    \n    \n      11\n      1453\n      DE\n      1453\n      0\n      0\n      \n      Balmer\n      m\n      1872-10-01 00:00:00+00:00\n      1951-07-20 00:00:00+00:00\n      ...\n      \n      \n      \n      2023-08-15 11:26:07.167000+00:00\n      Peter\n      Peter Balmer\n      0\n      \n      \n      0\n    \n  \n\n12 rows √ó 21 columns\n\n\n\n\n\nShow the code\nco2_business = client.get_data(\"Business\", Title__contains=\"CO2\", Language = \"DE\")\nco2_business.count\n\n\n294\n\n\n\n\nShow the code\nco2_df = pd.DataFrame(co2_business)\nco2_df\n\n\n\n\n\n\n  \n    \n      \n      ID\n      Language\n      BusinessShortNumber\n      BusinessType\n      BusinessTypeName\n      BusinessTypeAbbreviation\n      Title\n      Description\n      InitialSituation\n      Proceedings\n      ...\n      SubmissionCouncilAbbreviation\n      SubmissionSession\n      SubmissionLegislativePeriod\n      FirstCouncil1\n      FirstCouncil1Name\n      FirstCouncil1Abbreviation\n      FirstCouncil2\n      FirstCouncil2Name\n      FirstCouncil2Abbreviation\n      TagNames\n    \n  \n  \n    \n      0\n      19923245\n      DE\n      92.3245\n      5\n      Motion\n      Mo.\n      Senkung der CO2-Emissionen\n      \n      \n      \n      ...\n      NR\n      4404\n      44\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      \n    \n    \n      1\n      19952011\n      DE\n      95.2011\n      10\n      Petition\n      Pet.\n      CO2 Lenkungsabgaben\n      \n      \n      \n      ...\n      \n      4418\n      44\n      2\n      St√§nderat\n      SR\n      0\n      \n      \n      \n    \n    \n      2\n      19953546\n      DE\n      95.3546\n      5\n      Motion\n      Mo.\n      Reduktion des CO2-Ausstosses und Kernenergie\n      \n      \n      \n      ...\n      NR\n      4420\n      44\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      \n    \n    \n      3\n      19970030\n      DE\n      97.030\n      1\n      Gesch√§ft des Bundesrates\n      BRG\n      Reduktion der CO2-Emissionen. Bundesgesetz\n      Botschaft vom 17. M√§rz 1997 zum Bundesgesetz √º...\n      <p>Der Klimaschutz geh√∂rt zu den wichtigsten g...\n      <p> Der <b>St√§nderat </b>verabschiedete das CO...\n      ...\n      \n      4506\n      45\n      2\n      St√§nderat\n      SR\n      0\n      \n      \n      \n    \n    \n      4\n      20005227\n      DE\n      00.5227\n      14\n      Fragestunde. Frage\n      Fra.\n      Wer rechnet die CO2-Emissionen des Luftverkehr...\n      \n      \n      \n      ...\n      NR\n      4605\n      46\n      0\n      \n      \n      0\n      \n      \n      Verkehr|Umwelt\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      289\n      20233569\n      DE\n      23.3569\n      8\n      Interpellation\n      Ip.\n      Wirksamkeit der CO2-Sanktionen f√ºr neue Person...\n      \n      \n      \n      ...\n      NR\n      5120\n      51\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      Staatspolitik|Verkehr|Umwelt\n    \n    \n      290\n      20233940\n      DE\n      23.3940\n      6\n      Postulat\n      Po.\n      Alternativen zur CO2-Bet√§ubung. Auftrag des BL...\n      \n      \n      \n      ...\n      NR\n      5121\n      51\n      1\n      Nationalrat\n      NR\n      0\n      \n      \n      Umwelt|Landwirtschaft\n    \n    \n      291\n      20237089\n      DE\n      23.7089\n      14\n      Fragestunde. Frage\n      Fra.\n      CO2-Preis zur Lenkung?\n      \n      \n      \n      ...\n      NR\n      5118\n      51\n      0\n      \n      \n      0\n      \n      \n      Umwelt|Energie|Steuer\n    \n    \n      292\n      20237091\n      DE\n      23.7091\n      14\n      Fragestunde. Frage\n      Fra.\n      Stellungnahme des Bundesrats zur Medienmitteil...\n      \n      \n      \n      ...\n      NR\n      5118\n      51\n      0\n      \n      \n      0\n      \n      \n      Finanzwesen|Umwelt|Energie\n    \n    \n      293\n      20237413\n      DE\n      23.7413\n      14\n      Fragestunde. Frage\n      Fra.\n      Auswirkung ge√§nderter rechtlicher Vorgaben zu ...\n      \n      \n      \n      ...\n      NR\n      5121\n      51\n      0\n      \n      \n      0\n      \n      \n      Verkehr|Umwelt|Energie\n    \n  \n\n294 rows √ó 43 columns\n\n\n\n\n\n\n\n\nShow the code\nbusiness_oct19 = client.get_data(\n    \"Business\",\n    Language=\"DE\",\n    SubmissionDate__gte=datetime.fromisoformat('2019-10-01'),\n    SubmissionDate__lt=datetime.fromisoformat('2019-10-31')\n)\nbusiness_oct19.count\n\n\nPyODataModelError: Edm.DateTime accepts only UTC\n\n\n\n\nShow the code\nbusi_oct19 = pd.DataFrame(business_oct19)\nbusi_oct19 = busi_oct19.sort_values(by=['SubmissionDate']).reset_index(drop=True)\nbusi_oct19[['SubmissionDate', 'Title']]\n\n\nNameError: name 'business_oct19' is not defined\n\n\n\n\n\nThis script shows how to download votes from the Voting table by iterating over each session in a legislative period. The chunks are then saved in a directory as pickled DataFrames.\nLater on, those chunks can easily be combined together as a single DataFrame containing all the votes of a legislative period.\n\n\nShow the code\npath = os.path.join(__location__, \"voting50\")\n\ndef save_votes_of_session(id):\n    if not os.path.exists(path):\n        os.mkdir(path)\n    pickle_path = os.path.join(path, f'{id}.pks')\n    \n    if os.path.exists(pickle_path):\n        print(f\"File {pickle_path} already exists, skipping\")\n        return\n    \n    print(f\"Loading votes of session {id}...\")\n    data = client.get_data(\"Voting\", Language=\"DE\", IdSession=id)\n    print(f\"{data.count} rows loaded.\")\n    df = pd.DataFrame(data)\n    \n    df.to_pickle(pickle_path)\n    print(f\"Saved pickle at {pickle_path}\")\n    print(\"\")\n\n\n# get all session of the 50 legislative period\nsessions50 = client.get_data(\"Session\", Language=\"DE\", LegislativePeriodNumber=50)\nsessions50.count\n\nfor session in sessions50:\n    print(f\"Loading session {session['ID']}\")\n    save_votes_of_session(session['ID'])\n\n# Combine to one dataframe\npath = os.path.join(__location__, \"voting50\")\ndf_voting50 = pd.concat([pd.read_pickle(os.path.join(path, x)) for x in os.listdir(path)])\ndf_voting50\n\n\nLoading session 5001\nLoading votes of session 5001...\n\n\n\n\nShow the code\ndf_5005 = pd.read_pickle(os.path.join(__location__, \"voting50\", '5005.pks'))\ndf_5005\n\n\n\n\nShow the code\n# Combine to one dataframe\npath = os.path.join(__location__, \"voting50\")\ndf_voting50 = pd.concat([pd.read_pickle(os.path.join(path, x)) for x in os.listdir(path)])\ndf_voting50"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Blog",
    "section": "",
    "text": "Pandas Method Chaining\n\n\n\n\n\n\n\nPandas\n\n\nReadability\n\n\nMethod Chaining\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nBernardo Freire\n\n\n\n\n\n\n\n\nMy first post about statmodels - so be nice! :)\n\n\n\n\n\n\n\nstatmodels\n\n\nSeaborn\n\n\nPandas\n\n\nPython vs.¬†R\n\n\nFormula API\n\n\nRDatasets in Python\n\n\nPatsy in Python\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nBernardo Freire\n\n\n\n\n\n\nNo matching items"
  }
]